00:00 -
00:05 - SPEAKER: The plan for today is to talk about the evaluation,
00:09 - so instead of talking about how to build
00:14 - new types of generative models, we're
00:16 - going to discuss how to actually evaluate how good they are.
00:19 - And it's going to be--
00:20 - It's a challenging topic, where there's not really a consensus
00:25 - on what's the right way to do it,
00:27 - but we'll try to cover at least some
00:31 - of the ways that are out there.
00:32 - Nothing is perfect at this point,
00:34 - but we'll cover some of it.
00:38 - So just as a brief recap, we've talked a lot about modeling.
00:43 - We talked about different types of probability,
00:48 - probabilistic models that you can use.
00:50 - You can work directly with the probability density
00:52 - or the probability mass function,
00:54 - which case, we've seen autoregressive models,
00:57 - normalizing flow models, latent variable
00:59 - models, like the variational autoencoder.
01:01 - We've seen energy-based models.
01:04 - We've talked about probabilistic models or generative
01:08 - models, where, instead of representing a probability
01:11 - density function, you represent directly the sampling procedure.
01:14 - So generative adversarial networks would be one example.
01:18 - And then we've talked about score-based models, where
01:21 - instead of representing the density,
01:23 - you represent the score, which is just like the gradient,
01:26 - essentially.
01:27 - And that's yet another model family
01:30 - that you can use to model your data.
01:33 - And we've talked about a number of different training objectives
01:36 - that you can use to fit a model to data.
01:40 - We've talked about KL divergence,
01:42 - which is the same as--
01:43 - minimizing KL divergence is the same as maximizing likelihood,
01:46 - which is a very natural objective
01:48 - to use whenever the likelihood is accessible directly.
01:52 - So if you're modeling directly the probability density
01:55 - function, probability mass function,
01:56 - this is a very reasonable kind of objective to use.
01:59 - And so autoregressive models, flow models,
02:02 - the ELBO in variational autoencoders
02:04 - is also an approximation to the maximum likelihood objective.
02:10 - And, to some extent, contrastive divergence
02:12 - is also an approximation to--
02:14 - or it's exact to the extent that you can get perfect samples
02:18 - from the model.
02:20 - We've seen f-divergences and two-sample tests,
02:23 - which are very natural in the context
02:25 - of generative adversarial networks,
02:27 - if the only thing you have access
02:28 - to our samples from the distributions.
02:30 - Then this is a reasonable way of training a generative model.
02:34 - And then we've seen fisher divergence,
02:37 - which is essentially the same as score matching, which
02:39 - makes a lot of sense whenever you have access to scores
02:43 - or whenever you're working with energy-based models
02:45 - because it allows you to bypass the normalizing constant.
02:49 - And we've seen noise contrastive estimation, which
02:53 - works for energy-based models.
02:55 - And the question is, at this point,
02:59 - there is a lot of different pieces,
03:02 - a lot of different ingredients that you can use.
03:04 - There is many different kinds of model families
03:06 - that you can pick from.
03:07 - There's different kinds of training objectives.
03:09 - And a natural question is, how do you
03:11 - pick which one you should use for a particular data set?
03:15 - And eventually, this boils down to the question
03:17 - of which model is better?
03:20 - Should you train an autoregressive model
03:22 - on your data?
03:23 - Should you train a flow model?
03:24 - So you train a GAN?
03:25 - And in order to answer that question,
03:27 - you need to be able to say model A is better than model B,
03:31 - essentially.
03:32 - And that requires you to be able to evaluate, basically,
03:35 - the quality of a generative model.
03:38 - And that's really, really important
03:43 - because it allows you to make comparisons and pick
03:47 - a model that is most suitable for your problem.
03:50 - And if you think of it from a researcher perspective,
03:54 - it's a super important ingredient.
03:57 - We always want to make progress.
04:00 - We want to build better models.
04:02 - We want to get better and better.
04:03 - But in order to do that we need to be able to measure
04:07 - how good a model is, right?
04:09 - And so we live in a world where it's pretty easy to just--
04:13 - people make their models open source.
04:16 - You can clone a GitHub repo.
04:18 - You can improve, you can make a change to a model
04:21 - or to a training objective.
04:22 - You get something new out.
04:24 - It's very important to be able to quantify your proposed
04:27 - solution better than something that existed before.
04:30 - And again, that requires you to be
04:32 - able to evaluate different kinds of generative models.
04:36 - And unlike the case of discriminative models,
04:41 - typical machine learning models, evaluating generative models
04:44 - is unfortunately pretty hard.
04:46 - In the case of a typical machine learning model
04:48 - that you would use for a discriminative task,
04:50 - let's say you're training a classifier to label data,
04:55 - to map inputs to labels, or pretty
04:58 - low-dimensional simple output space,
05:01 - that's a setting that is pretty well understood,
05:03 - how to measure progress.
05:05 - Somebody comes up with a new architecture for, let's say,
05:08 - computer vision tasks.
05:10 - You can train the models.
05:11 - And you can check what kind of losses they achieve.
05:14 - You can use it on--
05:16 - you're going to define some kind of loss that quantifies
05:19 - what is it that you care about?
05:21 - Is it top one accuracy, top five accuracy, or whatever decision
05:26 - problem you intend to use the predictions that you
05:29 - get from the model in.
05:31 - You can specify a loss function, and then you can try to--
05:35 - given two models, you can evaluate the losses
05:37 - that they achieve on held out unseen data.
05:42 - And that gives you a pretty good handle
05:47 - on the performance of the model that tells you essentially
05:50 - if you were to at test time, when you deploy the model,
05:54 - you were to fit in data that looks like the one
05:56 - that you've been training on.
05:58 - This looks like the one that you have in the test set.
06:00 - Then that's the performance that you would expect.
06:02 - And so that allows you to compare different models
06:06 - and decide which one is better.
06:08 - And unfortunately, things are not
06:11 - so easy for a generative model.
06:14 - It's not clear what is the task.
06:16 - Essentially, that's the main challenge.
06:19 - What is it that we care about?
06:22 - Why are you training a generative model?
06:24 - And there is many different options and many different--
06:28 - and all of them are more or less valid.
06:31 - Perhaps you're training a generative model
06:33 - because you care about density estimation,
06:35 - you care about evaluating probabilities
06:37 - of, say, images or sentences.
06:42 - Maybe you care about compression.
06:44 - Maybe you care about generating samples.
06:46 - At the end of the day, you're training a diffusion model
06:50 - over images, and what you care about
06:52 - is being able to generate pretty outputs that
06:55 - are aesthetically pleasing.
06:57 - Or maybe you're really just trying
07:00 - to do representation learning or unsupervised
07:05 - learning at the end of the day.
07:06 - Like you have access to a lot of unlabeled
07:08 - data, maybe large collections of images or text
07:13 - that you've scraped from the internet.
07:14 - You'd like your model to learn something
07:16 - about the structure of this data,
07:18 - and you'd like to be able to get representations out
07:23 - of the models that then you can use to improve performance
07:27 - on downstream tasks.
07:28 - Instead of working directly on pixels,
07:30 - maybe you can work on representations
07:32 - obtained by a generative model.
07:34 - And then you can get better performance.
07:36 - You can reduce the amount of labeled data
07:38 - that you need to train a model.
07:41 - Or maybe you're thinking about many different tasks
07:45 - that you need to be able to use your model for.
07:49 - Perhaps you're trying to train a single good model over images
07:56 - that then you can use to do compressed
07:58 - sensing, semi-supervised learning, image translation.
08:01 - Or if you're thinking about language models,
08:05 - again, usually, you're trying to find a single model that
08:09 - has been trained on a lot of text,
08:11 - a lot of collected from the internet.
08:13 - And what you really care about is
08:14 - being able to leverage all the knowledge that
08:17 - has been encoded in this big language model, an LLM.
08:22 - And then what you really care about
08:24 - is being able to prompt the model
08:28 - to solve tasks using a small number of instructions
08:32 - or examples.
08:34 - So lots of different things you could do.
08:37 - And these different things will lead--
08:40 - and for each one of them or for some of them, at least,
08:43 - there is many different metrics that you could use to--
08:48 - even if you pick one of these one of these tasks,
08:53 - it's not entirely obvious how you measure performance
08:57 - on each one of them.
08:59 - The simplest one is probably density estimation.
09:02 - If you really care about density estimation,
09:05 - if you really care about being able to accurately quantify
09:09 - probabilities using a generative model,
09:12 - then likelihood is a pretty good metric for that.
09:16 - So what you can do is you can basically,
09:19 - you can split your data into train, validation, and test.
09:22 - You can fit your model using a training set.
09:26 - Maybe you pick hyperparameters on the validation set.
09:29 - And then you can evaluate the performance
09:32 - on the test set, where the performance is just
09:35 - the average log likelihood that the model assigns
09:39 - on test data, which is a pretty good approximation
09:42 - to the average log likelihood that you would expect the model
09:49 - to assign to samples drawn from this data distribution.
09:53 - And essentially, this is the same thing as compression.
09:58 - We've seen that maximizing likelihood
10:01 - is the same as minimizing KL divergence, which
10:02 - is the same thing as trying to compress data, essentially.
10:08 - So at the end of the day, what we're saying
10:12 - is that if you use that as a metric,
10:16 - you are comparing models based on how well they
10:20 - compress the data.
10:22 - And to see that, turns out that there
10:29 - is a way to take a probabilistic model
10:32 - and map it to a compression scheme, where what you would do
10:35 - is you would encode a data point x to some string that can be
10:42 - decoded back in a unique way.
10:44 - And the length of the string basically
10:46 - depends on the probability of the data point.
10:48 - So if you have data points that are very likely,
10:52 - that are very frequent, then you want to assign short codes.
10:56 - And if they are very infrequent, then you
10:58 - can afford to assign very long codes if are not
11:01 - going to see them very often.
11:04 - And that's a way to compress data using a code.
11:12 - And it goes back to the intuition
11:14 - that we had before if you think about the Morse code.
11:17 - It's based on this principle, right?
11:19 - So if you have vowels, like e and a, they are common.
11:23 - So you want to assign a short code.
11:25 - And then if you have letters that are less frequent,
11:27 - then you want to assign a long code to that.
11:30 - And if you train a generative model
11:32 - based on maximum likelihood, you're
11:33 - basically trying to do as well as you can at compression.
11:37 - And if you compare models based on likelihood,
11:39 - you are comparing how well they compress
11:41 - data, which might or might not be what you care about.
11:45 - And to see that, it's pretty clear
11:48 - that if the length of the code that you assigned to a data
11:52 - point x basically is proportional to--
11:55 - it is very close to the log of 1 over p,
11:59 - then you can see that the average code length that you get
12:02 - is going to be this quantity, which is roughly,
12:06 - if you get rid of the fact that the lengths have to be integers,
12:14 - if you approximate it, it's roughly
12:17 - equal to the negative log likelihood.
12:19 - So if you try to maximize the likelihood,
12:21 - you're minimizing the average length of the code that you get,
12:26 - so you maximize the compression that you can achieve.
12:29 - And in practice, if you use these kind of Shannon or Huffman
12:36 - codes that you might have seen before, it's actually expensive.
12:40 - And it's not tractable to actually build
12:43 - one of these codes.
12:44 - But there are ways to get practical compression schemes.
12:48 - So to the extent that you can get a good likelihood,
12:50 - there is an actual computationally efficient way
12:53 - of constructing compression schemes that
12:56 - will perform well as long as you get
12:57 - good likelihoods on the data.
12:59 - There's something called arithmetic coding, for example,
13:01 - that you can actually use.
13:02 - So if you're able to train a deep generative model that
13:05 - gets you good likelihoods, then you
13:07 - can potentially compress your data very well.
13:09 -
13:13 - And this is-- actually, if you've read papers
13:18 - on language models, the GPTs and those kind of things, that's
13:22 - essentially the same metric that they use when
13:25 - they compare language models.
13:27 - They call it perplexity in that setting.
13:30 - But it's essentially a scaled version of the log likelihood.
13:35 -
13:38 - Now, the question is, why compression, right?
13:41 - Is that a reasonable thing to do?
13:43 - Is that what we really care about?
13:46 - It's reasonable in the sense that, as we've
13:50 - discussed, if you want to achieve good compression rates,
13:54 - then you need to basically be able to identify patterns
13:57 - in the data.
13:58 - The only way you can achieve good compression
14:01 - is by identifying redundancy, identifying patterns,
14:04 - identifying structure in the data.
14:06 - So it's a good learning objective,
14:08 - and we know that if you can get the KL divergence to 0,
14:11 - then it means that you've perfectly
14:13 - matched the data distribution.
14:16 - And this makes sense if you're trying to build--
14:19 - train a generative model to capture knowledge
14:22 - about the world.
14:24 - This is a reasonable objective.
14:27 - We're training the model to compress the data.
14:29 - And by doing so, we're learning something
14:31 - about how the world works, essentially,
14:33 - because that's the only way to achieve compression schemes.
14:36 - And so the intuition could be something like this.
14:38 - And if you think about physical laws,
14:40 - like Newton's law or something like that,
14:42 - you can think of it as a one way of compressing data.
14:45 - If you know there is some kind of relationship
14:48 - between variables you care about, like F equals ma,
14:52 - then knowing that relationship allows you to compress the data.
14:56 - You don't have to store--
14:57 - let's say if you have a sequence of accelerations and forces,
15:00 - you don't have to store both of them.
15:02 - You can store just the accelerations,
15:04 - and you can recover the forces through the equation,
15:06 - for example.
15:07 - So any kind of pattern or structure in the data like this
15:13 - allows you to achieve better compression rates.
15:16 - And so, by training a model to compress,
15:18 - you might be able to discover some interesting structure
15:21 - in the data, including maybe knowledge about physical laws
15:25 - and things like that.
15:27 - And there's actually something called the Hutter prize.
15:32 - It's actually-- there's a half million dollars
15:36 - for developing a good compression
15:39 - scheme for Wikipedia.
15:41 - And the quote from the prize website
15:44 - is "being able to compress well is closely related
15:48 - to intelligence.
15:50 - While intelligence is a slippery concept,
15:52 - file sizes are hard numbers.
15:54 - Wikipedia is an extensive snapshot of human knowledge.
15:57 - If you can compress Wikipedia better than the predecessors,
16:02 - your decompressor is likely going to be smarter," basically.
16:05 - And the whole idea behind this prize
16:08 - is to basically encourage the development
16:10 - of intelligent compressors as a path towards achieving AGI.
16:14 - So the hypothesis here is that if you can really
16:17 - compress Wikipedia very well, then
16:20 - you must achieve a very high level of intelligence.
16:25 - And indeed, you can actually compare
16:29 - how well humans do at this--
16:31 - how good are humans at compressing text, right?
16:35 - There's actually an experiment that Shannon did many years ago.
16:39 - And he was very interested in this kind of topic, compression.
16:44 - And he invented the whole field of information theory.
16:47 - And he actually did experiments checking how good--
16:51 - humans have a lot of knowledge, a lot of context.
16:53 - If you see a string of text, you're
16:55 - probably going to be pretty good at predicting what comes next.
16:58 - And so he actually did an experiment
17:01 - with getting human subjects involved
17:04 - and trying to see how good are people basically
17:06 - at predicting the next character in English text.
17:10 - And what he found is that they achieve
17:13 - a compression rate of about 1.2, 1.3 bits per character.
17:17 - So there are 27 characters or something like that.
17:24 - So there's a lot of uncertainty.
17:26 - If you didn't know anything about it,
17:27 - you would need maybe 4 or 5 bits to encode a character.
17:33 - But people are able to do it with only 1 or 2.
17:37 - So there's not too much uncertainty.
17:38 - When you predict the next character in English text,
17:41 - people are pretty good.
17:42 - There's only-- if you think about one bit of information,
17:46 - it encodes two possibilities.
17:48 - And so that's the typical uncertainty that people have
17:51 - when they predict the next character in text.
17:55 - So one bit would correspond to, OK, there's two possibilities,
17:58 - and I'm uncertain about them, about which one it is.
18:03 - And you might ask how well do large language
18:07 - models, neural networks?
18:08 - They actually do better than humans already.
18:11 - And you can get something like people
18:13 - tried on Wikipedia on that Hutter prize data set,
18:15 - and they were able to get something
18:17 - like 0.94 bits per character, so even better than humans.
18:23 - And again, this is a reasonable objective, a reasonable way
18:27 - of comparing models.
18:28 - That's what people use for training large language models.
18:31 - They train them at maximum likelihood.
18:33 - It makes sense to compare them based on perplexity
18:35 - to some extent or try to forecast
18:37 - how good the perplexity is going to be if you were to increase
18:40 - data or you were to increase compute, scaling laws
18:43 - kind of things.
18:44 - But there are issues with compression,
18:46 - and the main issue is that it's probably not a task we actually
18:50 - care about or not entirely reflective of what
18:53 - we care about.
18:54 - And the issue is that basically not all bits of information
18:57 - are created equal.
18:58 - And so if you think about compression,
19:02 - a bit that is encoding a life or death kind of situation
19:07 - is worth exactly the same as something maybe less important,
19:10 - like is it going to rain or not tomorrow?
19:14 - So compressing or compressing the other gives you the same--
19:18 - is the same from the perspective of KL
19:21 - divergence or maximum likelihood.
19:23 - But obviously, it doesn't reflect
19:26 - the way we're going to use the information in downstream tasks.
19:29 - So there are some serious limitations
19:33 - of what you can say by just comparing models
19:35 - in terms of compression.
19:37 - Think about image data sets, same thing.
19:40 - There is certain pieces of information
19:43 - that are much less important to us.
19:45 - You could think about a slight change in color
19:48 - for a particular pixel.
19:49 - It doesn't matter too much.
19:51 - While there's information about what's the label of the image?
19:58 - That is much more important.
20:00 - But from this perspective, it is all the same, basically.
20:03 - It doesn't matter.
20:04 - So that's main limitation of density estimation
20:08 - or compression.
20:09 -
20:12 - And yeah, we'll talk about this more later.
20:16 - The other thing to keep in mind is
20:18 - that compression or likelihood is
20:21 - a reasonable metric for models which have tractable likelihood.
20:26 - But there is a bunch of models that don't even have it.
20:29 - So if you're working with VAEs or GANs or EBMs,
20:33 - it's not even obvious how you would compare models in terms
20:36 - of likelihood or compression.
20:40 - For VAEs, at least you can compare them
20:42 - based on ELBO values, which we know is a lower bound
20:45 - on likelihood, so it's a lower bound on how well you
20:47 - would compress data.
20:51 - But if you have GANs, for example, how would
20:54 - you compare, let's say, the likelihood
20:56 - that you achieve with a GAN to the one
20:58 - that you achieved with an autoregressive model or a flow
21:02 - model?
21:03 - You can't even compare them because there
21:05 - is no way to get likelihoods out of a generative adversarial
21:09 - network.
21:10 - Do you really care about compression?
21:11 - Maybe not.
21:12 - But if you wanted to compare the compression capabilities
21:15 - of a GAN to something else, you would not even
21:18 - be able to do that.
21:19 - And we'll see that, yeah, maybe that's not what you care about.
21:22 - Maybe you care about sample quality,
21:23 - and we'll see that there are other evaluation metrics that
21:27 - maybe make more sense where you can say, OK,
21:29 - is a GAN better than an autoregressive model
21:31 - trained on the same data set?
21:33 - But if you wanted to care about density estimation,
21:36 - then you need to at least be able to evaluate likelihoods.
21:40 - And it's not something you can directly do with a GAN.
21:44 - And so in general, it's a pretty tricky problem
21:49 - to figure out if you have a generative adversarial network.
21:52 - And you have, let's say, an image,
21:55 - and you want to know what is the probability
21:57 - that the model generated this particular image,
22:00 - it's pretty difficult to do.
22:02 - Even if you can generate a lot of samples from the GAN,
22:07 - it's actually pretty tricky to figure out
22:09 - what is the underlying probability density function.
22:12 - And typically, you have to use approximations.
22:16 - And one that is pretty common it's called a kernel density
22:21 - estimation that allows you to basically get
22:24 - an approximation of what is the underlying probability density
22:27 - function given only samples from the model.
22:31 - So it would look something like this.
22:33 - Suppose that you have a generative model for which you
22:36 - are not able to evaluate likelihoods directly,
22:39 - but you're able to sample from it.
22:41 - Then you can draw a bunch of samples.
22:44 - Here I'm showing six of them.
22:46 - And just for simplicity, let's say that the samples
22:49 - are just scalars.
22:50 - So you generate six of them, and the first one is minus 2.1,
22:54 - minus 1.3, and so forth.
22:57 - And these are representative of what
23:00 - is the underlying distribution that generated this data.
23:05 - And the question is, what can we say about probabilities
23:09 - of other data points?
23:10 - So given that you have these six samples from the model, what
23:15 - is the probability, let's say, that we should
23:17 - assign to the point minus 0.5?
23:19 -
23:22 - And one answer would be, well, the model never generated 0.5.
23:29 - Or 0.5 is not among the six samples that we have access to.
23:34 - So we could say, since it doesn't belong
23:36 - to this set of samples, maybe we should set the probability
23:39 - to 0, which is probably not a great answer
23:43 - because we only have six samples.
23:45 - It could be just due to chance we
23:47 - didn't see that particular data point in our set of samples.
23:52 - So a better way of doing things is to do some kind of binning,
23:55 - build some kind of histogram over the possible values
23:59 - that these samples can take.
24:01 - For example, you can build a histogram,
24:04 - let's say, where we have bins with two here.
24:08 - And then you basically count how frequently the data points
24:12 - land in the different bins.
24:15 - And then you sort of make sure that the object that you get
24:22 - is properly normalized so that the area under the curve
24:25 - is actually 1.
24:28 - So because we had a bunch of--
24:31 - we have two data points landing between minus 2 and 2, then
24:35 - we have a little bit higher--
24:36 - we assign a little bit higher probability to that region.
24:40 - And then you can see the shape of this histogram
24:44 - is related to where we're seeing the samples that we have access
24:48 - to in this set.
24:51 - And then, you can evaluate probabilities of new data points
24:57 - by basically checking, in which bin does this test data point
25:03 - land.
25:04 - Minus 0.5 lands in this bin where there is two data points,
25:09 - and so we assign probability density 1 over 6.
25:13 - And then if you take, let's say, minus 0.99, 1.99,
25:18 - I guess that's also in the first-- in this bin
25:20 - where there's two data points.
25:22 - And so it should also be 1/6.
25:24 - And then the moment you step over
25:27 - to the next bin on the left, then
25:31 - the probability goes down to 1 over 12 or something like that.
25:35 - So just basic histogram as a way of constructing
25:39 - an approximation of the probability density
25:41 - function based on samples.
25:42 -
25:45 - It's a reasonable thing, but you can kind of
25:47 - see that these transitions are probably not very natural.
25:52 - Perhaps there is something better we can do.
25:54 - And indeed, a better solution is to basically smooth
25:57 - these hard thresholds that we had because we were using bins.
26:04 - And so the way a kernel density estimator works
26:07 - is that when we evaluate the probability of a test data point
26:12 - x, we basically check how similar this data
26:16 - point is to all the samples that we have in our set.
26:21 - And we do that using this function k, a kernel function.
26:25 - And then we evaluate this probability
26:28 - by basically looking at all the n samples
26:32 - that we have access to, checking, evaluating the kernel
26:35 - on the difference between the data point
26:37 - that we're testing the density on and the samples
26:40 - that we have access to.
26:41 - And then the distance is scaled by this parameter
26:46 - sigma, which is called the bandwidth of the kernel.
26:50 - And to make things concrete, you can think of the kernel
26:53 - as being just a Gaussian function that
26:56 - has that functional form.
26:59 - And so the similarity between two data points
27:03 - decays exponentially based on that equation.
27:09 - And if you do that, then you get a smoother interpolation.
27:15 - Before, we had these bins that were not very natural.
27:20 - Now what we're doing, if you're doing a kernel density
27:23 - estimator using a Gaussian kernel,
27:25 - is we're basically putting little Gaussians centered
27:28 - around each data point that we have in the set of samples.
27:32 - And then we're summing up basically all these Gaussians.
27:36 - And we get an estimate of the density
27:38 - that is now much more smooth.
27:40 - Right?
27:42 - And so essentially, the probability
27:46 - is high if you are close to many data points,
27:50 - kind of like before.
27:52 - But now it's being close is smooth.
27:54 - It's not only about whether you are in the bin or not.
27:58 - Now there is some small effect even if you're very far away.
28:06 - Although, the effect of a data point
28:08 - decays according to that whatever function
28:14 - you choose for the kernel.
28:16 - OK, you choose the kernel.
28:17 - The kernel is basically telling you
28:19 - it should be a non-negative function that is normalized.
28:25 - So it integrates to 1.
28:26 - So that when you take the sum of n kernels,
28:33 - the total area is going to be n.
28:34 - And then you divide by n, and you get
28:36 - an object that is normalized.
28:38 - So you get a valid probability density.
28:41 - And then I guess it has to be symmetric
28:45 - because it's sort of intuitively like a notion of similarity
28:50 - between a pair of data points.
28:53 - And so the function value is going
28:56 - to be high when the difference is close to 0.
29:00 - And the bandwidth controls how smooth basically that
29:05 - interpolation looks like.
29:07 - So what you see here on the left are different kinds of kernel
29:12 - functions you could choose.
29:13 - You could choose Gaussian.
29:14 - You could choose more like a square kind of kernel.
29:19 - That determines how-- what you think
29:25 - is the right way of comparing how similar two data points are.
29:28 - So if you choose a Gaussian, you have
29:30 - that sort of functional form.
29:32 - If you choose some kind of square, kernel
29:34 - that looks like that, then it's more back
29:37 - to the histogram kind of thing where
29:40 - two points are similar if their distance is relatively small.
29:43 - After you're above this threshold,
29:45 - then the distance becomes extremely high.
29:47 -
29:50 - The bandwidth controls the smoothness.
29:52 - And so you can imagine that ideally you'd
29:59 - like to pick a bandwidth such that you
30:03 - get the distribution like the black one, the black curve
30:09 - here, that is as close as possible to the true curve that
30:11 - generated the data, which is shown in gray there.
30:15 - But you can see that if you were to choose a value of sigma that
30:19 - is too small, then you are going to get something
30:25 - like the red curve, which is very jagged again.
30:29 - And so it's kind of under smoothed.
30:32 - And if you were to choose a very high value of sigma,
30:35 - then everything is similar to each other.
30:37 - Then you're going to get a very smooth kind of interpolation.
30:42 - And you get something like the green curve, which again, is not
30:45 - a good approximation of the density that actually generated
30:49 - the data.
30:50 - So back to the question, how do you choose sigma?
30:53 - What you could try to do is you can
30:55 - try to tune it by trying to do cross validation, where you
30:58 - leave out some of your samples.
30:59 - And then you try to see which kind of sigma
31:01 - fits the samples that you've left out as best as possible.
31:04 -
31:08 - And so yeah, that's true at least in principle.
31:13 - It's a way that would allow you to compute--
31:17 - to get an estimate for the underlying density
31:20 - given only samples.
31:21 - Unfortunately, it's actually extremely unreliable
31:25 - the moment you go in high dimensions
31:28 - just because, of course, of dimensionality basically
31:30 - you would need an extremely large number of samples
31:33 - to cover the whole space and all the possible things that
31:37 - can happen.
31:38 - And so the more dimensions you have, the more samples you need.
31:41 - And in practice, it's not going to work
31:43 - very well if you're working on something like images.
31:47 - So there are limitations of what you can do.
31:49 -
31:54 - Now, what if you have latent variable models?
31:57 - If you have a latent variable model, again,
32:00 - but you would like to somehow get the likelihood,
32:04 - in theory, you can get it by integrating out
32:08 - over the latent variable z.
32:10 - We know that that's the expression
32:13 - that you would need if you want to evaluate the likelihood
32:15 - of a data point x.
32:17 - You can, in principle, get it by looking
32:19 - at all the possible values of the latent variables
32:21 - and then checking the conditional probability
32:23 - of generating that data point x given the different z's
32:27 - that you're integrating over.
32:29 - As we know, this can have very high variance,
32:31 - sort of like if the distribution of the prior
32:35 - is very different from the posterior, which basically means
32:42 - that, again, you're going to need a lot of samples
32:44 - to basically get a reasonable estimate of that likelihood.
32:49 - And there are ways to basically make the estimate more accurate.
32:55 - There is something called annealed importance sampling,
32:57 - which is a procedure to basically do importance sampling
33:02 - by constructing a sequence of distributions
33:04 - to draw these z variables.
33:07 - That is kind of interpolating between the bad or naive choice
33:12 - of just sampling from the prior p of z
33:14 - and the optimal thing that you would like to do,
33:16 - which is to sample from the posterior.
33:18 - And we're not going to go into the details.
33:20 - Let me actually skip some of this stuff.
33:22 - But if you have in your project you're working with latent
33:26 - variable models, you have a VAE and somehow you
33:29 - need to compute likelihoods, you might
33:31 - want to look into these kinds of things
33:33 - because they might help you get more accurate estimates
33:37 - of the likelihoods that you get from
33:39 - your variational autoencoder.

00:00 -
00:05 - SPEAKER: Now, what about sample quality, right?
00:08 - In a lot of these situations, we maybe
00:10 - don't care about likelihoods.
00:11 - We don't care about compression.
00:13 - We have two generative models, maybe,
00:17 - and we can produce samples from them.
00:19 - And we would like to know which one is producing better samples.
00:22 - Let's say if you're working on images.
00:24 - Maybe you have two groups of samples.
00:26 - And you'd like to know which one is better.
00:28 - And how to do that?
00:32 - It's not very obvious.
00:35 - It's actually pretty tricky to say this generative model that
00:39 - produce these samples better than the generative model that
00:42 - produce these samples.
00:44 - Not obvious how you could do that.
00:45 -
00:49 - Probably the best way to go about it
00:51 - would be to have involve humans.
00:55 - So ask some annotators to essentially compare the samples
01:03 - and check which ones are better.
01:05 - And of course, that's not going to be scalable.
01:10 - Maybe it's not something you can use during training.
01:12 - But if you have the budget for it,
01:17 - and you have the time for to go through a human evaluation,
01:20 - that's usually the gold standard.
01:24 - There is actually very interesting
01:26 - work in that in the HCI community
01:29 - where people have explored what are principled ways of getting
01:33 - feedback from humans, and try to figure out, and get
01:38 - them to compare the quality of different types of samples,
01:42 - or different kinds of generative models.
01:44 - This paper is actually from Stanford,
01:48 - looking at perceptual evaluation of generative models, which
01:55 - is based in psychology, cognitive science
01:59 - kind of literature.
02:00 - What they suggest is that what you should do
02:05 - is you should take samples from your model.
02:07 - You have real data.
02:08 - And then you can kind of check how much time people need
02:12 - to accurately decide whether or not the samples that they
02:16 - are seeing are fake or real.
02:18 - So if you can only look at a sample for a very small amount
02:21 - of time, you might not be able to perceive the difference
02:25 - from what is real and what is not.
02:27 - Maybe the hands are not rendered correctly.
02:30 - But if you don't have enough time
02:32 - to actually stare at the pictures long enough,
02:34 - you might not be able to see it.
02:36 - And so what they suggested is that we need to look
02:38 - at this time to get a sense of--
02:41 - the longer it takes for people to distinguish real from fake,
02:47 - the better the samples are.
02:49 - And the other metric that they propose is more traditional,
02:53 - and it would basically be the percentage of samples
02:56 - that deceive people when you're giving them
02:58 - an infinite amount of time to actually check, are these real
03:01 - or not?
03:02 - And so you can look at the website if you're interested.
03:06 - And this is sort of what it would do,
03:08 - what it would work like.
03:10 - If you want to determine how much time it takes
03:12 - for people to figure out whether or not samples are real,
03:16 - what you do is you might start with a very,
03:20 - maybe, a fairly large number of, maybe, 500 milliseconds,
03:25 - you give them to decide whether or not the image is real.
03:28 - Maybe they always get it right because they have a lot of time
03:31 - to figure out what kind of mistakes
03:33 - are made by the generative model.
03:35 - Then you start decreasing the time you give them
03:38 - until you get, maybe, around 300 milliseconds where people
03:41 - start kind of not being able to distinguish real from fake.
03:45 - And at that point, that would be the hype time
03:49 - score for this particular generative model.
03:51 - And then yeah, as I mentioned, the longer
03:54 - it takes people to figure that out, the better the samples are.
04:00 - And here you can see some of the examples,
04:04 - and then you can also rank different samples
04:06 - based on how long it would take for human evaluators
04:10 - to basically distinguish different types of samples.
04:13 -
04:17 - Now, the problem with human evaluations are great,
04:22 - and maybe you can use them for your project.
04:25 - The problem with human evaluation
04:27 - is that they tend to be expensive.
04:29 - You actually have to pay people to go
04:31 - through the process of comparing samples,
04:33 - deciding which ones look better.
04:36 - They are hard to reproduce.
04:37 - And there are strange--
04:39 - you need to be very careful on how you set up
04:41 - these human evaluations.
04:43 - The lay out that you use to ask them
04:46 - questions affects the answers that you get.
04:49 - The way you phrase the questions affect the answers that you get.
04:53 - So it's actually very tricky to rely entirely
04:56 - on human evaluations, and they tend
04:57 - to be pretty hard to reproduce.
05:00 -
05:04 - And the other thing you might not
05:06 - be able to get if you just do this is you
05:08 - might not be able to actually evaluate generalization.
05:12 - Again, if you imagine a generative model that is only
05:15 - just memorizing the training set,
05:17 - it will give you very good samples just by definition.
05:21 - And you might not be able to-- even you use humans,
05:23 - you might not be able to actually figure out
05:26 - that indeed the model is actually just memorizing
05:28 - the training set, and it's not actually
05:30 - able to generalize in any meaningful way.
05:34 - And so it would be nice if there was
05:38 - some kind of automatic evaluation metric
05:42 - to actually figure out the quality of the samples.
05:47 - And some that are very popular, that are often
05:50 - used in the literature, and you might
05:51 - need to implement or use also for your projects,
05:55 - are inception scores, FID scores,
05:58 - and KID scores, which actually I think
06:01 - came up at some point in the last lecture,
06:04 - and there were questions of what they actually are.
06:07 - So now we're going to see what they--
06:09 - how they are actually computed, and what they actually mean.
06:14 - So inception scores is something you
06:17 - can use when you're working on labeled data sets.
06:21 - So if somehow you're in a setting where the images have
06:25 - associated labels, then what you can do
06:29 - is you can try to essentially predict
06:31 - the labels on synthetic samples, and you
06:35 - can check what kind of distributions over the labels
06:38 - you get on synthetic samples versus real samples.
06:42 - So if you have access to a classifier that can essentially
06:46 - tell you what's the label for an image x, then what you can try
06:53 - to do is you can try to quantify how good a generative model is
06:59 - by looking at the behavior of the classifier on the samples
07:03 - that it produces.
07:04 - So there are two things that the inception score looks at.
07:09 - The first thing it looks at is something called sharpness.
07:12 - And essentially, you can imagine two sets of samples,
07:16 - one that looks like this, and one that looks like this.
07:19 - And if you were to--
07:21 - this is a labeled data set.
07:22 - Every sample has a label, which is just the number.
07:25 - This is MNIST, so it's kind of a toy example,
07:28 - but every digit, every image you produce
07:31 - can be mapped to a number that it represents.
07:36 - And you can kind of see that somehow these--
07:41 - the true samples are probably relatively easy to classify,
07:44 - while synthetic samples that are a little bit blurred,
07:47 - they're not very clear.
07:49 - They're going to be harder essentially
07:50 - to classify if you have a good classifier.
07:53 - And so the intuition behind sharpness
07:56 - is to basically look at how confident the classifier is
08:00 - in making predictions on the synthetic samples,
08:03 - on the generated samples.
08:05 - So the formula looks like this, and it's essentially something
08:09 - related to the entropy of the classifier
08:12 - when evaluated on samples.
08:14 - So you generate samples from the model,
08:17 - and then you make predictions-- you
08:18 - look at all the possible predictions
08:20 - that the classifier produces over the x's that are synthetic,
08:24 - then this quantity here is basically related
08:27 - to the entropy of the classifier.
08:29 - And when the classifier distribution--
08:34 - predictive distribution has low entropy,
08:36 - so kind of the classifier is putting all the probability
08:39 - mass on one single y, it's very confident in the prediction
08:43 - that it makes, then the sharpness
08:46 - value is going to be high.
08:47 -
08:51 - And the other thing we want to check
08:54 - is something called diversity.
08:55 - And the idea is that if you're working with a labeled data set,
08:59 - you'd like the model to essentially produce
09:03 - images of all the classes that are represented in the training
09:07 - set.
09:07 - So if you have, let's say, again,
09:11 - that generates samples that look like this,
09:13 - this would indicate something like mode collapse, where
09:16 - it's only producing once.
09:17 - And we would like to somehow say, OK,
09:19 - these are not good samples because there's not
09:21 - enough diversity.
09:23 - And the way to quantify it is to basically look
09:26 - at the marginal distribution over the labels
09:31 - that you get from the classifier when you evaluate it
09:34 - on the samples.
09:35 - And you basically try to make sure
09:38 - that this marginal distribution has high entropy,
09:43 - meaning that all the classes that are possible
09:46 - are actually predicted by the classifier
09:50 - over the synthetic samples, essentially.
09:52 - So it's not just producing once, the model.
09:57 - That's the formula.
09:58 - Again, it's basically looking at the entropy
10:02 - of the marginal distribution.
10:03 - Then the way you get the inception score
10:05 - is you multiply together these two numbers.
10:09 - And so high inception score is good
10:11 - because it means that you have high diversity,
10:13 - and you have high sharpness.
10:16 - So it's not perfect.
10:17 - And yeah, that's one example of a failure mode.
10:19 - If somehow it does represent all the digits, but only
10:22 - one kind of digit, you would have a potentially a
10:26 - high inception score, even though you're dropping modes
10:32 - within the clusters kind of corresponding
10:35 - to different labels.
10:36 - So not perfect for sure, but widely used, nevertheless.
10:43 - So higher inception score corresponds to better quality.
10:46 - Why is it called inception score?
10:48 - Well, if you don't have a classifier-- so if you're not
10:50 - sort of in the MNIST, or Toys, or situations, what you can do
10:54 - is you train a classifier train on ImageNet,
10:57 - like the InceptionNet, typically that people use for this.
11:01 - And then you compute these metrics with respect to that.
11:04 - This c of y, if you look at it, it's
11:06 - basically the marginal distribution
11:09 - over the predicted labels when you fit in synthetic samples.
11:13 - So if you were to only produce once,
11:16 - then this c y would be like a one hot vector,
11:20 - and then the entropy would be very low.
11:23 - And so you would be unhappy, basically.
11:26 - And so you want high entropy, meaning that ideally, it
11:29 - should be uniform--
11:30 - the c y should be uniform over the different y's
11:33 - that are possible, so then that means
11:35 - that all the classes are represented in equal numbers,
11:38 - essentially.
11:39 - That was one.
11:40 - And it was often used, but as we discussed, not perfect, far
11:44 - from perfect.
11:46 - One issue is that you're not really--
11:49 - you're kind of only looking at samples
11:50 - from the synthetic samples, but you're not really ever comparing
11:54 - them to real data.
11:56 - If you think about these formulas,
11:57 - you're just looking at synthetic samples.
11:59 - You pass them through the classifier.
12:01 - And you look at statistics of what comes out
12:02 - from the classifier, which seems suboptimal because you're never
12:07 - even comparing synthetic samples to real samples.
12:10 - So there is something called FID score, which
12:13 - tries to essentially compare the similarity of the features
12:18 - extracted by a large pre-trained model on synthetic samples
12:22 - versus real samples.
12:25 - So what you do is this-- you generate a bunch of samples
12:29 - from your generative model, and you have a bunch of real data
12:33 - from, let's say, the test set.
12:36 - And then you feed each sample through some kind
12:38 - of pre-trained neural network, like an InceptionNet,
12:42 - for example.
12:43 - That's why, again, it's called FID score.
12:47 - And then you get features for each data point.
12:50 - There's going to be a distribution over these features
12:53 - because each every data point is going
12:54 - to have a different corresponding feature vector.
12:57 - And what you can try to do is you
12:59 - can feed a Gaussian to the features
13:03 - that you get from the synthetic samples and the features
13:05 - that you get in the real samples.
13:07 - And you're going to get two different Gaussians,
13:11 - meaning that the Gaussians will have different means
13:13 - and different variances.
13:16 - And the closer these two Gaussians are, the closer the--
13:23 - the higher the quality of the samples, essentially.
13:26 - Because if the samples from the synthetic model
13:29 - are very different from the real ones,
13:31 - then you might expect that the features that
13:33 - are extracted by a pre-trained model are going to be different.
13:36 - And therefore, these two Gaussians
13:38 - might be different, maybe have different means,
13:40 - or they have different standard deviations, different variances.
13:45 - Then you get a scalar out of this by taking the Wasserstein-2
13:48 - distance between these two Gaussians which,
13:50 - you can compute in closed form, and it's essentially looking
13:53 - at the difference between the means of the Gaussians and some
13:57 - quantity that basically quantifies how different
14:00 - the two--
14:01 - the variances that you got by fitting a Gaussian
14:03 - to the real data and the fake data
14:06 - are with respect to each other.
14:08 - You could use other things.
14:10 - The reason they use multivariate Gaussians
14:11 - is that this Wasserstein-2 distance
14:14 - can be computed in closed form, but yeah, not
14:16 - particularly principled.
14:18 - Well, if the model is doing a good job
14:21 - at fitting the data distribution,
14:22 - then you would expect the statistics extracted
14:25 - by a pre-trained network would also be similar.
14:28 - So if for example, this pre-trained network is looking
14:31 - at--
14:32 - is extracting statistics, high-level features,
14:35 - like what's in the image, where the objects are located,
14:40 - and things like that, which you might expect
14:42 - these networks to do because they perform pretty well when
14:45 - you fine-tune them on a variety of different tasks.
14:48 - Then in some sense, you're hoping
14:51 - that looking at these statistics will tell you
14:54 - something about how similar the samples are in terms of,
14:58 - do they capture a similar distribution?
15:01 - The features are the ones that are
15:03 - extracted by a pre-trained model, which could be anything.
15:06 - In the FID case, it's InceptionNet.
15:08 - That's why it's called inception distance.
15:10 - And so that's a pre-trained model
15:13 - typically on some large-scale image data set,
15:16 - where you have a lot of different classes
15:18 - in order to perform well at classification
15:20 - and probably has to extract reasonable features.
15:23 - Comparing the feature space kind of makes sense.
15:26 - The other question was, why not just compare the means
15:29 - of the samples themselves?
15:31 - That would be a very simple kind of feature,
15:33 - right, where you're just looking at the individual pixels.
15:36 - You could, but it may be not exactly what we
15:40 - what we care about.
15:40 - It's more interesting to compare these higher-level features that
15:44 - are extracted by a model.
15:46 - You could train on FID, then you can no longer
15:49 - use it as an evaluation metric.
15:51 - So it's not-- the moment you start training on something,
15:54 - it stops to become a good--
15:57 - so, but you could, yeah, to the extent
16:00 - that it's not too expensive to compute,
16:02 - which I think it might be.
16:03 - But you could try, at the very least.
16:07 - And then, yeah, in this case, lower FID is better.
16:12 - And the other thing you can do is
16:14 - to do something called the kernel inception distance.
16:18 - And the idea is to basically do a two-sample test, kind
16:23 - of the same thing we've used for training models.
16:25 - But instead of doing it at the level of the samples themselves,
16:29 - we're going to, again, do it at the level of the features
16:31 - extracted by a pre-trained model.
16:34 - And so the MMD is another kind of two-sample test,
16:37 - where you have samples from two distributions p and q.
16:40 - And what you do is you compare the difference
16:44 - in different moments, what was suggested by dev right now,
16:48 - looking at the mean the variance, and so forth.
16:51 - And more specifically, the way you do it is,
16:53 - back to the kernel idea, you use a kernel
16:56 - to measure similarity between data points.
16:59 - And what you do is you do this--
17:01 - if you have distribution p and q,
17:03 - you check, what is the average similarity between two samples--
17:08 - two real samples, let's say, what's
17:10 - the average similarity between two fake samples?
17:15 - And then you compare that to the average similarity
17:17 - between a real and a fake sample.
17:21 - And if p, again, is equal to q, then you
17:23 - can see that this thing evaluates to 0
17:26 - because the difference between real and fake samples
17:30 - is the same as the difference between two real samples or two
17:35 - fake samples.
17:37 - And the idea is that we're now allowed
17:40 - to use a kernel to basically compare
17:42 - how similar two samples are.
17:44 - And so we don't we don't necessarily
17:45 - have to compare samples in terms of their raw pixel values.
17:52 - But what we can do is we can, again,
17:54 - sort of do MMD in the feature space of a classifier.
17:59 - And so what you would do is you would use a kernel
18:02 - to compare the features to sample to real samples
18:08 - to fake samples, and a real and a fake sample, basically.
18:13 - And it's similar to FID.
18:17 - The key difference is that KID is a little bit more principled,
18:22 - but it's more expensive because you're--
18:24 - if you have n samples, then it has kind of a quadratic cost,
18:28 - as opposed to a linear one because you
18:30 - have to make all pairwise comparisons between the two
18:33 - groups of samples--
18:36 - but similar flavor to FID.
18:39 - It is not obvious from this perspective,
18:41 - but you could also think of it as basically the kernel,
18:47 - you could basically map the samples
18:50 - in the reproducing kernel Hilbert space of the kernel.
18:53 - So it's kind of like if the kernel is comparing data
18:56 - points based on some features, then this is basically
19:00 - the same thing as kind of embedding the real data points
19:04 - and the fake data points in the feature space of the kernel
19:06 - and then comparing those two objects.
19:08 - But the nice thing is that the kernels
19:10 - could have-- could be looking in an infinite number of features.
19:13 - So it's kind of the kernel trick, where you're
19:15 - allowed to compare data points using
19:17 - an infinite number of features without ever having to compute.
19:20 -
19:26 - OK.
19:26 - So that was the three main kind of metrics
19:31 - that are used for evaluating sample quality.
19:36 - There is many more that you might
19:37 - need to consider, especially if you're
19:39 - thinking about text-to-image models,
19:42 - then there's many things you have to worry about.
19:44 - So if the generative model is supposed to take a caption
19:47 - and generate an image, then you do care about image quality,
19:50 - but you do care about other things.
19:52 - For example, you care about the whether or not
19:55 - the images that you generate are consistent with the caption that
19:59 - was provided by the user.
20:01 - But then you might care about other things.
20:03 - You might care about the kind of biases
20:05 - that are shown by the model.
20:06 - You might care about whether or not
20:08 - it's producing toxic content that you might
20:10 - need to filter, how good it is, about reasoning about,
20:15 - if the caption talks about different objects
20:17 - and their spatial relationship, how good is
20:19 - the model at understanding the sort of spatial relationship,
20:24 - and spatial reasoning problems.
20:26 - So there's actually something pretty new
20:30 - that also I was involved in, came out of Stanford.
20:34 - So we put together this benchmark called HEM,
20:38 - holistic evaluation of text-to-image models,
20:41 - where we've considered all the different metrics
20:45 - that we could think of.
20:46 - And we've considered different kind of evaluation scenarios.
20:52 - And so you can see some examples here trying
20:55 - to look at quality, where, maybe, we
20:57 - use the usual FID and inception and KID that we just
21:01 - talked about.
21:02 - But then, we also look at other things, how robust the models
21:05 - are if you change words in the captions,
21:08 - and the alignment between the image that you generate,
21:10 - and the caption, various kinds of aesthetic scores,
21:13 - various kind of originality scores.
21:15 - So a lot of different metrics.
21:17 - And we actually try to do-- yeah.
21:20 - But I think today it is the most comprehensive evaluation
21:23 - of existing text-to-image models.
21:25 - We took a lot of existing models and then
21:27 - we tried to compare them with respect
21:29 - to all these different metrics.
21:30 - And you can go if you're interested and see the results
21:33 - and see which model produces the highest quality
21:38 - images as measured by all these different metrics, both real--
21:41 - both human and automated, or other things.
21:46 - If you care about the biases that these models have,
21:48 - we have a bunch of metrics to measure that.
21:52 - So that might be a useful resource again
21:54 - as you develop your projects.

00:00 -
00:05 - SPEAKER: Now, another thing you might
00:07 - want to do with the model is to get features.
00:11 - We've talked about this idea of doing unsupervised learning.
00:14 - You have a lot of unlabeled data.
00:16 - You might be able to get good features from the model.
00:19 - How do you evaluate whether you have good features
00:22 - or not, which you know already, what's
00:26 - the task you are thinking about.
00:29 - You're trying to get features because then
00:31 - at the end of the day, you care about classifying.
00:33 - You have a classification problem in mind.
00:35 - Then you can always sort of measure the performance
00:39 - on the downstream task.
00:40 - So in that case, it's not too hard.
00:44 - It's a lot more tricky to--
00:47 - if you don't have a task in mind and you're just trying to say,
00:50 - OK, is this model is producing better features
00:52 - than this other model, then it's a lot more
00:54 - tricky to be able to say something definitive there.
00:59 - And there is different aspects that you
01:05 - might want to consider if you are in the unsupervised setting,
01:09 - where there is no task, there is no labels, so no objective way
01:13 - of basically comparing different sort of representations
01:16 - that you might get.
01:17 - You might care about how good the model is at clustering,
01:20 - maybe you care about compression,
01:23 - maybe you care about disentanglement.
01:25 - So maybe you care about this idea
01:27 - that we briefly talked about that if you have a latent
01:30 - variable model, you would like the latent variables to have
01:32 - some kind of meaning, and maybe you
01:34 - might want to be able to control different factors of variation
01:37 - by changing the variables individually.
01:40 - So that's what's kind of referred as disentanglement,
01:43 - where the different variables have kind of separate meanings,
01:46 - and they control different aspects
01:48 - of the data-generating process.
01:52 - So if you care about clustering, ideally, you
01:57 - would to be able to group together data points
02:01 - that have somehow the similar meaning,
02:03 - or that they are similar in some way.
02:05 - And this is all very cyclical, but that's the problem
02:08 - with unsupervised learning.
02:09 - And one thing you can do is you can
02:11 - get-- you can take your VAE or your model that gives you
02:16 - latent representations.
02:17 - You can map points in to this feature space.
02:22 - And then you can apply some kind of clustering
02:24 - algorithm like k-means to group them together.
02:27 - And so here's an example of the kind of thing,
02:30 - you train two generative models on MNIST,
02:32 - and then you map the data points to a two-dimensional latent
02:35 - space.
02:36 - And here the colors represent the different classes.
02:39 - I don't even remember exactly what is B and what is D,
02:42 - but these are two different models,
02:44 - and they produce two different kind of embeddings of the data.
02:49 - And you know, which one is better?
02:54 - Is B better?
02:55 - Is D better?
02:57 - It's unclear.
02:59 - They both seem to be doing something reasonable or kind
03:01 - of data points belonging to the same class end
03:05 - up being grouped together in this latent space.
03:09 - It's not obvious which one you would prefer.
03:11 -
03:15 - So for labeled data sets, again, there
03:18 - is many quantitative metrics.
03:19 -
03:23 - So if you do have labels that you use to--
03:29 - use unlabeled data to come up with the clusters,
03:32 - and then you use labels to evaluate
03:34 - the quality of the clusters, then
03:36 - there's a bunch of metrics, things
03:38 - like the completeness score, homogeneity score, V measures.
03:42 - I'm going to go through them quickly.
03:43 - But there is a bunch of measures that you can use.
03:46 - If you have a label data set, you
03:48 - pretend you don't have the labels, you get representations.
03:53 - You check-- you do clustering, and then you
03:55 - can use the labels to see how good the clusters that you get
04:00 - are.
04:00 - And intuitively, what you want to do
04:02 - is you would like to be able to group
04:04 - together points that belong to the same class.
04:07 - And so maybe you care about making sure
04:10 - that all the points that belong to the same class end up--
04:14 - land in the same cluster.
04:16 - Or maybe you care about homogeneity within the clusters,
04:19 - so you would like to make sure that all the points
04:21 - that land in the same cluster have the same label, or maybe
04:25 - some combination of these two scores.
04:30 - So there's different metrics that you can use.
04:32 - And again, if your project kind of involves something like this,
04:37 - you can look into this into more detail.
04:40 -
04:43 - Another thing you might want to do
04:44 - is to check how well basically the latent
04:48 - representations preserve information
04:51 - about the original data points.
04:53 - So to what extent, basically, you can reconstruct data
04:57 - given the latent representations, which
04:59 - is kind of the task you care about
05:01 - if you're trying to do Lossy compression.
05:03 - So you have data.
05:05 - It might make sense to map it to a latent representation,
05:07 - especially if that latent representation is
05:10 - lower dimensional.
05:11 - And in this case, maybe, you care
05:12 - about being able to reconstruct the original data point as
05:15 - accurately as possible.
05:17 - And so here you see some examples
05:19 - of different representations.
05:23 - And you have the original images on the top,
05:27 - and then you can-- what you see here
05:28 - is what you get if you map them to the latent space,
05:31 - and then you try to reconstruct the image from the latent
05:34 - representation.
05:35 - And so you would like the reconstructions
05:37 - to be as close as possible to the original data,
05:41 - while basically reducing the size of the latent space
05:45 - as much as possible.
05:46 - So here, for example, they are looking
05:48 - at different kinds of representations,
05:51 - where maybe if you compress using JPEG,
05:53 - you would get something like a 17x compression in your images
05:58 - with a small loss in accuracy or quality.
06:01 - While there are these other representations
06:03 - that you get from training a generative model,
06:07 - where, maybe, you can get something
06:09 - like a 90x compression, meaning that the latent vectors
06:13 - that you get by mapping data to the latent space
06:17 - are much smaller than the original data points,
06:20 - and still you're able to do very well at reconstructing
06:23 - the original data points as measured
06:25 - by kind of reconstruction metrics
06:27 - like mean squared error or PSNR or SSIM.
06:30 - Yeah, so here, these have reconstruction loss
06:33 - embedded in them.
06:35 - So it would make sense that they do reasonably well at that.
06:38 - But if you had a different model,
06:40 - maybe you're looking at the representation
06:42 - that you get from a GAN, and you want to know,
06:44 - are those better than the ones I have from my VAE?
06:46 - It depends on what you want to do with this representation.
06:49 - Do you care about clustering?
06:51 - Do you care about reconstruction quality?
06:54 - So this is one aspect that you might care about
06:59 - if you're trying to compare two different types
07:01 - of representations that you get in generative models.
07:05 - Now, the other thing that you might care about the latent
07:08 - space is disentanglement, the idea
07:10 - that we would like these latent representations, the latent
07:14 - variables to kind of capture independent and interpretable
07:19 - attributes of the observed data.
07:21 - Something like, if you have a generative model of faces,
07:27 - maybe if you change one of the latent variables,
07:29 - you change the skin color of the image you produce,
07:32 - or maybe there is another latent variable that
07:35 - controls the age of the people you
07:38 - generate through this generative model, and so forth.
07:42 - And so, for example, maybe there is a latent variable Z 1
07:51 - that is controlling the size of the objects you produce.
07:54 - So if you don't change Z 1, then the size of the object
07:57 - never changes.
07:58 - And as soon as you change the Z 1,
08:00 - then you change the sizes of the objects you produce.
08:04 - Or yeah, that sort of would be the ideal outcome--
08:08 -
08:11 - kind of PCA, but in a non-linear way.
08:18 - You find important aspects, latent factors of variation
08:21 - in the data, and then you're able to control them separately,
08:24 - essentially.
08:26 - And again, there is many metrics that people have come up with,
08:31 - for example, the accuracy of a linear classifier
08:35 - that tries to predict a fixed factor of variation
08:38 - and a bunch of others that I'm not going to go over.
08:41 - But there are some libraries that
08:43 - would allow you to compute these metrics.
08:46 - So if you're doing a project around disentanglement,
08:50 - this might be a good resource to look into.
08:53 - And the kind of unfortunate aspect
08:56 - here is that it's provably impossible
08:59 - to learn a generative model that is disentangled if you only
09:04 - have unlabeled data.
09:06 - So if you never get to see the true kind of latent
09:09 - factors of variation, there is no labels associated
09:12 - with these factors that you're trying to discover from data,
09:15 - it's actually provably impossible to do this.
09:19 - So there has been some empirical success,
09:22 - but it's not well understood why these methods work,
09:25 - and there is some theoretical results showing that it's
09:27 - actually not possible.
09:28 - So I guess there are some limitations there.
09:31 -
09:35 - Cool.
09:36 - Now the other thing that, of course, is very, very popular
09:41 - these days is this idea that if you are working with a language
09:47 - model, perhaps you don't care about going
09:51 - through this process of, let's take the data,
09:53 - let's map it to a latent space, and then
09:55 - let's try to somehow use these representations to improve
10:00 - performance in some kind of downstream task.
10:03 - If you have a generative model of language,
10:06 - then you might be able to directly use
10:09 - the model to solve tasks that involve language
10:12 - by basically asking--
10:13 - by specifying the tasks in natural language.
10:16 - So there are kind of two different ways
10:17 - of using the generative model.
10:19 - You could try to train the generative model
10:22 - in an unsupervised way, and then try to leverage the knowledge
10:25 - that it discovered by mapping data points in this latent
10:28 - space, and then training classifiers
10:30 - the usual way on these latent representations.
10:32 - Or if you're working with a language model,
10:38 - then there is this idea of pre-training the model,
10:42 - using a lot of unlabeled data, and then trying
10:45 - to adapt it, for example, through
10:47 - prompts to actually get it to solve
10:50 - a variety of different tasks.
10:52 - So even though these models have been
10:53 - trained by maximum likelihood, which
10:55 - we know is just compression, we know that they are--
10:59 - if they do well at compression, it
11:01 - means they've learned something about the structure of the data,
11:03 - they've memorized a lot of interesting things,
11:06 - and then the hope is that we can leverage
11:08 - that knowledge in different kinds of downstream tasks.
11:12 - So for example, let's say that you
11:15 - are doing sentiment analysis, where you're basically given,
11:19 - let's say, a review, maybe of a movie,
11:22 - and the goal is to predict whether the sentiment
11:24 - of that review is positive or negative.
11:28 - It's a classic NLP task.
11:31 - How would you use a language model to solve this problem?
11:35 - And the idea is that because we're working with natural
11:38 - language, what you could do is you could try to-- so we have--
11:42 - our interface is a model that takes a sentence
11:44 - and predicts the next word.
11:46 - Let's say it's an autoregressive model.
11:47 - It takes up a piece of language, and then it
11:50 - predicts the next word.
11:52 - Then what you can do is you can craft the sentence here
11:56 - such that this prediction is the only thing the model can do,
12:00 - predict the next word, given the previous text is actually
12:04 - solving the task for you.
12:06 - And so what you can do is you can construct a sentence like,
12:09 - classify the sentiment of the movies
12:11 - below as either positive or negative.
12:13 - Then you give it an example of a movie review,
12:17 - which is positive maybe.
12:18 - This has got to be one of the best episodes, blah, blah, blah,
12:21 - with a positive sentiment, and then you
12:23 - give it another example, maybe, with negative sentiment.
12:26 - And then you have this review that you'd like to classify,
12:29 - and then you fit in the text of the review,
12:33 - and then you have sentiment, and then blank.
12:37 - And then you use the model to predict the next word.
12:39 - You use the model to predict what goes--
12:42 - what should you replace blank with?
12:44 - which is exactly consistent with the API of the model, which is,
12:47 - predict the next word given some context.
12:51 - Then if the model predicts positive,
12:54 - then you're going to classify this as a positive example.
12:56 - And if the model outputs negative there,
12:59 - then they're going to classify it as a negative example.
13:01 - And so this is an example of prompting.
13:04 - And of course, there are many smarter ways of doing it.
13:07 - There's a whole prompt engineering kind of job
13:10 - where people supposedly are good at extracting knowledge
13:13 - from the models by crafting smart prompts.
13:17 - But that's the basic idea, getting the knowledge
13:19 - from these generative models by kind
13:23 - of crafting prompts such that that encode the kind of task
13:29 - that you want to solve without actually
13:31 - going through representations.
13:33 - Of course, it's also possible to just fine-tune
13:35 - the model, which is closer to the idea of getting
13:38 - representations.
13:40 - You could also just take the model
13:42 - and then fine tune it to solve the tasks you care about.
13:45 - So presumably, the pre-trained model
13:48 - is already mapping the inputs like a sentence
13:50 - here, to some representation that
13:52 - is good for predicting the next word.
13:55 - So you might be able to fine-tune the model
13:56 - to do something interesting.
13:58 - That's also quite successful.
14:01 - I think prompting is perhaps nicer
14:03 - because it doesn't involve any training that is somewhat
14:06 - special for language models.
14:09 - And it tends to work pretty well,
14:11 - especially if the language model is a very powerful one.
14:16 - And again, what kind of tasks are you going to consider?
14:23 - That's still a pretty much a-- very much
14:25 - an open problem in terms of, which generative model
14:29 - of language is better?
14:31 - There's many of them out there.
14:33 - How can you say whether model A is better than model B?
14:37 - And you have compute perplexity, which is the same as likelihood
14:41 - but does not quite reflect what we care about.
14:45 - Maybe what we care about is all these kind of scenarios
14:48 - that we might want to be able to ask questions to the language
14:53 - model, or we might want to ask it to do movie reviews for us,
14:58 - or whatever it is that we care about, or do math,
15:01 - or solve riddles for us, or do question-answering.
15:05 - And so again, this is a space where it's not clear what
15:09 - is the right task to consider.
15:12 - And so one way to go about it is to consider
15:15 - a lot of different tasks, a lot of different scenarios,
15:17 - a lot of different metrics because maybe you
15:21 - care about accuracy, but maybe you care about other things.
15:24 - And you can try to see how these different models that exist out
15:29 - there perform on all these different tasks.
15:33 - So you can consider different scenarios.
15:35 - You can consider different adaptation strategies,
15:39 - let's say, different prompting strategies.
15:41 - You can have different metrics, for example, accuracy,
15:45 - or whatever it is when you use the model to solve
15:47 - the task that way.
15:48 - And then you can compare many of the existing
15:51 - models that are out there with respect to all these metrics.
15:54 - And that allows you to, maybe, say, in a more precise way,
15:58 - this model is better than this other model with respect
16:01 - to these metrics on these kind of scenarios.
16:03 - So then there is different efforts out there.
16:05 - There is one from Stanford, HELM, that
16:08 - looked at a lot of different metrics,
16:09 - a lot of different scenarios, a lot of different--
16:12 - which is very thorough.
16:14 - There is one that was led by Google, where they also--
16:17 - was more like collaborative effort,
16:20 - where they ask all the people around the world
16:22 - to come up with tasks.
16:23 - And then they are all part of this big benchmark where there
16:28 - is over 200 tasks that you can ask your language
16:32 - model to solve, and you can see the performance
16:35 - that you get across these different tasks.
16:38 - Yeah, I think it's a good question
16:41 - that somehow the prompting idea has not quite
16:45 - been applied so heavily to the-- so if you have a good generative
16:49 - model of images, how can you use it
16:52 - to solve tasks through prompts?
16:54 - And it's not as natural because the output is an image.
16:56 - And instead of-- it's easy to think of the output
16:59 - to map, say, labels to text, or even bounding boxes to text.
17:07 - And so if the API of your model has text as an output,
17:11 - it's relatively easy to use it to solve a variety of tasks.
17:15 - I think it's a bit less natural if you think
17:17 - of-- if the API has images as an output,
17:20 - but it might be possible.
17:21 - I think it's an interesting kind of area
17:23 - that people are still exploring.
17:24 - And yeah, I don't think there's anything particularly good
17:27 - there, but, yeah.
17:29 - The underlying mechanics is just predicting the next word.
17:33 - And then it has been probably done something
17:37 - like instruction fine-tuning.
17:38 - So it has been actually pre-trained
17:41 - on a lot of unsupervised text, just predicting the next word.
17:44 - That's just compression is not-- it wouldn't probably
17:47 - do very well if you start asking questions in a zero shot way.
17:51 - So what you have to do is you have to sort of fine-tune it
17:53 - on a slightly different kind of data set that is emphasizing
17:57 - more the sort of tasks that you might expect the model
18:01 - to be asked at inference time.
18:04 - And again, there is a little bit of a question
18:06 - of what is the right way of--
18:08 - what kind of task?
18:09 - What is the right distribution?
18:10 - Do we care about movie reviews?
18:12 - Or do we care about question-answering?
18:13 - And how do we weight those tools?
18:16 - It's not clear what's the-- that seems to help,
18:19 - but we don't yet have a good handle in terms of evaluating
18:24 - or seeing what works and what doesn't.
18:26 - It's very coarse at the moment.
18:28 - We're doing actually like using similar things in--
18:31 - like just right now we're basically working on applying--
18:34 - I mean, now we're not the only ones.
18:36 - But people are trying to do basically--
18:40 - you train a model on all the images on the internet.
18:43 - But then maybe you really have some--
18:44 - there's some kind of underlying preference
18:47 - that we would the model to generate images
18:51 - with higher aesthetic value, or maybe we
18:53 - would the model to be non-toxic, or we would the model
18:56 - to be more fair, less biased, and how do you adapt the model
19:00 - to that kind of downstream use.
19:05 - And so what you can do is you can collect preference data,
19:08 - and maybe you can show--
19:09 - you can have a caption, you produce two images,
19:13 - and you ask a user, which one do you prefer?
19:15 - You get preference data on what we like and what we don't like.
19:18 - And then you can fine-tune the diffusion model
19:21 - to be more consistent with this kind of preference data.
19:24 - So that's possible, too.
19:26 - Yeah so prompting is great because you
19:28 - don't have to actually-- you don't have to have access
19:31 - to any compute, and you don't even
19:33 - need to know how to program.
19:34 - The only thing you need to do is you
19:36 - need to be able to specify a natural language, what
19:38 - you want the model to do.
19:40 - And so it can be completely done in a black box way,
19:44 - without even having to know what the model is
19:46 - doing, what the weights are.
19:49 - Fine-tuning requires you to actually train
19:52 - the model for a little bit, at least, on some new data,
19:56 - or some new task, or something new.
19:58 - So the bar is a lot higher in terms of the cost
20:02 - and the expertise that is required for that.
20:05 - The takeaway, a very high-level messages,
20:07 - is it's still a pretty much--
20:09 - it's still a pretty open kind of area
20:11 - how to evaluate generative models.
20:14 - That is still a lot more than we don't.
20:15 - We have some coarse ways of comparing models.
20:19 - And we have a sense of, OK, we're
20:20 - making progress over the years, but there is a lot more work
20:25 - to be done in this space in terms of coming up
20:27 - with better metrics.
20:28 - And even if you have all these large-scale benchmarks,
20:34 - we have a lot of tasks, a lot of metrics,
20:36 - it's still not obvious how you weight them
20:38 - and what is the right distribution of tasks you might
20:40 - expect to use the model on.
20:42 - And so, yeah, lots of work to be done in this space.
20:46 - But hopefully this was helpful.
20:47 - I know many of you are starting to get
20:50 - into the weeds of the project.
20:52 - And so I'm sure you have a lot of questions
20:54 - on how to evaluate models.
20:56 - And so hopefully, you got a sense of what's out there.
20:59 - Unfortunately, we don't have any definitive answer yet,
21:01 - but at least it gives you some ideas
21:03 - of the kind of things you can use for the project.