00:00 -
00:05 - SPEAKER: All right.
00:06 - So the plan for today is to continue
00:09 - our discussion of score-based models,
00:11 - and we'll see how they are connected to diffusion models.
00:15 - And we'll kind of see some of the state-of-the-art stuff that
00:20 - currently has been used to generate images, videos,
00:22 - some of the things we've seen in the very first introductory
00:26 - lecture.
00:28 - So brief reminder, this is kind of the usual roadmap
00:33 - slide for the course.
00:34 - Today, we're talking about diffusion models
00:38 - or score-based models.
00:39 - You can think of them as one way of defining this model
00:43 - family by parameterizing the score
00:45 - and then learning the data distribution by essentially
00:49 - using some kind of score matching loss.
00:51 - And we've seen that, yeah, that's
00:55 - kind of the key underlying idea is
00:58 - that to represent the probability distribution,
01:01 - we're going to use a neural network, which is vector valued.
01:07 - So for every point, it gives you a vector.
01:10 - And that vector is supposed to represent
01:13 - the gradient of the log likelihood at that point.
01:18 - So you can think of it as a vector field,
01:21 - like the one you see here, that is parameterized
01:23 - by some neural network.
01:25 - And so as you change the weights,
01:26 - you get different vector fields.
01:29 - And we've seen that it's possible to fit
01:33 - these models to data by doing score matching.
01:36 - So we've seen that the kind of machinery
01:40 - that we talked about in the context of energy-based models
01:43 - can be applied very naturally to these settings.
01:46 - And so there is a way to fit the estimated gradients
01:52 - to the true gradients by minimizing
01:54 - this kind of loss, which only depends on the model.
01:58 - This is the thing we derived by doing integration by parts.
02:02 - And it's a principled way of fitting the model.
02:06 - The issue is that it's not going to work in practice if you're
02:10 - dealing with high-dimensional settings because
02:12 - of this trace of the Jacobian term
02:15 - that basically would require a lot of back propagation steps.
02:19 - So it's not going to work if you're
02:21 - trying to save model images.
02:24 - And so in the last lecture, we talked about two ways
02:26 - of making score matching more scalable.
02:29 - The first one is denoising score matching,
02:31 - where the idea is that instead of trying
02:34 - to model the score of the data distribution,
02:36 - we're going to try to model the score
02:38 - of this noise-perturbed data distribution.
02:41 - And typically the way we obtain this noise-perturbed data
02:46 - distribution is by starting from a data point and then applying
02:50 - this kind of perturbation kernel,
02:52 - which gives you the probability of error--
02:55 - given that you have a clean image x,
02:56 - what is the distribution over noisy images x tilde?
03:01 - And it could be something as simple
03:02 - as, let's add Gaussian noise to x.
03:06 - And it turns out that estimating the score
03:08 - of this noise-perturbed data distribution
03:11 - is actually much more efficient computationally.
03:14 - And so the usual kind of score matching loss
03:17 - where you do regression, some kind
03:20 - of L2 loss between the estimated score
03:23 - and the true score of the noise-perturbed data density.
03:27 - That's kind of the key difference here.
03:28 - We're no longer estimating the score of p data.
03:30 - We're estimating the score of q sigma.
03:33 - It turns out that it can be rewritten
03:35 - in terms of the score of this transition kernel, perturbation
03:40 - kernel, q sigma of x tilde given x, which is just, let's say,
03:44 - a Gaussian.
03:45 - And so in the case of a Gaussian distribution,
03:49 - this objective function basically
03:51 - corresponds to denoising because the--
03:56 - yeah.
03:56 - Basically, the score of a Gaussian
03:58 - is just like the difference from the mean, essentially.
04:01 - And so you can equivalently think
04:05 - of denoising score matching as solving a denoising problem,
04:08 - where what you're doing is you're sampling a data point,
04:11 - you're sampling a noise vector, and then you're
04:14 - feeding data plus noise to the score model as data.
04:19 - And the goal of the score model is to try to estimate z
04:22 - essentially, which is the amount of noise
04:24 - that you've added to the clean data x.
04:28 - And so there's this equivalence between learning
04:31 - the score of the noise-perturbed data density
04:35 - and performing denoising.
04:37 - And as you see, this is much more efficient
04:39 - because we no longer have to deal with traces of Jacobians.
04:43 - This is-- everything is-- it's a loss that you can efficiently
04:47 - optimize as a function of theta.
04:50 - And so the pros is, yeah, it's much more scalable.
04:53 - It has this intuitive kind of correspondence to denoising,
04:58 - meaning that probability architectures that work well
05:00 - for denoising are going to work well for this kind of score
05:03 - estimation task.
05:05 - The negative side of this approach
05:09 - is that we're no longer estimating the score
05:11 - of the clean data distribution.
05:13 - We're now estimating the score of this noise-perturbed data
05:17 - density.
05:18 - And so we're kind of shifting the goal post here
05:22 - because we're no longer estimating
05:24 - the score of the true data density,
05:26 - but we're estimating the score.
05:27 - Even if we're doing very well at solving this problem,
05:30 - even if we can drive the loss to 0, we don't overfit,
05:33 - everything works well, we're no longer
05:35 - estimating what we started out with,
05:38 - but we're estimating this noise-perturbed data density
05:42 - score.
05:43 - And then, we've seen the alternative is
05:46 - to do some kind of random projection,
05:47 - and that's the sliced score matching approach, where
05:51 - essentially instead of trying to match
05:53 - the true gradient with the estimated
05:56 - gradient at every point, we try to just match their projections
06:01 - along some random direction v. And so at every point,
06:05 - we sample a direction vector v.
06:08 - Based on some distribution, we project the true score,
06:12 - the estimated score at every point.
06:14 - After the projection, you get scalars.
06:17 - And then you compare the projections.
06:19 - And if the vector fields are indeed the same,
06:21 - then the projections should also be the same.
06:24 - And it turns out that, again, this objective function
06:27 - can be rewritten into one that only
06:30 - depends on your model, kind of the same integration
06:32 - by parts trick.
06:34 - And now this is something that can be evaluated efficiently,
06:40 - can optimize efficiently as a function of theta,
06:44 - because essentially it only involves
06:46 - directional derivatives.
06:48 - And so it's much more scalable than vanilla score matching.
06:53 - It also estimates the score of the true data density,
06:55 - as opposed to the data density plus noise.
06:59 - But it's a little bit slower than denoising score
07:03 - matching because you still have to take derivatives, basically.
07:09 - So that's sort of where we ended kind of last lecture.
07:14 - And then the other thing we talked about is
07:17 - that how to do inference.
07:20 - And we said, well, if you somehow
07:23 - are able to estimate the underlying vector
07:26 - field of gradients by doing some kind of score matching,
07:30 - then there are ways of generating samples
07:32 - by using some kind of Langevin dynamics procedure, where
07:35 - you would basically--
07:36 - These scores are kind of telling you in which direction
07:39 - you should go if you want to increase
07:41 - the probability of your data point,
07:42 - and so you just follow these arrows,
07:44 - and you can generate samples, basically.
07:48 - And what we've seen is that this didn't actually
07:51 - work in practice, this variant of the approach.
07:54 - It makes sense but it doesn't work for several reasons.
07:57 - One is that, at least for images,
08:00 - we expect the data to lie on a low dimensional manifold,
08:04 - meaning that the score is not really a well-defined object.
08:09 - We have this intuition that we're not
08:12 - expecting to be able to learn accurate scores
08:15 - when we're far away from the high data density regions.
08:19 - If you think about the loss, it depends on samples
08:22 - that you draw from the data distribution.
08:24 - Most of the samples are going to come
08:26 - from high-probability regions.
08:29 - When you're far away, you have an object that looks nothing,
08:32 - let's say, like an image, you've never seen these things
08:35 - during training, it's unlikely that you're
08:37 - going to be able to estimate the score very accurately.
08:41 - And that's a problem because then kind of Langevin dynamics
08:44 - depends on this information to find high-probability regions.
08:47 - And so you might not-- you might get lost,
08:49 - and you might not be able to generate good samples.
08:52 - And then, yeah, we've seen that there
08:55 - are issues with the convergence speed of Langevin dynamics.
08:57 - It might not even converge if you
09:00 - have zero-probability regions somewhere.
09:02 - It might not be able to go from one region of the state--
09:05 - the space of possible images to another one.
09:08 - And so that's also an issue.
09:11 - And so what we are going to see today
09:14 - is that there is actually a very simple solution
09:16 - to all of these three issues that we just talked about.
09:21 - And that basically involves adding
09:23 - noise, adding, let's say, Gaussian noise to the data.
09:27 - And to see this, we notice that, well, one issue
09:32 - is that if the data lies on a manifold,
09:35 - then the score is not really defined.
09:37 - But the moment you add noise to the data,
09:40 - then it becomes kind of supported over the whole space.
09:43 - Noisy data, you are adding noise,
09:48 - so any possible combination of pixel values
09:51 - has some probability under this noise-perturbed distribution.
09:55 - And so even though the original data
09:57 - lies on a manifold, the moment you add noise,
10:00 - you fall off the manifold, and it becomes
10:02 - supported over the whole space.
10:03 -
10:06 - Score matching on noisy data will
10:10 - allow us to basically estimate the score much more accurately.
10:14 - This is kind of some empirical evidence showing
10:17 - if you try to do score matching on CIFAR-10 on clean images,
10:20 - the loss is very, very bumpy.
10:22 - You're not learning very well.
10:24 - But the moment you add noise to the data, tiny little amount
10:27 - of noise to the data, with some tiny little standard deviation,
10:31 - then the loss converges much more nicely.
10:33 -
10:37 - And it solves the issue of the fact
10:42 - that score matching is not accurate in low-data density
10:46 - regions.
10:47 - But remember, kind of the intuition
10:50 - was that most of your data points are going to come from--
10:54 - let's say if your data is a mixture of two Gaussians,
10:56 - one here and one here, most of the data will be--
10:59 - the samples that you see during training
11:01 - are going to come from this region or this region, the two
11:04 - corners, where the data is distributed.
11:07 - And as a result, if you try to use data fit a score model,
11:13 - there is a true score model in the middle-- there
11:15 - is a true score in the middle.
11:16 - There is an estimated score on the right
11:18 - it's going to be accurate around the high-data density regions.
11:21 - It's going to be inaccurate the moment you go far away.
11:24 - But if you think about adding noise,
11:28 - again, it's kind of a good thing for us
11:31 - because if you add noise to the data,
11:33 - then it's going to-- the samples of the noise-perturbed data
11:39 - densities are going to be, again, kind of spread out all
11:41 - over the space.
11:43 - And so what happens is that now if you think about where you're
11:47 - going to see your samples during training, if you add
11:49 - a sufficiently large amount of noise,
11:51 - the samples are going to be all over the space.
11:54 - They're going to be kind of spread around the whole space.
11:59 - And what this means is that if you are willing to add noise
12:02 - to your data, and you add a sufficiently large amount
12:04 - of noise, then we might be able to estimate the score accurately
12:10 - all over the space.
12:13 - And now, of course, this is good because it
12:16 - means that we might be able to get
12:19 - good information from our Langevin dynamics sampler.
12:23 - Like if we are relying on these arrows
12:25 - to go towards high-probability regions,
12:27 - Langevin dynamics will probably work if we do this.
12:31 - The problem is that we're no longer kind of approximating.
12:35 - We're no longer-- if you do Langevin dynamics
12:39 - over these estimated scores, you're
12:42 - going to be producing samples from the noisy data
12:44 - distribution, so you're going to be generating images
12:47 - that kind of look like this instead of generating
12:51 - images that look like this.
12:53 - So that's kind of the tradeoff here.
12:56 - Yes, we're going to be able to estimate the score more
12:58 - accurately, but we are estimating
13:00 - the score of the wrong thing.
13:01 - Before, what we were doing is we were estimating the score
13:04 - of the noisy data distribution.
13:06 - And so here, if you were to do this,
13:08 - yeah, you would be using the other score matching.
13:10 - You would solve a denoising problem.
13:12 - You will learn the score of the noisy data distribution.
13:15 - Now you follow that score, and you are producing noisy samples.
13:19 - On the one hand, you'd like sigma, the amount of noise
13:22 - that you add, to be as small as possible
13:24 - because then you're learning the score of the clean data.
13:27 - So presumably if you follow those scores,
13:29 - you're going to generate clean samples.
13:31 - On the other hand, if you do that,
13:33 - we're not going to be expected to learn the score very
13:37 - accurately.
13:38 - And so that's the dilemma that we have here, basically.
13:41 - You could use denoising score matching to estimate the score.
13:44 - In fact, that's what we would end up doing.
13:47 - So you could-- if you were to use denoising score matching,
13:51 - you would take data, you would add noise,
13:53 - you would solve the denoising problem.
13:55 - What you end up learning is the score
13:57 - of the perturbed data density.
14:00 - So you end up learning this.
14:02 - But that's not this, which is what you wanted.
14:07 - It's not the score of the clean data density.
14:10 - So in particular, if you were to then follow those scores
14:15 - that you have here, you would produce samples
14:18 - that according to their noise-perturbed data density,
14:22 - in particular, the images would look like this, not like this.
14:25 - So as I said, how you do this, or you could even
14:29 - do sliced score matching here or vanilla
14:31 - score-- you could do sliced score matching, for example,
14:34 - not vanilla, but you could do sliced score matching here
14:36 - to estimate this.
14:39 - Denoising score matching would be a much more natural choice
14:42 - because it's faster, and it automatically
14:44 - gives you the score of a noise-perturbed data density.
14:47 - So here I'm just saying even you were
14:50 - able to estimate the scores, they are not what you want.
14:53 - Using denoising score matching would be a very natural way
14:56 - of estimating these scores.
14:58 - And that's what we're actually going to do.
15:00 - If you recall from the last slide
15:03 - we were saying, OK, if you were to do a PCA of the data,
15:05 - and you keep a sufficiently large number of components,
15:09 - you reconstruct the data almost perfectly, which basically means
15:13 - that the different pixels in an image,
15:15 - they are not linearly independent.
15:17 - They kind of-- once you know a subset of them,
15:20 - you get all the others automatically,
15:22 - which basically means that the images lie
15:25 - on some kind of plane, essentially, which
15:28 - is sort of what I'm visualizing here with this shape.
15:32 - So not all possible pixel values are actually
15:37 - valid in the data distribution, essentially.
15:40 - There is some kind of constraints,
15:41 - which you can think of it as encoding this kind of curve.
15:44 - And all the images that we have in the data,
15:47 - they lie on this surface or this curve
15:50 - in a high-dimensional space.
15:52 - And so the score is not quite well
15:55 - defined because what does it mean to go off the curve,
15:58 - then kind of the probability is 0 the moment
16:00 - you go off the curve.
16:01 - And so it can explode, basically.
16:05 - The moment you add noise, then basically
16:07 - any combination of pixel values is valid
16:10 - because there's always some probability
16:11 - of adding the right amount of noise
16:13 - such that that combination was possible.
16:16 - So if you imagine data that lies on a plane or that kind
16:20 - of surface, and then you add noise,
16:22 - you're kind of moving the value by a little bit,
16:26 - and then it's no longer lies on that plane,
16:28 - or no longer lies on the surface.
16:30 - So you're breaking that constraint
16:33 - that held for the real data no longer holds
16:37 - for noise-perturbed data, and that helps estimating
16:41 - the gradient more accurately.
16:43 - Yeah, so noise does become more stable,
16:45 - but you do have this problem that, as you said--
16:48 - if you add sufficiently small amount of noise--
16:51 - it's going to be more or less.
16:53 - There is not really a discontinuity.
16:55 - So yeah, you add a very small amount of noise,
16:58 - your noise-perturbed data distribution
16:59 - is very close to what you wanted, so that's great.
17:02 - But you're not really solving the problems
17:04 - that we have here, basically.
17:06 - That's what I have in the next slide here.
17:08 - That's the question, how much noise do we want to add?
17:12 - Do you want to add a very little small amount of noise?
17:14 - Do you want to add a lot of noise?
17:16 - That is kind of-- if you think about the different amount
17:21 - of noise that you can add, you're
17:23 - going to get different sort of trade-offs.
17:26 - You can imagine that there is the real data density.
17:30 - There is the real scores.
17:31 - And if you try to estimate them using score matching,
17:34 - there's going to be a lot of error,
17:36 - kind of in this region, as we discussed.
17:40 - Then you could say, OK, now, I'm going
17:42 - to add a little bit of noise.
17:43 - So I'm no longer estimating the right thing,
17:46 - so there's going to be a little bit of error
17:48 - everywhere because I'm estimating
17:50 - noise-perturbed scores instead of true scores,
17:53 - but my estimation starts to become a little bit better.
17:57 - And then you can add even more noise.
18:00 - And then at some point, you are doing a great job
18:03 - at estimating the scores.
18:04 - But you're estimating the scores of something
18:06 - completely wrong because you added
18:08 - too much noise to the data.
18:11 - And maybe that's the extreme where you add a ton of noise,
18:14 - you've completely destroyed the structure
18:17 - that was there in the data.
18:19 - So what you're estimating has nothing
18:21 - to do with the clean images that you started from,
18:24 - but you're doing a very good job at estimating the score
18:26 - because it becomes very easy.
18:29 - So those are the things that you need to balance.
18:32 - We want to be able to estimate the score accurately,
18:34 - and so we would like to add as much noise as possible
18:36 - to do that.
18:37 - But at the same time, adding noise
18:40 - reduces the quality of the things
18:42 - we generate because we're estimating
18:44 - the score of a noise-perturbed data density.
18:47 - So we're no longer estimating what
18:49 - we wanted, which is the thing that we have up here,
18:52 - but we're estimating the score of a data distribution
18:55 - with a lot of noise added.
18:57 - The noise you can estimate any of this.
18:59 - You can use the noisy score matching
19:00 - to estimate the score of any of these slices.
19:03 - But it's going to perform--
19:06 - it might become very bad if the amount of noise that you add
19:09 - is very small.
19:10 - And so that's kind of what you see here.
19:13 - Well, this is maybe the clean score,
19:15 - or maybe this is a little bit of noise,
19:18 - denoising score matching is not going to work very well.
19:21 - Now, denoising score matching is a way
19:23 - of estimating the score of a noise-perturbed data density
19:27 - with any amount of noise that you want.
19:30 - The question is, how much noise do you want to add?
19:32 - You'd like to add as little noise as possible
19:34 - because you want to estimate something
19:36 - close to the real data.
19:38 - But the more noise you add, the better estimation becomes.
19:43 - And so that's kind of the problem,
19:45 - that you want to trade off these two things,
19:46 - and it's not clear how to do that.
19:50 - So this is perhaps another way to think about it.
19:53 - Imagine that somehow the data lies on this curve,
19:56 - and this is just like a curve in a 2D space.
20:01 - Most of your samples are going to be
20:02 - close to that to that thick line that we have here.
20:08 - What's happening?
20:09 -
20:12 - And so if you were to estimate the score far away
20:16 - from the black curve, it's going to be fairly inaccurate.
20:20 - Then you can imagine, OK, let's add a lot of noise, sigma 3.
20:24 - Then most of the samples are going
20:26 - to be pretty far away from the from the black curve.
20:30 - And so we're going to get pretty good directional information
20:34 - when you're far away from a clean sample.
20:37 - But it's going to be inaccurate the moment you get closer
20:40 - to the real--
20:42 - where the real data lies.
20:45 - And then you can imagine a setting
20:47 - where you have an ensemble of different noise levels.
20:52 - You're not just considering a single noise level,
20:54 - but you are considering many of them
20:57 - so that you are able to get good directional information,
21:01 - both when you're far away and when
21:02 - we are a little bit closer and a little bit
21:04 - closer to the real data distribution.
21:08 - And that's kind of the main underlying
21:11 - idea of a diffusion model or score based model.
21:14 - The key idea is that we're not just
21:16 - going to learn the score of the data,
21:19 - or we're not just going to learn the score of the data
21:22 - plus a single amount of noise, but we're
21:24 - going to try to learn the score of the data perturbed
21:27 - with different kinds of amounts of noise.
21:31 - That's the intuition.
21:33 - And so specifically, we're going to consider
21:36 - different amounts of noise, sigma 1, sigma 2, all the way
21:39 - to sigma l.
21:41 - And we're going to use something called annealed Langevin
21:45 - dynamics to basically generate samples.
21:49 - And the basic idea is that when we start, when we initialize
21:54 - our Langevin dynamics procedure, there's
21:57 - probably going to be very little structure in the samples.
22:00 - They don't look like natural images.
22:03 - And so what we can do is we can follow the scores that
22:07 - were estimated for the data distribution, plus a lot
22:11 - of noise.
22:13 - And for a little bit, if you were to keep running this thing,
22:16 - then you would be able to generate samples from the data
22:20 - distribution, plus a lot of noise,
22:22 - which is not what we want.
22:24 - But what we can do is we can use these samples to initialize
22:29 - another Langevin dynamics procedure, where
22:31 - we've decreased the amount of noise by a little bit.
22:36 - And then you, basically, keep running your Langevin dynamics
22:39 - procedure following the scores, corresponding
22:42 - to the data density plus a smaller amount of noise sigma 2.
22:47 - Then you decrease it even more, and you initialize--
22:51 - because you got closer and closer to the high-data density
22:54 - regions, then we know that now we
22:58 - are starting to see more structure in the data.
23:00 - And so we should follow the score for the data density plus,
23:05 - let's say, a very small amount of noise.
23:07 - And then, again, you kind of follow the arrows,
23:10 - and then you're generating samples that we actually--
23:13 - like the ones we actually want because at this point,
23:16 - we kind get the best of both worlds
23:17 - because at the end of this procedure,
23:20 - we're generating samples from data plus a very
23:24 - small amount of noise.
23:26 - But throughout the sampling procedure,
23:28 - we are always kind of getting relatively accurate estimates
23:31 - of the scores because we are considering
23:34 - multiple kind of noise scales.
23:36 - So at the very beginning, where there
23:38 - was no structure in the data, we were
23:40 - following the score corresponding
23:41 - to data plus a lot of noise.
23:43 - And then as we add more and more structure to the data,
23:46 - because we are moving towards higher-probability regions
23:48 - by following these arrows, then we can afford to reduce the--
23:54 - basically consider the gradients of data
23:57 - that was perturbed with smaller amount of noise.
24:01 - And this procedure will get us the best of both worlds
24:05 - because Langevin dynamics is never lost.
24:07 - We're always following a pretty accurate estimate
24:10 - of the gradient.
24:12 - But at the same time, at the end,
24:14 - we're able to generate samples for a distribution of data
24:18 - plus noise, where this noise level sigma 3
24:21 - can be very, very small.
24:22 - So this final samples that you produce
24:24 - are going to be almost clean.
24:26 - Yeah, so typically people use 1,000.
24:29 - That's the magic number.
24:30 - But then we'll talk--
24:32 - in the second part of the lecture,
24:33 - we'll talk about an infinite number of noise levels.
24:35 - So the natural way to do things is
24:37 - to actually consider continuous number, like an infinite number.
24:42 - And that's kind of it gets you the best of--
24:45 - a lot of structure, a lot of interesting things.
24:47 -
24:51 - So that's sort of like the intuition.
24:53 - And you can see here another example of,
24:57 - what happens if you were to just run Langevin dynamics?
25:01 - It kind of gets has this problem where you're
25:05 - seeing too many particles down here because it doesn't
25:09 - mix sufficiently rapidly.
25:11 - And even though there should be more probability mass up here,
25:14 - meaning that more particles should end up here,
25:17 - they're just kind of too many down here
25:20 - because the arrows are basically-- you're not
25:22 - estimating things accurately.
25:24 - And if you do annealed Langevin dynamics,
25:27 - so you use this procedure where you
25:29 - run multiple lines of dynamics chains
25:31 - corresponding to different amounts of noise,
25:34 - then it ends up giving you the right sort of distribution,
25:36 - where you see there is many fewer particles down here
25:40 - representing the fact that there should be less probability
25:43 - mass down there.
25:44 -
25:47 - And yeah, here is another example showing this,
25:51 - but let me skip.
25:52 - So what does it mean in practice?
25:55 - What it means in practice is that in order to do this,
25:58 - you need to be able to estimate the score, not just of the data
26:04 - density, not just of the data density
26:06 - plus a certain fixed amount of noise.
26:09 - But you need to be able to jointly estimate
26:12 - the score of the data plus different amounts of noise
26:16 - levels, various amounts of noise levels.
26:19 - So you need to be able to know what is the score of data
26:22 - plus a lot of noise.
26:23 - You need to be able to know what is the score of data
26:26 - plus a little bit less noise, all the way
26:28 - down to a very, very small amount of noise
26:30 - added to the data where it's almost kind of the true data
26:36 - density.
26:37 - And that's fine because if you do a annealed Langevin
26:40 - dynamics-- even though this score is only ever going
26:43 - to be estimated accurately close to the high-data density
26:46 - regions, we still have that problem.
26:48 - This score here is not going to be estimated
26:51 - accurately everywhere.
26:52 - It's only ever going to be estimated accurately
26:55 - when you're very close to, let's say, a real image.
26:58 - But that's fine because we're using this Langevin dynamics
27:00 - procedure, and we're only going to use this score
27:03 - model towards the end of the sampling, where we already
27:06 - have a pretty good guess of the kind of images we want.
27:09 - While this score here, which is data plus a ton of noise,
27:13 - is going to be estimated pretty accurately.
27:16 - Everywhere, it's going to be good at the beginning
27:18 - of the sampling, but we don't want
27:20 - to just keep following that because we
27:21 - want to be able to sample from something close
27:24 - to the clean data distribution.
27:28 - And so to make things efficient, what we would do
27:32 - is we would have--
27:34 - you could potentially train separate score networks,
27:38 - one for every noise level.
27:40 - If you have, let's say, 1,000 noise levels, that
27:43 - would mean 1,000 different neural networks, kind
27:46 - of training each one being trained
27:49 - on a different kind of vector field.
27:53 - To make things more efficient in practice, what you can do
27:56 - is you can have a single neural network that
27:59 - takes an additional input parameter sigma, which
28:03 - is basically just the amount of noise that we're considering,
28:06 - and the single neural network will jointly
28:09 - estimate all these different vector fields.
28:12 - So when you fit in a large value of sigma here as an input,
28:16 - then the network knows that it should be estimating the vector
28:20 - field for, let's say, data plus a lot of noise,
28:23 - while when you fit in as an input a small value of sigma,
28:26 - then the network knows that it needs
28:28 - to estimate the vector field for data density
28:31 - plus a small amount of noise.
28:34 - And so this is just basically a way
28:36 - to kind of make the computation a lot more efficient because we
28:39 - have now a single model that is trained to solve
28:42 - all these different estimation problems.
28:45 - It's going to be worse than just training 1,000 separate models,
28:49 - but it's going to be much more efficient because we're just
28:51 - training a single neural network at that point.
28:55 - Yeah, so what we're learning here,
28:57 - so this vector fields are not necessarily conservative,
29:00 - unless you parameterize the network in a certain way.
29:03 - You could potentially parameterize the network such
29:06 - that it's the gradient of an energy function.
29:09 - Actually, it doesn't hurt performance
29:10 - too much if you do that, but it doesn't actually seem to help.
29:14 - So in practice, you can just use a free form
29:16 - kind of neural network that goes from, say, images to images,
29:20 - and that's not a problem.
29:23 - But you're right that it's not necessarily
29:25 - the gradient of a potential of an energy function,
29:27 - and so weird things can happen.
29:29 - Where if you follow a loop the probability can go up or down,
29:33 - even though if there was really an underlying energy,
29:36 - it shouldn't change the probability.
29:39 - So that could be a problem.
29:41 - But in practice, it works.
