00:00 -
00:05 - SPEAKER: So the basic idea is that we're
00:08 - going to think about exactly that problem of deciding
00:12 - whether or not two distributions are similar by looking
00:17 - at the samples you get if you're going to sample
00:20 - from one or the other.
00:22 - So you can imagine this setting where
00:24 - I give you a bunch of samples that
00:27 - are sampled from some distribution P,
00:29 - and I give you a bunch of samples
00:31 - that are coming from some distribution Q.
00:34 - And what we can try to do is we can try to figure out
00:41 - if there is a way to tell whether or not
00:44 - P, this distribution that we use to generate
00:47 - the first batch, this group of samples,
00:50 - is actually the same as the distribution
00:51 - that we used to generate the right group of samples.
00:58 - If you can do that, then that could
01:02 - be a good way of comparing probability distributions
01:05 - because if we don't have a way of telling whether or not
01:09 - the distributions are different, then
01:11 - it probably means that the distributions
01:13 - are close to each other.
01:16 - And so that's basically what we're
01:17 - going to do is we're going to use something
01:20 - called a two-sample test, which is basically a procedure
01:23 - for determining whether or not these two groups of samples,
01:29 - S1 and S2, are coming from the same distribu--
01:32 - are generated by the same probability distribution
01:35 - or not.
01:37 - And so it's an hypothesis testing problem,
01:39 - which you might have seen before in other contexts where
01:43 - basically there is a null hypothesis, which is
01:45 - the distributions are the same.
01:47 - So P is equal to Q. So the samples in the first group
01:52 - have the same distribution as the samples
01:54 - in the second group.
01:56 - And then there is an alternative hypothesis,
01:58 - which is that the distributions are different.
02:00 - Yes.
02:02 - You want to compare the distributions because, I guess,
02:04 - we will need a training objective.
02:06 - So we typically use KL divergence
02:09 - to figure out how close our model is to the data.
02:11 - And we said, OK, KL divergence is good but perhaps
02:16 - not ideal in the sense that you can
02:17 - get pretty close in KL divergence
02:19 - and have terrible samples.
02:21 - So maybe there is room for choosing different comparisons,
02:27 - different ways of comparing probability distributions that
02:30 - are closer to perceptual quality of the samples
02:33 - and will allow us to do things differently.
02:36 - And that's the main motivation.
02:38 - So one way to go about it is to say,
02:40 - OK, let's say I have a bunch of samples from P data.
02:42 - Let's say I have a bunch of samples from P theta.
02:44 - How do I compare them?
02:46 - And the most basic thing to do is to say,
02:48 - Can I decide whether or not they are different?
02:50 - Because if I cannot figure out if they are different,
02:53 - then it means that I'm close.
02:56 - They are the same.
02:58 - So if I fail at this hypothesis testing problem--
03:02 - or it's very hard to do well in this task,
03:07 - then it means the distribution are similar.
03:09 - And so I have a pretty good way of--
03:11 - and then I have a reasonable way of comparing.
03:13 - Then maybe I'm doing well on my learning problem.
03:16 - So that's the intuition.
03:18 - How do you do it?
03:19 - Typically what you would do is you would come up
03:21 - with a test statistic which is just a function that you
03:26 - use to compare these two sets.
03:29 - For example, you might try to look
03:31 - at what is the mean of the samples in the first group
03:35 - versus the mean of the samples in the second group.
03:38 - Or you could look at the variance, the sample
03:41 - variance of the samples in S1 versus
03:44 - the sample variance in S2.
03:47 - And presumably, if indeed P is equal to Q,
03:51 - then you would expect the mean of the samples in here
03:54 - to be similar to the means of the samples in here.
03:56 - And you would expect the variances to be similar.
04:00 - So, for example, one statistic you could try
04:04 - is to do something like this where
04:06 - you compute the mean in the first group,
04:09 - you compute the mean in the second group,
04:11 - and then you compare them.
04:13 - And what you could do is you could say, OK,
04:16 - if this statistic is larger than some threshold,
04:21 - then I reject the null hypothesis.
04:24 - Otherwise, I say that H0, so the null hypothesis
04:29 - is consistent with my observation.
04:31 - And as you were saying, there is always some type one and type
04:34 - two error in the sense that T is random
04:37 - because S1 and S2 are random.
04:39 - And so even when P is equal to Q,
04:42 - it's possible that the means are different
04:45 - just because of randomness.
04:48 - It's not going to be super important to what
04:51 - we're going to do.
04:53 - But, yeah, there is no-- it's a hard problem.
04:56 - Even if you had a good statistic,
04:59 - there is still going to be some probability of error.
05:02 - But you could ask the question of, OK,
05:04 - what's the best statistic for determining
05:07 - whether or not to solve this hypothesis testing problem?
05:12 - And you could try to minimize type one and type two errors.
05:14 - There's going to be false positives.
05:16 - There's going to be false negatives.
05:17 - But you can try to choose a statistic that minimizes
05:21 - these types of errors.
05:23 - In practice, one is going to be real data.
05:26 - One is going to be--
05:27 - let's say, S1 is going to be samples from the data
05:29 - distribution, and S2 is going to be samples from our model,
05:33 - but it doesn't have to.
05:34 - It could be two models.
05:35 - It could be-- yeah, this is pretty generic.
05:39 - But the way we're going to actually
05:40 - use it for a generative adversarial network,
05:42 - it's going to be one group of samples are going to be real.
05:45 - One group of samples are going to come from that model.
05:47 - So they're going to be fake.
05:48 - And then we're going to use some statistic
05:51 - to try to determine whether or not
05:54 - these two samples are similar.
05:55 -
06:00 - And, yeah, the key observation here
06:03 - is that at least there is some room for choosing
06:09 - different kind of test statistics,
06:11 - and you can choose some statistics which do not
06:15 - depend on the likelihood of P or Q. For example,
06:18 - if you just look at the means, you don't
06:20 - need to know the probability.
06:23 - You don't need to be able to evaluate probabilities under P.
06:25 - You don't need to evaluate probabilities under Q.
06:28 - You can compute this function just
06:29 - based on a bunch of samples.
06:31 - Yeah, so the question is, Does it
06:33 - solve the overfitting problem?
06:35 - You still have the overfitting problem
06:37 - just like in even if you do maximum likelihood,
06:40 - you can still have overfitting problem.
06:41 - So this does not directly address the overfitting
06:44 - directly although, yeah, you can--
06:48 - sometimes, at least, you can use validation and other things
06:52 - to see what would be the--
06:55 - yeah.
06:55 - So, yeah, so you could try to choose test statistics that
06:59 - are based on the likelihood of the model,
07:02 - but you don't have to.
07:03 -
07:05 - And so, again, the setting of generative
07:11 - modeling with two sample tests is one where we have--
07:15 - there's going to be a bunch of samples
07:16 - that are just going to be coming from the data distribution.
07:20 - Recall that that's all we always assume that somebody is giving
07:23 - us access to a bunch of step samples
07:26 - that are coming from the data distribution,
07:28 - and that's our training set.
07:29 - And so that's going to be the first group of samples.
07:33 - Then just like before, we have a set of models.
07:36 - We have a set of distributions P theta that we
07:39 - are willing to consider.
07:41 - And as long as these distributions
07:44 - are easy to sample from, which is not a terrible requirement
07:48 - because presumably you're going to use the model
07:50 - to generate samples anyways.
07:51 - But as long as you can somehow sample from the model,
07:54 - you can always generate this second set
07:57 - of samples S2, which are just basically
08:00 - samples from the model.
08:03 - And then what you can do is you can try to train--
08:07 - basically the high level idea is going
08:09 - to be instead of trying to find a model P theta,
08:16 - just going to optimize over the set
08:18 - so that we minimize the KL divergence between these two
08:21 - distributions, we are going to try
08:24 - to find a model that tries to minimize this whatever test
08:30 - statistic that we've decided to use
08:32 - to compare two set of samples.
08:37 - And, for example, in the previous example,
08:41 - it could be something like try to make
08:43 - sure that the means of the samples that I produce
08:46 - matches the mean of the samples that I had in the training set,
08:51 - which would not be very useful, but that's the high level idea.
08:57 - And so the problem is that finding a good statistic
09:02 - is not easy.
09:05 - For example, if you were to just try
09:08 - to minimize this kind of test statistic here,
09:12 - you would end up with a generative model that
09:14 - produces samples with the same mean as the training set, which
09:20 - is-- it's the property is desirable,
09:26 - but it's not sufficient.
09:28 - Like, if you just match the mean,
09:30 - you can still produce very bad samples
09:33 - and fool this kind of test statistic.
09:37 - And that's the problem is that in high dimensions,
09:41 - it's very hard to find some good test statistic.
09:45 - And so intuitively, you could say,
09:48 - OK, let's say that you start comparing
09:51 - probability distributions by matching the mean.
09:53 - And so if you compare the means, you
09:55 - would be able to distinguish, let's say,
09:57 - that this green Gaussian is different
09:59 - from this red Gaussian.
10:02 - But then you could say, OK, just matching
10:04 - the means is not sufficient.
10:06 - Here are two Gaussians that have the same mean
10:09 - but different variances.
10:11 - So if you just compare the means,
10:14 - you would not be able to distinguish between those two
10:16 - Gaussians.
10:17 - And then maybe you--
10:19 - and if you match, let's say, the mean and the variance,
10:22 - you could have different distributions
10:25 - that have the same first moments or same mean and same variance,
10:28 - like a Gaussian and Laplace density, same mean,
10:33 - same variances, but different shapes.
10:35 - So you can get a sense that especially the more dimensions
10:41 - you have-- you see they are modeling a lot of pixels
10:43 - or a lot of tokens.
10:45 - There is many different ways in which
10:47 - two or many different things you could look at when you compare
10:52 - two probability distributions.
10:53 - There's many different features that you
10:55 - could try to compare samples with respect to.
10:59 - And so handcrafting test statistic and just say,
11:05 - OK, let's try to train a model based
11:06 - on that is unlikely to work because you're going to match
11:11 - the test statistic, and there's going
11:13 - to be other differences that you didn't
11:14 - think about that actually matter in practice.
11:20 - So what we are going to do is we're going to try to--
11:27 - instead of picking a fixed handcrafted test statistic,
11:32 - we're going to try to learn one to automatically identify
11:36 - in which ways these two set of samples, that one
11:39 - from the data and the one from the model,
11:41 - they differ from each other.
11:44 - Instead of keeping it fixed and just say
11:46 - let's compare the mean, we're going
11:47 - to try to learn what makes these two samples different.
11:54 - And how to do that?
11:57 - Any guess?
11:59 - You've got to try to just basically train a classifier
12:02 - essentially, which in the language
12:04 - of generative adversarial networks
12:07 - is called a discriminator.
12:08 - So what we're going to machine learning,
12:11 - that's exactly what you would do.
12:13 - If you are trying to solve a classification problem,
12:16 - you're trying to figure out what distinguishes
12:19 - the positive class from the negative class, the job
12:22 - of a classifier or, for example, a deep neural network
12:26 - is to identify features that allow
12:27 - you to discriminate and distinguish
12:29 - these two groups of samples.
12:31 - So we're going to use the same idea to figure out
12:34 - what features of the data should we look at to discriminate
12:38 - between the green curve and the red curve, essentially.
12:44 - And so that's basically what we're
12:46 - going to do is we're going to train a classifier--
12:49 - again, it's called a discriminator in this context--
12:52 - to basically classify-- we're going
12:55 - to use a binary classifier, for example, a neural network,
12:59 - to try to distinguish real samples,
13:03 - which are basically the ones from the first set,
13:05 - the one generated by the data distribution, which, let's say,
13:08 - label one, from fake samples, which are the ones generated
13:13 - by P theta by our model, which we can say, for example,
13:16 - label zero.
13:18 - And we can train a classifier to do that.
13:22 - And as a test statistic, we can use the minus
13:26 - the loss of the classifier.
13:28 -
13:32 - Why do we do this?
13:33 - Well, what happens if the loss of the classifier is high?
13:36 -
13:41 - Well, let's say, if the loss of the classifier is low,
13:45 - then it means that you're doing a very good job
13:47 - as distinguishing the two.
13:50 - They are very well separated.
13:52 - Your classifier is doing a very good job
13:55 - at distinguishing these two groups of samples.
13:59 - And so they are very different.
14:01 - So we want the test statistic to be small--
14:03 -
14:06 - to be large, sorry.
14:07 - And if we have a high loss, then the real and the fake samples
14:13 - are hard to distinguish.
14:14 - And so we expect that them to be similar.
14:18 - So it's based on the likelihood of the classifier.
14:21 - It's not based on the likelihood of the generative model.
14:24 - So by doing things this way, we're
14:26 - going to use the likelihood, but it's
14:27 - going to be the likelihood of a classifier.
14:30 - And that's much easier because that's just
14:31 - going to be a likelihood over a basically a binary variable.
14:35 - And so essentially, it doesn't really
14:37 - put any restriction on the kind of neural network
14:39 - you can use as opposed to the likelihood of X
14:44 - over the input, which requires you
14:46 - to either use autoregressive models
14:48 - or invertible neural networks.
14:50 - It puts a lot of restrictions on the kind of architectures
14:52 - you can use.
14:53 - Here, it's just going to be a likelihood
14:55 - over basically a binary random variable, which
14:57 - is the class label.
14:59 - And so all you have to do is you have
15:01 - to have a softmax at the end that maps it to a probability.
15:05 - But then you can do whatever you want with respect to the X.
15:08 - So you can extract whatever features you want of the input
15:12 - X to come up with a good classifier.
15:15 - And there is really no restriction
15:16 - on the kind of architecture.
15:18 - The goal of this classifier, the way
15:20 - we're going to train this classifier
15:22 - is to maximize the two-sample test statistic to basically try
15:31 - to figure out what kind of statistic
15:33 - basically can maximally distinguish basically
15:37 - between the data and the model, which is the same as minimizing
15:41 - the classification loss.
15:42 - So we're going to train the classifier the usual way
15:45 - to minimize the loss because that's also
15:48 - what gives us the most power to basically distinguish
15:51 - these two distributions, these two set of samples.
15:54 - This is the actual statistic.
15:55 - It could be something like this.
15:57 - So it's actually more like a family
15:58 - of statistics, which are all the ones
16:00 - that you can get as you change.
16:02 - You have a classifier.
16:03 - And then you can imagine changing the parameters
16:05 - of the classifier.
16:06 - And then you can think about if you
16:10 - were to try to find a classifier that maximizes
16:15 - this objective function, which is just minimizing
16:18 - cross-entropy, which you approximate based on data
16:22 - because you only have samples from P data,
16:23 - and you only have samples from the model,
16:25 - you end up with something that looks like this, which
16:28 - is going to be the statistic.
16:29 - Remember before we were just taking the mean of X in S1
16:33 - and the mean of X of the samples in S2.
16:37 - Now, we don't just look at the means.
16:38 - Now we look at what the classifier
16:40 - says on these two samples.
16:42 - And that's what we're going to use.
16:45 - And basically as we discussed before,
16:47 - like if the law, which this is just-- this
16:51 - is the negative loss of the classifier
16:53 - because I'm maximizing.
16:54 - So if this quantity is large, then it
17:00 - means that you're doing a good job at separating them,
17:04 - and it means that they are different.
17:07 - And if the loss is--
17:09 - if this quantity is low, then it means
17:11 - that you're very confused.
17:13 - You're not doing a good job at distinguishing
17:16 - them, which supports the idea that probably they are similar.
17:19 - It's not hard.
17:20 - It's hard to distinguish.
17:22 - What we have here is the--
17:25 - the setting is the one we had before where we're saying
17:28 - we're going to use as a statistic.
17:30 - We're going to use a classifier, which
17:32 - we're going to denote as D5 because it's
17:35 - going to be trainable.
17:36 - It's going to be another neural network
17:38 - that we're going to train.
17:40 - And they're going to train this neural network
17:42 - to try to distinguish between the samples in S1
17:46 - and the samples in S2 where the samples in S1
17:50 - are the-- it's just a sample from a group already sampled
17:55 - from the data distribution.
17:56 - And S2 is just a group of samples from the model
17:59 - distribution, P theta.
18:01 - And if you maximize this objective function
18:05 - over the classifier, you are basically
18:07 - trying to do as well as you can.
18:10 - You're basically just training the classifier the usual way
18:13 - by minimizing cross-entropy.
18:15 - And this is going to be our statistic in the sense
18:19 - that the loss of the classifier will tell us how well,
18:23 - how similar basically these two groups of samples are.
18:28 - And basically, yeah, the discriminator, the phi
18:34 - is performing binary classification
18:36 - with the cross-entropy objective.
18:37 - And you can see that you are going to do--
18:41 - to do well here, what you're supposed to do
18:44 - is you're supposed to assign probability one to all
18:47 - the samples that come from P data,
18:49 - and you're supposed to assign probability zero to all
18:52 - the samples that come from the model
18:53 - if you want to maximize that quantity, which, again, is
18:59 - basically what you would do if you were to train a classifier
19:01 - or distinguish the two groups of samples.
19:03 - And that's just like the negative cross-entropy.
19:09 - So for now, P theta is just the--
19:11 - it's just fixed.
19:13 - It's the model distribution.
19:16 - The data distribution is as usual is fixed.
19:19 - You just have a bunch of samples from it.
19:21 - That's S1.
19:22 - And what we're saying is we're going
19:24 - to try to optimize the classifier
19:27 - to do as well as it can at this task of distinguishing
19:31 - between real samples and fake samples
19:35 - because the loss of the classifier
19:37 - will basically tell us how similar the samples that
19:41 - come from the model are to samples
19:43 - that come from the data.
19:45 - Imagine-- do I have it here?
19:48 - Yeah.
19:49 - So imagine that somehow the--
19:53 - maybe-- yeah, OK.
19:55 - Imagine that P-- that the two distributions are the same.
20:00 - So P theta is the same as P data.
20:03 - Then these two samples would come
20:06 - from the same distribution.
20:08 - So the classifier basically cannot do better than chance
20:11 - because you are literally just taking two groups of samples
20:14 - that come from the same distribution,
20:16 - and then there is no way to distinguish them because they
20:18 - are actually coming from the same distribution.
20:20 - So you cannot do better than chance.
20:22 - And so you would have a high loss.
20:27 - On the other hand, if the samples were very different,
20:30 - the classifier would do a pretty good job of separating them,
20:34 - and then in which case the loss would be small.
20:37 - And so based on that, we can come up
20:39 - with a statistic that would basically
20:41 - say based on the loss of the classifier,
20:43 - we're going to decide whether or not
20:45 - the samples are similar or not.
20:47 - So to the extent that you can separate them well using
20:50 - a classifier, then we say that they are different.
20:53 - If somehow they are all overlapping,
20:55 - and there is no way of coming up with a good decision boundary,
20:59 - then we we're saying, OK, then probably the two samples
21:02 - are similar.
21:03 - For now we're just optimizing phi,
21:05 - and that depends on both clearly.
21:07 - You are right when you optimize with respect to theta,
21:10 - you only care about the second term,
21:13 - and indeed the gradients will only involve that term.
21:16 - Yeah, so the notation here is saying--
21:20 - recall that we have a group of samples, S1,
21:23 - that are coming from P data, which is your training set
21:26 - or a mini batch of samples from P data.
21:29 - And then we have another group of samples S2
21:31 - that are coming from a model distribution P theta.
21:33 - And then we have some kind of objective function here
21:37 - that depends on the model and the discriminator.
21:42 - And what we're seeing is that the discriminator
21:45 - is going to try to optimize this quantity which depends
21:50 - on the model and the discriminator,
21:53 - and it's going to try to maximize it.
21:55 - And that's equivalent to basically trying
21:57 - to do as well as it can at distinguishing
22:00 - real samples from fake samples which are coming from P theta.
22:06 - The reason we have this V is that we're
22:10 - going to also try to then optimize
22:11 - this function with respect to theta
22:13 - because we want to train the generative model.
22:15 - And so what we will show up later
22:17 - is basically a minimax optimization problem
22:20 - where we're going to optimize this V both with respect
22:25 - to theta and with respect to phi.
22:27 - So there's going to be a competing
22:29 - game where the discriminator is trying
22:31 - to optimize this V quantity.
22:34 - It is trying to maximize this quantity,
22:36 - and the model is going to try to minimize that quantity
22:42 - because the model is-- we're trying to make it--
22:44 - we're trying to change P theta to fool the discriminator
22:49 - or try to make the classification problem as
22:51 - hard as possible.
22:52 - So later there will be an outer minimization with respect
22:56 - to theta.
22:57 - And that's how we train the model.
22:58 -
23:03 - Cool.
23:04 - And it turns out that the optimal discriminator actually
23:10 - has this form, which makes sense.
23:16 - This is just the--
23:17 - if you just use Bayes rule, and you compute--
23:20 - what is the true conditional probability of a point X
23:27 - belonging to the positive class, which in this case
23:30 - is, let's say, the data distribution real samples?
23:34 - Well, the true conditional distribution
23:36 - is basically the probability that, that point
23:39 - was generated by the data distribution divided
23:42 - by the total probability that, that point was actually
23:44 - generated by either the model or the real data distribution.
23:52 - And so in particular, you can see
23:54 - that if X is only possible according to the data
24:03 - distribution, then the model should assign--
24:06 - the optimal discriminator would assign one
24:09 - because you have basically one over one plus zero,
24:12 - and then it would be one.
24:15 - While, for example, if the two models are the same,
24:20 - so if a point is equally likely to come from P data or P theta,
24:24 - then this quantity should be one half, which makes sense
24:28 - because if X is equally likely under P theta and the P data,
24:34 - then the best you can do is to say one half probability.
24:38 - While if a point is much more likely to have come from P data
24:42 - because this ratio is large, then the classifier
24:45 - should assign high probability to that point.
24:48 - And if a point is very unlikely to have come from P data
24:52 - because the numerator is small, then the classifier
24:56 - should assign low probability to that point.
24:58 - It's true that the KL divergence is
25:02 - going to be minimized when the two distributions are the same.
25:05 - And that's the global optimum of the KL divergence.
25:08 - So whatever we do, we're still going
25:10 - to go towards that global optimum.
25:12 - In practice, you cannot actually reach it.
25:14 - And so really what matters is that if you have an imperfect
25:18 - model, so you cannot really achieve this,
25:22 - the KL divergence will take some non-zero value.
25:25 - This quantity might take some other non-zero value.
25:30 - The argument could be that perhaps among the suboptimal
25:34 - models, you should prefer one that cannot fool
25:40 - a discriminator as opposed to one that gives you high
25:42 - compression because maybe that's more aligned to what you care
25:46 - about.
25:47 - There are actually variants of way of training generative
25:52 - models that are along those lines
25:55 - where we're going to talk a little bit about that
25:58 - when we talk about noise contrastive estimation.
26:00 - I think is going to be pretty similar to what
26:03 - you're suggesting.
26:04 - So if you have access to a likelihood or part
26:07 - of the likelihood, then you can take advantage of it
26:09 - and try to design.
26:11 - But then that defeats the purpose.
26:13 - As what we'll see is that the main advantage of this
26:16 - is that you don't have to have access to a likelihood.
26:18 - The only thing you need is to be able to sample
26:20 - from the model efficiently, which
26:22 - means that you can use essentially an arbitrary neural
26:25 - network to define the generative process, which
26:28 - is a big advantage of this kind of procedure.
26:31 -
26:37 - Yeah, that's what I was saying.
26:38 - If you check, you can see that if P theta is equal to P data,
26:42 - then this quantity is going to be one half for every X, which
26:46 - basically means that the best you can do,
26:50 - the classifier will output 0.5 for every X, which is indeed
26:55 - the best you can do.
26:57 - If the distributions are the same,
26:58 - you cannot possibly do better than chance.
27:01 - So this is when the classifier is maximally confused,
27:04 - basically.

00:00 -
00:04 - SPEAKER: So now, how do we get the next step?
00:09 - How do we use--
00:10 - now that we've decided that that's
00:12 - going to be our notion of the way
00:15 - we're going to compare how similar basically pdata is
00:19 - to p theta, now we can define a learning objective where
00:24 - we try to optimize p theta to basically fool
00:27 - the discriminator.
00:29 - And so it's going to be like a game.
00:31 - It's going to be a minimax optimization
00:34 - problem between a generator, which
00:36 - is just your generative model, and this discriminator,
00:39 - this classifier.
00:41 - And the generator is just going to be a generative model
00:48 - typically that basically looks like a flow model in the sense
00:51 - that you start with a latent variable z.
00:55 - And then you map it to a sample through
00:59 - some deterministic mapping, which is parameterized
01:04 - by a neural network.
01:05 - And we're going to call it G theta.
01:06 -
01:09 - So the sampling procedure is the same as a flow model.
01:11 - You sample z from a simple prior, for example, a Gaussian.
01:15 - And then you transform it through this neural network.
01:18 - And crucially, this is similar to a flow.
01:23 - But the mapping does not need to be invertible.
01:25 - It can be an arbitrary neural network.
01:27 - It's an arbitrary sampler.
01:28 - You start with some random vector.
01:31 - And you transform it into a sample,
01:33 - no restrictions on what G is.
01:36 - You could use any generative model, yeah.
01:38 -
01:42 - We'll see that it's actually convenient.
01:44 - Well, to train it, it would be good
01:46 - if you can sort of backprop through the generative process.
01:52 - But to some extent, you can indeed
01:55 - use other generative models.
01:57 - But the advantage is that basically you
02:00 - don't have any restrictions on this neural network, right?
02:03 - So we don't have to--
02:06 - there is going to be some distribution over the outputs
02:09 - of this neural network.
02:11 - But we're not ever going to compute it.
02:15 - So unlike autoregressive models, or flow models,
02:18 - or VAE where we were always very worried about being
02:21 - able to compute, given an x, what
02:23 - was the likelihood that my model produces that particular x?
02:28 - For these kind of models, we don't even care,
02:30 - because we're going to use two sample tests to compare,
02:33 - to train them.
02:34 - And so we don't need to be able to evaluate likelihoods.
02:38 - And so we don't have any restriction basically on what
02:43 - this sampling procedure does.
02:44 - It can essentially be any.
02:45 -
02:49 - And what we do then is we're going
02:52 - to train this generator to do the opposite basically
02:57 - of the discriminator.
02:59 - The generator is going to try to change this mapping, which
03:04 - implicitly also changes the kind of samples
03:07 - it produces to try to minimize this statistic that we were
03:12 - using in support of the fact that this null hypothesis that
03:17 - says the data is equal to the distribution of samples
03:21 - that I get by using this model.
03:24 - And so the end result is this.
03:27 - You have this minimax optimization problem
03:30 - where the function V is the same as this--
03:33 - basically the loss or the negative loss of the classifier.
03:38 - And then these two players in the game, the generator
03:43 - and the discriminator, they have opposing objectives.
03:46 - The discriminator is going to try to maximize this,
03:49 - which, again, this is the same as what we had before.
03:52 - This is just saying classifier.
03:54 - The discriminator is trying to do
03:56 - as well as it can to distinguish samples coming
03:58 - from the data, two samples coming
04:01 - from this generative model from this generator.
04:05 - And the generator is going to do the opposite.
04:07 - It's going to try to minimize this objective function, which
04:12 - basically means the generator is trying
04:14 - to confuse the classifier as much as it can.
04:18 - So it's going to try to produce samples
04:20 - such that the best classifier you can throw at that--
04:25 - when you compare them to the data distribution,
04:27 - the best classifier is going to perform poorly,
04:31 - which supports the fact that if a classifier cannot distinguish
04:36 - the samples I produce from the samples that are in the data
04:39 - set, then I probably have pretty good samples.
04:43 - And that's sort of the training objective
04:46 - that we're going to use for training
04:49 - this class of generative models.
04:52 - Now, it turns out that this is related
04:54 - to a notion of similarity between probability
04:58 - distributions.
05:00 - We know that what the optimal discriminator is,
05:04 - it's just the density ratio, pdata over pdata plus p model
05:08 - basically.
05:09 - And we know that basically the optimal discriminator
05:14 - is going to depend on what the generator does.
05:17 - And we can evaluate the value of this objective function
05:21 - when basically the second player, the discriminator,
05:24 - is picking the best possible thing it
05:27 - can do given what the generator is doing because we
05:33 - know what that looks like.
05:34 - We know that when the discriminator is optimal,
05:39 - the discriminator is just going to give us this density ratios,
05:41 - pdata over pdata plus p model.
05:44 - So we can plug it into this expression.
05:47 - And we get this sort of equation.
05:50 - So this is the optimal loss that you
05:53 - get by choosing-- whenever you choose generator G,
05:59 - if the classifier picks-- if we pick the best classifier given
06:02 - the G and given the data distribution,
06:04 - this is the value of that objective function.
06:08 - This ki nd of looks like a KL divergence.
06:12 - It's an expectation of a log of some density ratios.
06:17 - Remember, KL divergence is expectation
06:20 - under p of log p over q.
06:22 - This kind of has the flavor.
06:24 - Now, the denominators here are not probability distributions.
06:31 - They are not normalized.
06:33 - You have to divide by 2 if you want to get a something that
06:36 - integrates to 1.
06:39 - But you can basically just divide by 2 here and here
06:42 - and then subtract off that logarithm of 4
06:46 - that you just added in the denominators.
06:49 - And now this is really just two KL divergences.
06:54 - You can see that this is the KL divergence between the data
06:58 - and a mixture of data and model.
07:02 - And this is KL divergence between model and a mixture.
07:06 - The same thing, mixture of data and model.
07:08 -
07:11 - And then it's shifted by this log
07:13 - 4, which is just because I had added these two here and here.
07:17 - And so you need a log 4 there to make it equal.
07:21 - And so what this is saying is that this objective,
07:25 - as a function of the generator, is
07:29 - equal to this sum of KL divergences,
07:32 - which actually has a name.
07:34 - It's called the Jensen-Shannon divergence
07:38 - between the data distribution and the model distribution.
07:44 - So this thing is essentially two times
07:47 - this quantity called the Jensen-Shannon divergence, which
07:52 - is also known as symmetric KL divergence.
07:56 - If you look at that expression, it's
07:58 - basically that-- if you want to compute
08:00 - this Jensen-Shannon divergence between p and q,
08:02 - you basically do the KL divergence between p
08:05 - and a mixture of 1/2 p and 1/2 q.
08:08 - And then you do the reverse KL divergence between q
08:11 - and a mixture of p and q.
08:15 - And this has nice properties.
08:18 - We know KL divergence is nonnegative.
08:20 - Sum of two KL divergences also has to be nonnegative.
08:25 - What is the global optimum of this?
08:28 - When can it be zero?
08:29 -
08:32 - Yeah, so it also has the nice property
08:34 - that the global optimum can be achieved if
08:36 - and only if basically the distributions are the same.
08:40 - It's symmetric, which is nice.
08:42 - Remember, KL divergence was not symmetric.
08:44 - The KL p, q is not the same as KL q, p.
08:47 - This is symmetrized basically by definition.
08:52 - And it also has triangle inequality but not
08:54 - super important.
08:56 - And so what this means is that somehow you
09:02 - can optimize this quantity here as a function of G.
09:09 - So if you minimize this V as a function of G,
09:11 - which is what you do here on the outside,
09:15 - you will basically choose a model distribution that matches
09:21 - the data distribution exactly.
09:24 - So the global optimum is the same as what you
09:26 - would get with KL divergence.
09:29 - And you would get that optimal loss.
09:33 - So the summary is that basically as a recap, what we're doing
09:37 - is we're changing the way we're comparing the data distribution
09:41 - and the model distribution.
09:43 - And we choose this similarity based on a two-sample test
09:48 - statistic.
09:49 - And the statistic is obtained by optimizing a classifier
09:52 - and under ideal conditions so that the classifier can
09:55 - basically be optimal, which in practice is not
09:58 - going to be because if you use a neural network,
10:00 - it might not be able to learn that density ratio.
10:04 - But under ideal conditions, this basically
10:07 - corresponds to not using KL divergence here and instead
10:11 - using this Jensen-Shannon divergence to compare
10:15 - the data to the model.
10:16 -
10:19 - And the pros is that to evaluate the loss and optimize it,
10:24 - you only need samples from p theta.
10:26 - You don't need to evaluate likelihoods, which is great
10:29 - because that means you don't have restrictions
10:31 - on autoregressive, normalizing things, normalizing flows.
10:35 - You don't have to worry about it.
10:38 - Lots of flexibility in choosing the architecture
10:40 - for the generator.
10:41 - Basically, it just has to define a valid sampling
10:44 - procedure, which is essentially always the case.
10:47 - If you feed in random noise into a neural network,
10:49 - you get a sampling procedure.
10:51 - A valid sampling procedure, that's really all you need.
10:54 - And it's fast sampling because you
10:57 - can generate a sample in a single pass
10:59 - through the generator.
11:01 - So it's not like autoregressive models, one variable at a time.
11:03 - Everything is generated in a single pass.
11:07 - The con is that it's very difficult to actually train
11:11 - in practice because you have this minimax sort
11:14 - of optimization problem.
11:17 - And so in practice, what you would have to do
11:20 - is you would have to do something like this.
11:22 - You would have to, let's say, start
11:24 - with a minibatch of training examples.
11:28 - And then you get a sample of noise vectors
11:31 - from the prior of the generator.
11:34 - And then you pass them through--
11:37 - you pass these noise vectors through G
11:41 - to generate m fake samples.
11:45 - And then you basically have these two minibatches .
11:49 - You have m real data points and m fake data
11:52 - points, which is what you get by passing zi
11:55 - through the generator.
11:58 - And then what you do is you try to optimize
12:01 - your classifier, your discriminator
12:03 - to maximize that objective.
12:06 - So the classifier, which is just the usual training
12:09 - of a classifier, just try to--
12:11 - you take a step in a gradient ascent in this case
12:16 - step on that objective function to try
12:18 - to improve this optimization objective to do better,
12:25 - basically classifying a distinguishing real data
12:28 - from fake data.
12:31 - And as it was mentioned before, then the generator
12:39 - is also trying to--
12:42 - is also looking at the same objective function.
12:44 - But it has an opposite objective.
12:46 - The generator is trying to minimize
12:48 - that objective function.
12:49 - You can still do gradient descent.
12:52 - And what you do is you compute the gradient
12:55 - of this quantity with respect to theta, which
12:58 - are the generator parameters.
13:00 - And the first term does not depend on theta.
13:03 - That's just the data.
13:04 - So you cannot change what the data looks like.
13:07 - But what you can do is you can try
13:09 - to adjust the parameters of G, so
13:13 - the parameters of the generator, so that the samples that you
13:16 - produce by passing random noise through G
13:21 - look like the real data as measured by this discriminator d
13:26 - phi, which is what you get by taking
13:28 - this kind of gradient descent step on that objective.
13:32 - Yeah, so it's, unfortunately, very tricky to train this.
13:37 - And this is not guaranteed to converge.
13:40 - And it can be--
13:43 - in practice, you would use the new phi.
13:46 - And you would do-- you would keep
13:47 - going and trying to kind of play this game where each player is
13:52 - trying to play a little bit better and hope
13:53 - that it converges.
13:55 - You repeat this and hope that something good happens.
14:00 - And here is kind of a visualization
14:03 - of what happens if you do this.
14:05 - So what's happening here is you can
14:08 - imagine there is a bunch of z vectors
14:10 - that are then mapped by G to different x locations.
14:15 - So here z and x are just one dimensional.
14:18 - And that is giving us a distribution,
14:20 - which is this green curve.
14:22 - So you see that most of the samples from the generator
14:24 - end up here.
14:26 - And then there is a data distribution, which is just
14:28 - this red curve that's fixed.
14:31 - It is whatever it is.
14:33 - And then let's say you start with a discriminator, which
14:36 - is not very good.
14:37 - And it's this wiggly kind of blue line here.
14:41 - Now, given that you have a bunch of red samples
14:44 - and you have a bunch of green samples,
14:46 - so real samples and fake ones coming
14:49 - from the current generator, what you would do
14:53 - is you would try to come up with a good classifier.
14:58 - And the better classifier that you
15:00 - get after you update the discriminator
15:02 - is this blue curve.
15:04 - So as you can see, if x's are coming from this left side,
15:08 - then they're probably coming from the real data distribution.
15:12 - There is no chance they come from the generator
15:14 - because the generator has very low probability here.
15:17 - The data is pretty high.
15:18 - And so the discriminator should say samples around here
15:22 - should have high probability of being real.
15:24 - Samples around here should have high probability
15:26 - of being fake or low probability of being real.
15:30 - And then here in between, it's unclear.
15:34 - And then you just decrease the probability
15:35 - as you move towards the right.
15:39 - So that's what happens when you optimize phi.
15:41 - You basically come up with a better discriminator.
15:45 - Once you have the better discriminator,
15:47 - you can try to update the generator
15:50 - to fool this discriminator.
15:53 - So what would happen is you would change these arrows here,
15:57 - which are basically the G, which is telling you
15:59 - how you map the random noise from the prior, the z,
16:03 - to x's that you like.
16:06 - And in particular, if you're trying
16:09 - to fool the discriminator, you are
16:11 - going to try to shift probability mass to the left.
16:16 - And so you might end up with a new generator
16:18 - that looks like this.
16:19 - And that confuses the discriminator more
16:24 - because you can see it's overlapping
16:26 - more with the red curve.
16:29 - And the discriminator is going to have
16:31 - a hard time trying to distinguish these two samples.
16:34 - And then you keep going until you reach hopefully
16:37 - this convergence where the discriminator--
16:40 - where the generator matches the data distribution.
16:43 - So the green and the red curves are overlapping.
16:45 - They're identical.
16:46 - And the discriminator is maximally confused and is
16:49 - producing 1/2 everywhere because it cannot do better than chance
16:53 - at that point.
16:54 - As part of what you need to do, the job of the discriminator
16:57 - is to basically look at a bunch of samples like these two.
17:02 - And you need to decide which one is real
17:05 - and which one is fake essentially.
17:08 - So which one do you think is real?
17:09 - And which one is--
17:10 - both are fake
17:11 - [LAUGHTER]
17:14 -
17:18 - And this these technologies improved a lot over the years,
17:24 - like you can see from 2014, all the way to 2018.
17:28 - And there are even better improvements now.
17:31 - Very successful in a lot of tasks.
17:34 - But it's very challenging in practice
17:36 - to get them to work because of the unstable optimization.
17:39 - There are several problems with GANs.
17:41 - The first one is unstable optimization.
17:44 - Because you have this minimax objective,
17:46 - it's very tricky to train them.
17:48 - It's very hard to even know when to stop because it's
17:51 - no longer like likelihood.
17:52 - And you can see it going down.
17:54 - And at some point, you can just stop or up,
17:57 - let's say, or you're maximizing likelihood.
18:01 - You see it goes up.
18:01 - And at some point, you see it's not improving anymore.
18:04 - You can stop.
18:04 - Here is no longer the case that you know when to stop basically.
18:08 - And it can have this problem called mode collapse, which
18:11 - we'll see basically.
18:15 - While the KL divergence is mode covering,
18:17 - we'll try to put probability mass everywhere
18:19 - because otherwise, you get a huge penalty.
18:21 - If something is possible under pdata
18:24 - but you put zero probability, then you have infinite penalty.
18:27 - This GAN tend to be much more mode-seeking.
18:30 - And so they might just focus on a few types of data points
18:34 - and completely stop generating other kinds of data points
18:38 - that are present in the training set.
18:41 - And so in practice, you need a lot of tricks
18:43 - to train these models.
18:46 - And I'm going to point you to some reference for how this--
18:50 - yeah, where you can see some of them.
18:52 - I mean, in theory, under some very unrealistic assumptions,
18:56 - that kind of procedure where you do updates on the discriminator
19:02 - and the generator or at every step or you find the optimal
19:05 - discriminator is supposed to work, in practice, it doesn't.
19:11 - In practice, what you see is that the loss keeps
19:13 - oscillating during training.
19:15 - So it might look something like this
19:18 - where you have the generator loss, which
19:22 - is the green one, the discriminator loss, and the two
19:25 - types of samples, the real and the fake ones.
19:27 - You can see it keeps oscillating because you are not
19:33 - reaching the--
19:35 - yeah, it doesn't converge basically
19:37 - through this gradient procedure.
19:40 - And there is no robust stopping criteria.
19:43 - You don't know when should you stop.
19:45 - You don't really know.
19:46 - The only thing you can do is you look at the samples and see,
19:49 - OK, it's doing something meaningful,
19:51 - and then you just stop.
19:52 - But it's very hard to come up with a principled way
19:55 - of deciding when to stop the training.
19:58 - And so the other problem is that you have mode collapse, which
20:03 - is this problem that, again, the generator is
20:06 - basically collapsing on a few types of samples.
20:09 - And it doesn't generate the other ones.
20:12 - And you can see an example here where kind of like,
20:15 - if you look at the samples, it really
20:17 - likes this type of data points.
20:20 - And it keeps generating it over and over.
20:24 - And you can see a more toy example that gives you
20:27 - a sense of what's going on.
20:29 - Imagine the data distribution is just
20:32 - kind of a mixture of a bunch of Gaussians that are in 2D.
20:36 - And they are lying on this circle.
20:41 - And then what happens is that as you train your generator,
20:45 - it kind of keeps moving around.
20:46 - So maybe at some point, it produces one of the modes.
20:50 - And then the discriminator is doing a good job
20:53 - at distinguishing what it does from the real data.
20:56 - Then the generator is moving the probability mass
20:58 - on a different mode.
20:59 - And it keeps moving around.
21:01 - But it never actually covers all of them at the same time.
21:03 - Yeah.
21:04 - So there is all kinds of tricks that you have to use.
21:08 - And here is our example on MNIST where
21:10 - you can see how it's collapsing on generating
21:14 - one particular digit.
21:16 - And it stops learning.
21:18 - And there's this great blog post, GAN Hacks,
21:22 - where you can see all of hacks that you can use to get
21:27 - GANs to work in practice.
21:29 - And there is all kinds of techniques,
21:31 - including noise and various tricks
21:34 - that you can look up on the website.
21:37 - Unfortunately, it's all very empirical.
21:39 - There is nothing that is guaranteed to work.
21:41 - And you have to try.
21:43 - And there are better architectures.
21:45 - There are tricks that sometimes works and sometimes don't.
21:49 - But I would say the fact that these models are so hard
21:52 - to train is why they are no longer kind of state-of-the-art.
21:55 - And people have kind of largely given up on GANs.
21:58 - And people are using diffusion models
22:00 - instead because they are much easier to train.
22:03 - And they have a clean loss that you
22:06 - can evaluate during training.
22:08 - And you know how to stop.
22:09 - And there is no instability.
22:11 - It's just a single optimization problem that you have to do.
22:13 - And I would say this is the main reason GANs are no longer so
22:19 - much in fashion anymore.
22:23 - And people are still using them.
22:24 - And it's a powerful idea.
22:26 - And it might come back.
22:28 - But I think that's the main drawback, very, very
22:30 - hard to train.
22:32 - The discriminator is seeing the data.
22:33 - And then the generator is learning
22:36 - from what the discriminator has learned about the data
22:39 - essentially.
22:40 - Then depending on what kind of features
22:42 - the discriminator is looking for,
22:44 - you might try to catch up with those
22:47 - but then just keep changing.
22:48 - And then you never really converge to anything.
22:50 - In fact, there is even recent papers at ICML this year
22:54 - where they were taking a diffusion model.
22:55 - And then they had a clever way of incorporating
22:57 - basically a discriminator to improve performance.
23:01 - And often if you can throw in maybe a little bit
23:05 - of discriminator to kind of compare
23:08 - samples in a meaningful way, that often helps.
23:14 - That's why we're still talking about this idea
23:15 - because it's actually powerful.
23:17 - And you can use it in combination
23:19 - with other existing models.
23:21 - And yeah, it's still valuable for other things.
23:24 - This, I think, was the first model generated art.
23:31 - It was auctioned at Christie's a few years ago.
23:35 - This is a painting generated kind of by a GAN, one
23:39 - of the best one at that time.
23:41 - I think it was expected to sell for something in that range.
23:44 - And I think it went for almost half a million dollars.

00:00 -
00:05 - SPEAKER: The plan for today is to continue
00:07 - talking about generative adversarial networks.
00:11 - As a recap, remember that the nice thing
00:15 - about generative adversarial networks
00:18 - is that it allows us to train models
00:21 - in a likelihood-free way, which basically means that you no
00:26 - longer have to choose special architectures
00:30 - or factorize a distribution according to chain rule
00:35 - because you're forced to be able to evaluate
00:38 - the probability of each data point
00:39 - because you want to train by maximum likelihood.
00:43 - The idea is that there are ways to basically compare
00:47 - the probability distribution of your generative model
00:50 - to the data distribution, that it does not
00:54 - involve KL divergence, and does not require basically you having
00:59 - to evaluate the probability of samples according to your model.
01:02 - And in particular, we've seen that there
01:04 - is one very reasonable way of figuring out
01:09 - how well your generative model matches a data distribution.
01:14 - And that involves basically training a classifier.
01:17 - And the classifier is often called
01:19 - a discriminator in this space.
01:21 - And the discriminator is supposed to--
01:24 - is trained to distinguish kind of whether or not the samples
01:29 - it's receiving are real, meaning they come from the data
01:32 - distribution, or fake, meaning they come
01:35 - from the model distribution.
01:36 - And you can think of the performance of this classifier
01:41 - as an indicator of how well your generative model has been
01:48 - trained or how similar the samples it produces
01:51 - are to the real data distribution.
01:54 - If the discriminator is having a very, very hard time
01:57 - distinguishing your samples from the real ones,
01:59 - there is a good chance that your samples are actually
02:02 - pretty good.
02:03 - And so based on this intuition, we
02:06 - have this kind of training objective here
02:09 - which involves a minimax game.
02:12 - So it's kind of an optimization problem
02:14 - where there are two players.
02:15 - There is a generator that is trying to produce samples.
02:18 - That's your generative model.
02:20 - There is a discriminator, D, that
02:22 - is trying to distinguish real samples from phase samples.
02:25 - And there is this performance metric, which is basically
02:29 - the loss of the discriminator.
02:33 - It's just basically the negative cross-entropy loss
02:36 - of the discriminator on this task
02:38 - of distinguishing real versus fake.
02:40 - And you have this minimax game where the discriminator
02:42 - is trying to do as well as it can
02:45 - in this classification problem, binary classification problem.
02:48 - And the generator is trying to make the discriminator perform
02:53 - poorly.
02:54 - So they're playing kind of the game.
02:57 - And this is like a minimax game in the sense
02:59 - that they're pushing the objective function
03:01 - into different directions.
03:02 - And the generator is being trained
03:04 - to try to fool basically the discriminator, trying
03:07 - to produce samples that are as close
03:10 - as possible to the ones in the data
03:12 - as measured by a discriminator not being able to distinguish
03:16 - these two types of samples.
03:19 - And we've seen that under some assumptions.
03:22 - So if you assume that somehow you
03:26 - are able to compute the optimal discriminator,
03:29 - recall that optimal discriminator is basically
03:31 - giving you density ratios.
03:34 - And if you plug that optimal discriminator into this loss
03:39 - function, then you get a mixture of two types of KL divergences.
03:44 - And we've seen that, that divergence as a name
03:47 - is called the Jensen-Shannon divergence.
03:49 - And up to scaling and shifts, you
03:52 - can think of this training objective
03:55 - from the perspective of the generator.
03:58 - Assuming the discriminator is optimal,
04:01 - you can think of this as trying to minimize
04:03 - this Jensen-Shannon divergence between the data distribution
04:06 - and the model distribution.
04:09 - And so this is not too different from
04:12 - traditional maximum likelihood learning
04:14 - where we're minimizing KL divergence
04:16 - between the data and the model.
04:18 - Under these assumptions, you're trying
04:20 - to make to instead minimize some mixture of KL divergences
04:26 - that are basically between the data and mixtures of models
04:29 - and data.
04:30 - This restriction scaling is just it
04:32 - happens to show up if you define the loss this way.
04:37 - It just happens to be the case that Jensen-Shannon divergence
04:39 - is defined that way.
04:40 - And it doesn't have this--
04:42 - this is kind of optimal loss that you can have.
04:44 - Not super important.
04:45 - It's just like if you've worked through the math,
04:47 - you get a shift in scale.
04:48 - But, yeah, we don't care about--
04:50 - of course, the loss is the same basically.
04:55 - You're just changing the landscape by shifting it.
04:57 - So it doesn't really matter.
04:59 - It just happens to show up if you do the derivation.
05:04 - And in practice, of course, this is not feasible in the sense
05:11 - that you cannot get the optimal discriminator.
05:13 - But in practice, what you would do
05:15 - is you would have two neural networks, a generator,
05:18 - and a discriminator, and they play this game.
05:20 - And then the generative distribution
05:25 - is defined as what you get by transforming simple samples
05:29 - from a prior distribution like a Gaussian
05:32 - through the generator network, and then
05:35 - you just optimize this sort of objective function.
05:38 - And there's been a lot of success based on this paradigm.
05:44 - This is a cool kind of repo where
05:47 - you can see a lot of different GANs and variants
05:50 - of these ideas that have been proposed in the past.
05:54 - And what we're going to see today
05:57 - is that this idea of setting up a minimax game
06:01 - is actually very powerful.
06:03 - And not only you can use it to minimize
06:05 - the Jensen-Shannon divergence, but you can actually
06:08 - use it as a tool that under some conditions
06:14 - allows you to optimize a much broader class of divergences
06:18 - between the data and the model distribution,
06:20 - something called f-divergences.
06:22 - And we'll see that there is also another extension
06:28 - or similar framework that allows you to approximately minimize
06:34 - some notion of the Wasserstein distance between model and data
06:38 - distribution.
06:39 - And we'll also see how to get latent representations
06:44 - from generative adversarial networks.
06:46 - So similar to a VAE, we'll see to what extent
06:49 - it's possible to essentially not only generate samples
06:53 - but also map samples to latent representations
06:57 - then you can use perhaps on to do semi-supervised learning
07:00 - or use them on other kinds of downstream tasks.
07:03 -
07:06 - And then we'll also see maybe CycleGANs that
07:09 - are sort of like conditional generative adversarial networks,
07:12 - are also pretty cool.
07:14 - All right, so first, let's go back to the high level picture.
07:19 - Again, remember that we've been in the first part of the course.
07:25 - We were always kind of choosing this divergence between the data
07:28 - and the model to be the KL divergence which plays nicely
07:32 - with kind of likelihood-based models.
07:34 - If you can evaluate probabilities under your model
07:37 - comparing similarity in terms of KL divergence,
07:40 - makes a lot of sense.
07:41 - And we know that that's optimal in a certain sense.
07:46 - We've seen that to some extent you
07:48 - can optimize the Jensen-Shannon divergence through the GAN
07:52 - objective.
07:53 - And what we'll see today is that you can actually
07:56 - optimize a broader class of divergences that
07:59 - are called the f-divergences.
08:01 - And an f-divergence is defined as follows.
08:05 - So if you have two densities p and q,
08:09 - you can define a divergence between them
08:11 - by looking at the expectation with respect
08:15 - to the second argument, which is q, of this f function applied
08:21 - to the density ratio between--
08:24 - at each point between p and q where f is basically a function,
08:31 - a scalar function that has to be convex, lower semicontinuous,
08:37 - and it has to evaluate to 0 when you plug-in 1.
08:41 - And as you change basically this f function,
08:45 - you get different ways of basically comparing
08:48 - how similar p is to q.
08:52 - And just to be precise, what these technical conditions mean,
08:58 - well, one is that you have the function
09:01 - f has to be convex, which hopefully you
09:03 - know what it means.
09:04 - It means that if the graph basically--
09:08 - if you take two points, and you connect them,
09:10 - that line is above.
09:11 - So the graph of the function has to be
09:13 - lower semicontinuous, which is just a very technical thing.
09:16 - It basically means something similar to continuous.
09:19 - And if it's discontinuous, then kind of
09:23 - on one of the directions, then it
09:25 - has to be above the value of the function
09:30 - where there is a discontinuity, not super important.
09:34 - But the intuition is that somehow f
09:38 - is what tells you how much you're being penalized when
09:44 - p and q are assigned different probabilities to one
09:47 - of the possible things that can happen, let's say,
09:50 - one of the possible images.
09:52 - So it's similar in some sense to KL divergence
09:54 - where remember what we were doing
09:56 - is we were going through all the possible things that can happen,
09:58 - and then we were looking at the ratio of probabilities assigned
10:02 - by p and q, and then we were taking some kind of log.
10:05 - This is a generalization in the sense
10:06 - that you can use different kind of convex functions
10:10 - to score how happy or unhappy you are with different density
10:14 - ratios.
10:15 - And ideally, if p and q are the same,
10:18 - then they are going to assign the same probability to every x.
10:22 - And so the density ratio is one.
10:24 - And then this function f is going
10:28 - to give you a penalty of zero.
10:30 - And that's the best that can happen.
10:34 - But f is essentially measuring how much you care about p and q
10:43 - assigning different probabilities
10:45 - to the various axes, to the various samples that
10:47 - can be generated by the model.
10:50 - And the interesting thing is that because f is convex,
10:58 - then you can still use the same trick
11:00 - that we did for KL divergence to basically show
11:02 - that this f-divergence is non-negative.
11:07 - And in particular because we have
11:09 - an expectation of a convex function of some density ratio,
11:13 - this is always at least as large as the function
11:16 - applied to the expectation.
11:18 - And now that expectation, you can expand it.
11:21 - It's basically the integral with respect
11:24 - to this probability distribution q of the density
11:28 - ratio, which is just p over q.
11:30 - So the q's simplify, and you're left with the integral of p. p
11:36 - is normalized.
11:37 - It evaluates to 1, that integral.
11:39 - And so this is f of 1, which is 0.
11:41 - And so you get the desirable property
11:45 - that basically this f-divergence is
11:49 - non-negative for any choice of p and q.
11:52 - And if you plug in p equals to q,
11:56 - then this density ratios here are always going to be 1.
11:59 - f of 1 is 0.
12:01 - And so this whole expectation is 0.
12:04 - And so it behaves similarly to KL divergence in the sense
12:07 - that it tells you how similar or different p and q are
12:12 - by kind of looking at all these density ratios
12:15 - and scoring them throughout.
12:17 - If the distributions are the same,
12:19 - then the density ratios, the two p and q
12:20 - are assigned exactly the same probabilities
12:22 - to everything that can happen.
12:23 - And that f-divergence is going to be 0.
12:27 - In general, it's going to be non-zero.
12:30 - It's going to be greater than or equal to zero.
12:32 - And so it makes for a reasonable objective
12:36 - function to try to minimize this quantity.
12:39 - So we could have one of p and q be the data distribution,
12:43 - the other one being the model distribution,
12:45 - and then we can try to minimize this as a function of the model.
12:51 - And if you-- the nice thing about the f-divergences
12:56 - is that if you plug in different types of f's, you
13:00 - get many existing reasonable kind of divergences
13:03 - that you might want to use to compare probability
13:05 - distributions.
13:06 - For example, if you choose f to be u log u,
13:12 - and you plug it into this formula,
13:16 - then you will see that this expression evaluates
13:21 - to the usual KL divergence where the way
13:24 - you compare two probability distributions p
13:27 - and q is by going through everything
13:29 - that can happen, look at the density ratios,
13:31 - and scoring them with respect to this log function.
13:35 - There are many other f-divergences.
13:37 - So the nice thing is that if you plug in different f's, you
13:41 - get different divergences.
13:44 - So we have the Jensen-Shannon divergence
13:46 - which you get by choosing, for example, this odd-looking choice
13:50 - of u.
13:51 - You can get the usual KL divergence.
13:54 - By choosing u, you can get the reverse KL divergence
13:58 - where you basically swap the argument of p and q
14:02 - and the regular KL divergence.
14:03 - By choosing minus log u as the function f,
14:07 - and you can get many more.
14:08 - You can get-- you can see here squared
14:12 - Hellinger, total variation, alpha
14:16 - divergences, a lot of different kind
14:17 - of ways of comparing similarities between p and q
14:21 - by choosing a different f function.
14:25 - And what will turn out to be the case is that generative
14:29 - adversarial network, like objectives,
14:31 - cannot only be used to minimize an approximate version
14:35 - of the Jensen-Shannon divergence which corresponds to a very
14:38 - particular choice of f, but it can actually be used to optimize
14:42 - all of them.
14:43 - So you can pick any f which satisfies
14:48 - those constraints, that defines a valid f divergence.
14:51 - And what we'll see is that we can use a GAN-like objective
14:54 - to minimize the corresponding f-divergence approximately.
14:59 - The basic setup is that as usual, we're
15:03 - trying to train a generative model.
15:05 - So we have a data distribution.
15:06 - We have a model distribution.
15:08 - And it would be nice if we could choose an f,
15:11 - and then either minimize the f-divergence between the model
15:16 - and the data or perhaps the f-divergence between data
15:19 - and model.
15:20 - Now, this is reasonable because we've
15:22 - seen that for any choice of f that satisfies
15:25 - those constraints, this objective function is
15:28 - non-negative and is going to be zero if the two arguments match.
15:33 - So if your generative model matches the data distribution,
15:36 - then this loss function is going to be zero, is non-negative.
15:40 - And so if you set up a learning objective
15:43 - where you try to minimize this as a function of data,
15:45 - you might be able to learn a good generative model.
15:50 - Now, the issue is that the expression, kind
15:54 - of when we started looking at KL divergence the first time,
16:01 - doesn't look like something you can actually optimize.
16:04 - It doesn't look like something you can evaluate
16:06 - and doesn't look like something you can actually
16:08 - optimize as a function of data.
16:10 - First of all, you have an expectation outside with respect
16:13 - to, let's say, the data distribution.
16:16 - Well, we don't know p data, but we have access to samples.
16:19 - So we can approximate that expectation
16:21 - with a sample average.
16:22 - So that's not a problem.
16:24 - The real problem is that it requires
16:28 - you to evaluate the probability of x under the model
16:32 - and under the data distribution.
16:35 - And even if you have a likelihood-based model,
16:38 - even if you can evaluate p data, we
16:40 - can never evaluate probabilities under the data distribution.
16:44 - And so that density ratio is unknown.
16:48 - So like in the KL divergence case where
16:50 - we have that log density ratio, and we couldn't actually
16:53 - evaluate it, and we could only actually
16:55 - optimize KL divergence up to a shift, up
16:58 - to the entropy of the data, we have the same problem
17:01 - here that this kind of objective function
17:04 - seems reasonable but doesn't look like something
17:07 - we can actually optimize.
17:09 - And if you try to swap, you try, OK, maybe we can do--
17:13 - instead of doing f-divergence between p data and p data,
17:16 - we could try to do p data to p theta,
17:19 - and you end up with something similar.
17:21 - We have, again, an expectation with respect to samples drawn
17:25 - from the model, which is fine.
17:27 - But, again, you have this density ratio
17:29 - that is not something we can compute in general,
17:33 - even if you have a likelihood-based model.
17:35 -
17:37 - And so what we need to do is we need to somehow rewrite this
17:44 - f-divergence or approximate this expression and write it
17:48 - into something that ideally does not depend on either
17:53 - the probabilities--
17:55 - basically does not require you to be
17:57 - able to evaluate probabilities under the data distribution
18:00 - and ideally not even according to the model distribution.
18:04 - Because if the objective function does not
18:06 - involve neither p theta nor p data,
18:10 - and it only requires you to be able to sample
18:12 - from both of them, then we're back in the setup just
18:18 - like a generative adversarial network
18:19 - where we can basically use any sort of architecture
18:22 - to define p theta implicitly as whatever you get if you were
18:28 - to sample from a simple prior, feed the samples
18:32 - through a neural network, which is the generator that
18:35 - defines a potentially very complicated p theta or x
18:39 - to the extent that we can write down
18:40 - the objective function in an equivalent way
18:43 - or approximately equivalent way that does not require
18:46 - us to evaluate probabilities,
18:48 - then we can use a very flexible network architectures,
18:53 - like in generative adversarial networks.
18:55 - The question is, OK, is p theta--
18:59 - p data x1?
19:01 - And in general, no.
19:03 - That's basically the probability that the model
19:06 - assigns to every possible x.
19:10 - So that's just an--
19:12 - there is an underlying, as usual, data generating process
19:16 - that we assume we have access to only through samples.
19:19 - So we assume we have a training set
19:21 - that was sampled from p data, and that distribution is not
19:26 - uniform.
19:27 - This is not the empirical distribution
19:29 - on the data set, which could be just like one over n
19:34 - where n is the size of the data set.
19:36 - This is the true data generating process.
19:39 - You could set it up trying to fit
19:41 - the empirical distribution on the data set,
19:44 - but it's not quite.
19:45 - You could even think of that as an approximation of p data
19:49 - where you have a very simple kernel density estimator based
19:52 - on the training set.
19:54 - But that doesn't work particularly well because
19:57 - in high dimensions, it's going to be-- it might not generalize.
20:00 - So you're overfitting too much to the training set.
20:04 - This machinery works if you can evaluate p theta.
20:10 - But as we know, evaluating p theta
20:14 - constrains you in the kind of models you can use.
20:16 - You have to then either use autoregressive models,
20:19 - or you have to use invertible neural networks, which
20:21 - is kind of undesirable.
20:23 - And if you could set up a learning objective where
20:25 - p theta is not even something that you have to evaluate,
20:28 - you just need to be able to sample from it.
20:30 - Then that opens up the possibility
20:33 - of using implicit models, like feed noise
20:38 - through a neural network like a simulator
20:41 - almost where you don't even need to know how it works.
20:44 - You don't need to know how it assigns probabilities
20:46 - to data points.
20:47 - You just need to be able to sample from it.
20:49 - So that opens up more of a kind of broader set of models,
20:56 - including these implicit ones where you just
20:59 - need to be able to sample from it essentially.
21:01 - Remember KL divergence is an expectation with respect
21:04 - to p of log p over q.
21:08 - Yeah, so you have to multiply by-- yeah, to basically change
21:12 - the expectation to 1 with respect to p.
21:15 - But if you see-- in fact, if you want reverse KL,
21:18 - then it's just minus log u because reverse KL
21:22 - would be an expectation with respect to the second argument.
21:25 - So the u in front is basically to change the expectation
21:28 - from 1 under q to 1 under p, basically.
21:33 - OK, so now let's see how we can actually move forward
21:38 - and come up with, again, like way
21:40 - of approximating this f-divergence that
21:43 - does not require likelihood.
21:46 - The reason we were able to do it for, I guess,
21:50 - Jensen-Shannon divergence is exactly what
21:53 - we're going to see now, which is basically a way
21:55 - to reduce this expectation which looks like something that you
22:01 - might not be able to compute.
22:03 - If you look at the Jensen-Shannon divergence,
22:05 - it looks like something you're not able to compute.
22:07 - But if you have an optimal discriminator,
22:10 - intuitively, the optimal discriminator
22:12 - computes these density ratios for you.
22:16 - And so that's how you get around it,
22:19 - like you are offloading this problem of computing the density
22:25 - ratios to a discriminator.
22:27 - And this might be good or bad.
22:29 - But the hope is that neural networks
22:31 - seem to work really well for kind of supervised learning
22:34 - classification problems.
22:35 - And so we might be able to come up
22:38 - with reasonable estimates of these density ratios
22:40 - by training a classifier because to do well on classification,
22:44 - if you're trying to classify real samples from fake samples,
22:48 - you essentially need to estimate that.
22:50 - The optimal classifier requires you
22:52 - to know for every x, how likely is this point to come
22:57 - from one versus the other?
22:58 - And so that's kind of the trick.

00:00 -
00:05 - SPEAKER: So the machinery for doing
00:08 - this goes through something called
00:11 - the Fenchel conjugate or the convex conjugate of a function.
00:16 - Which is defined like this if you have a function f.
00:21 - You can obtain another function f
00:24 - star which is called the convex conjugate of f
00:28 - by using the following expression.
00:33 - So f star is not going to be a function of t.
00:37 - And the value of f star at t is the solution
00:41 - to basically this optimization problem where you're
00:43 - taking the supremum over all the u's in the domain of f
00:48 - and then you have this relatively simple objective
00:52 - which is just ut minus f of u.
00:55 -
01:01 - Seems a little bit random.
01:04 - But this convex conjugate has a bunch of useful properties.
01:08 - In particular, it's a convex function.
01:12 - Even when f is not.
01:15 - And the reason is that the argument here in the supremum
01:19 - as a function of t is just basically
01:22 - a bunch of affine functions.
01:24 - It's just linear in t.
01:25 - And so the supremum of a bunch of convex functions
01:30 - is also convex.
01:32 - And so you can think of this as the supremum
01:34 - of a collection of functions that are indexed by u.
01:37 - And, but as a function of t, they are all very simple.
01:40 - They're just linear functions.
01:42 - And then when you take the supremum
01:43 - of a bunch of convex functions, you get something convex.
01:46 - The other interesting property that we're going to use
01:51 - is that we can look at the conjugate of the conjugate.
01:55 - Which we're going to denote as f star.
01:58 - Which is just what you get if you
02:00 - take the conjugate of the conjugate of a function f.
02:04 - And again, you basically just apply the same definition.
02:07 - But now the function f becomes f star.
02:12 - And it turns out that this convex conjugate
02:16 - is a lower bound to the original function f.
02:20 -
02:23 - So it's always less than or equal to f.
02:27 - And so the proof is actually very simple.
02:30 - You can kind of see that by the definition that we have up here.
02:34 - We have that for every choice of t,
02:35 - f star is at least as large as ut minus f of u.
02:40 - Because it's the supremum.
02:41 - So it has to be at least as large at all the possible values
02:44 - that you can get for any choice of u.
02:48 - And if you rearrange, you can move the f on the other side
02:52 - and you can write it as f of u is at least as large as ut
02:56 - minus f star.
02:57 - If you just move this and this on the other side.
03:01 - And now this definition means that f
03:04 - of u, because this holds for any t and for every u.
03:08 - Then it means that f of u is at least as large as the sup.
03:11 - The supremum of ut minus f star of t.
03:15 - The convex conjugate.
03:17 - Which is exactly the definition that we want, right?
03:19 - That's exactly the conjugate of the conjugate f double star.
03:24 - And so we see that this conjugate of the conjugate
03:31 - is always less than or equal to the original function
03:34 - that we started with.
03:37 - And it turns out that when f is convex, then this f,
03:44 - the conjugate of the conjugate is actually
03:47 - equal to the original function.
03:49 - If you start with a function, you
03:50 - can get the conjugate and then your conjugate again,
03:52 - you go back to the original function when f is convex.
03:57 -
04:02 - Now the reason this is going to be useful
04:04 - is that this is going to be similar to the ELBO
04:09 - or the evidence lower bound.
04:10 - What we're going to do is we're going
04:12 - to try to write down f in our definition of the f divergence
04:17 - in terms of the conjugate.
04:19 - And we're going to get bounds on the value of the f divergence
04:22 - by going through this characterization
04:25 - of the f function and an f divergence in terms
04:28 - of this convex conjugate.
04:32 - And so that's basically kind of the idea that
04:38 - underlies this framework for training generative models based
04:43 - on f divergences through a GAN like objective.
04:47 - So what we do is we have the original definition
04:50 - of the f divergence which depends on this density ratio
04:55 - that we don't have access to.
04:58 - We can equivalently-- because f is convex,
05:00 - we can equivalently rewrite this in terms of the conjugate.
05:06 - Which is just the conjugate of the conjugate f double star.
05:11 - Which by definition is just this supremum.
05:14 -
05:17 - Recall that we're evaluating f double star at the density
05:22 - ratio.
05:23 - So we can write f double star as the supremum of t argument
05:28 - minus f star evaluated at t.
05:31 -
05:35 - That's just the definition of the conjugate of the conjugate.
05:39 - And now this is starting to look like something a little bit more
05:44 - manageable because we see that the density ratio that before
05:47 - was inside the argument of this f
05:50 - function that we didn't know how to handle.
05:52 - Now it becomes a linear dependence on the density ratio.
05:57 - Like now except for this annoying supremum.
05:59 - Then the dependence on px or qx is outside the argument of f.
06:07 - Which will allow us to basically simplify things.
06:10 -
06:13 - Now what you can see is that for every value of x,
06:19 - there is going to be a different value of the density ratio
06:22 - and that is going to be a different value of t that
06:25 - achieves the supremum.
06:28 - And we can denote that supremum that you get for any--
06:34 - for any particular x as t star of x.
06:39 - So this is just the value of the supremum
06:42 - when we're looking at data point x.
06:47 - And this is going to be what the discriminator is
06:52 - going to do later on.
06:54 - But you see that now we have an expression that is not too bad.
06:58 - It's an expectation with respect to q that we know how
07:01 - to approximate using samples.
07:03 - And now we have the density ratio
07:06 - is outside the argument of this f function
07:10 - that we use to score them.
07:12 - And what this means is that basically, if you expand it,
07:17 - it will look something like this.
07:18 - The expectation with respect to q
07:20 - is just an integral where every x is weighted using q of x.
07:24 - And now if you simplify it further
07:29 - and you notice that this q of x simplifies with this q of x.
07:34 - This whole expression basically just
07:37 - looks like the difference of two expectations.
07:39 - That is an expectation with respect to p
07:42 - and there is an expectation with respect to q.
07:45 - But that's similar to what we had in the GAN framework
07:50 - where we had an expectation of something with respect
07:53 - to the data distribution.
07:54 - An expectation of something else with respect
07:57 - to the model distribution.
07:58 - And that was giving us our estimate
08:02 - of the Jensen-Shannon divergence in that case.
08:05 - You can see that the same sort of idea
08:08 - holds more generally for different choices of f.
08:12 - Supremum is the same as the max basically.
08:15 - It's just like, yeah, it's-- the domain does not necessarily
08:21 - exist the max.
08:21 - So it's a little bit of a technicality
08:23 - but think of it as the max basically.
08:27 - I'm just denoting it to star because this is basically the--
08:31 - but it's just a way of denoting the value.
08:36 - What this supremum over t evaluates to
08:40 - for any particular x.
08:43 - There's going to be a value of t that achieves the supremum.
08:48 - I'm just going to denote it, t star.
08:51 - So the good thing is that this is an expectation--
08:54 - I mean, it still--
08:55 - it looks like yeah, it still depends on p of x.
08:57 - But if you look at the formula, this
08:59 - is basically the way I have it.
09:02 - OK.
09:03 - Maybe it comes up later.
09:04 - But it's an expectation with respect to p of x.
09:06 - OK.
09:07 - And that you can approximate by taking samples which we have
09:10 - because you have a training set.
09:13 - Then the next step is that basically, equivalently, you
09:17 - can just say, well, there's going
09:18 - to be some function that we're going to call t.
09:21 - That gives you this optimal value of t star for every x.
09:26 -
09:29 - This doesn't change anything.
09:30 - Basically for every x there is an optimal choice of t
09:34 - which comes from the supremum.
09:36 - Here I'm denoting it t star.
09:38 - Equivalently, you can say, OK, there is a function
09:41 - t that takes x as an input.
09:43 - And gives you as an output.
09:45 - The supremum of that--
09:46 - of that definition of the convex conjugate.
09:52 - And then that's where you get the bound is you can say, well,
09:58 - I cannot--
10:00 - this would require you an arbitrarily, flexible function
10:03 - t that can take any x and map it to the solution
10:07 - to this optimization problem.
10:09 - Recall this has a little bit of the flavor of a VAE,
10:12 - amortized inference in a VAE, where
10:14 - you have this encoder that is supposed to take x as an input.
10:19 - And then map it to the optimal variational parameters,
10:22 - solving an optimization problem for you.
10:24 - This kind of has the same flavor.
10:26 - But we can say is well you can always optimize over
10:30 - a set of functions--
10:33 - an arbitrary set of functions a set of neural networks and that
10:37 - would give you a lower bound on this f divergence.
10:42 - So if instead of optimizing over all possible functions,
10:46 - you optimize over a set of neural network architectures
10:49 - that you're willing to consider.
10:51 - You're always going to get something that is less than
10:56 - or equal.
10:58 - Because you might not have sufficient flexibility
11:00 - for mapping x to the corresponding value
11:03 - t star of x that you would have gotten
11:05 - if you could actually solve this optimization problem exactly.
11:10 - But you definitely get a lower bound
11:12 - for any choice of this family of mappings
11:16 - that you use to map data points to essentially,
11:20 - something that looks like an estimate of the density ratio.
11:25 - And the more flexible, this family
11:27 - script to t of neural networks.
11:30 - You can choose then the tighter, this inequality is.
11:34 - So the better of an approximation
11:36 - you get to the true value of the f divergence
11:40 - that you started with.
11:44 - And back to your question.
11:45 - OK.
11:46 - Does this depend, it looks like this still depends on p
11:49 - and this one still depends on q.
11:51 - You notice that these two are just expectations
11:54 - with respect to p and q.
11:56 - Which in our case will be the data distribution and the model
12:00 - distribution.
12:01 - And so this is essentially the same as a GAN
12:05 - generative adversarial network training objective.
12:08 - Remember when you-- the objective
12:11 - that we were using for training a GAN, is the min over g.
12:14 - And then we had the max over the discriminator of something
12:18 - that looked a lot like this.
12:20 - So you were evaluating the discriminator
12:23 - on the data samples.
12:25 - You were evaluating the discriminator
12:26 - on the fake samples.
12:28 - And you were trying to kind of distinguish them, contrast them
12:32 - through the cross-entropy loss.
12:34 - And here we get something that has a very similar flavor where
12:39 - we're sort of evaluating this discriminator t over data
12:44 - samples, over model samples and we're
12:47 - trying to essentially distinguish them
12:49 - by maximizing that quantity.
12:53 - When we do this optimization over t in this script t,
12:57 - that's going to be where we optimize
12:59 - the discriminator or a critic.
13:00 -
13:03 - And this script t is going to be a family of neural networks
13:06 - that we're going to use to choose t from.
13:11 - If you want to have an exact estimate of the f divergence,
13:15 - then the discriminator has to be optimal.
13:18 - But if you don't then that you're going to get at least a
13:21 - lower bound.
13:22 - So the lower bound part holds even if the discriminator is not
13:27 - necessarily optimal.
13:28 - Yeah.
13:29 - It's a problem and it says that you're optimizing a bound.
13:31 - So it might or might not be the right thing to do.
13:37 - And this is a lower bound.
13:38 - So minimizing a lower bound might not
13:40 - be-- might not be going in the right direction.
13:44 - And so yeah, you still have those problems.
13:47 - So in that sense, it's approximately
13:49 - optimizing an f divergence.
13:51 - If you could somehow optimize over all possible
13:54 - discriminators.
13:55 - Then I guess you had infinite data
13:57 - and you were able to actually solve this optimization
13:59 - problem perfectly.
14:00 - Then you could really optimize an f divergence.
14:03 - But in practice no, there is always approximations.
14:07 - It's essentially computing the conjugate of the conjugate of f.
14:13 - And it corresponds to finding supporting hyperplanes.
14:17 - Which are encoding the graph of the function as a convex hull.
14:22 - And that optimization problem is trying
14:24 - to find essentially tangent to the graph of the function.
14:30 - So that's essentially what's going on
14:32 - in that optimization problem.
14:35 - That's what I was saying that in the outer optimization problem,
14:37 - you're going to be minimizing this.
14:39 - And then this is a bound that goes in the wrong direction.
14:42 - And unfortunately, getting upper bounds is much harder.
14:46 - There is work where people have tried to come up with bounds,
14:51 - especially, as it relates to--
14:53 - it turns out that you need to do something similar
14:55 - if you want to estimate mutual information between--
14:59 - between random variables.
15:01 - Which are also basically involves some estimating density
15:04 - ratios.
15:05 - And there is literature and trying to get bounds there.
15:09 - But nothing that works particularly well.
15:12 - It doesn't meet likelihood.
15:13 - And it achieves-- as we know, divergence
15:16 - is all based on compression.
15:18 - Which might or might not be what you want.
15:22 - These other f divergences are not necessarily
15:25 - capturing a compression like objective.
15:28 - Because you're evaluating the density ratios
15:30 - in a different way.
15:31 - You don't just care about the log of the density ratios.
15:34 - You can plug-in different sort of f's that
15:37 - captures different preferences for how
15:40 - close is the model density ratios to the true density
15:43 - ratio.
15:44 - That's kind of captured through f and that gives you
15:46 - more flexibility basically in defining a loss function
15:50 - for training your model.
15:51 - Depends what you want to choose.
15:53 - So it could either be p is data and q is model
15:57 - or it could be vice versa.
15:58 - In both cases, you would end up with something
16:00 - that you can handle in the sense that it's a different of two
16:03 - expectations.
16:04 - And depending, do you want a KL data model
16:07 - or do you want KL model data.
16:09 - Depending on what you want, you need to choose the right order.
16:13 - And the right f that gives you the right thing.
16:16 - So it could be-- it could be both-- it doesn't matter
16:18 - for the perspective of this.
16:19 - It's just the difference of two expectations.
16:21 - You have samples from both and you can estimate both of them.
16:24 - Yeah.
16:25 - Monte Carlo.
16:26 - Yeah.
16:27 - So this one is basically saying that there's
16:29 - going to be an optimal t star for every x.
16:32 - And if you are allowed to have an arbitrary function--
16:36 - an arbitrarily complicated function
16:39 - that basically just maps every x to the corresponding t
16:42 - star of x.
16:43 - Then you get the same result. So you could say for every x,
16:49 - I'm going to choose a t star.
16:51 - Or you could say I'm going to first choose a function
16:54 - that maps x's to t stars.
16:57 - And to the extent that this function
16:59 - can do whatever you want, then there is no difference.
17:03 - You can kind of memorize all the t stars into a table
17:06 - and then encode that table into the function t.
17:09 - And so choosing the function or choosing the individual that
17:13 - basically outputs of the functions
17:15 - across the different x's is actually the same thing.
17:18 - This is a generalization.
17:20 - Like in the GAN framework, the original simple thing,
17:22 - we started from an expression that looked like this.
17:26 - And then we showed the, oh, by the way,
17:28 - it gives you the Jensen-Shannon divergence.
17:31 - This is kind of showing how you can actually
17:33 - start for any f divergence you want
17:35 - and you can get a loss that kind of looks like a GAN.
17:39 - And by the way, if you were to start
17:40 - from Jensen-Shannon divergence, you
17:42 - would get exactly the GAN loss that we--
17:45 - that we started with right up to shifts in scales.
17:47 -
17:52 - Cool.
17:52 - And so yeah, then thing to note is that the lower bound is
17:58 - likelihood free, in the sense that you can evaluate just
18:00 - based on samples.
18:02 - And once you have this kind of lower bound on the f divergence,
18:09 - you can get a GAN like objective as follows.
18:12 - You can choose an f divergence of your choice.
18:15 - You let, let's say p to be the data distribution.
18:18 - Q to be the model distribution defined implicitly
18:23 - through some--
18:24 - through some generator g.
18:27 - And then you parameterize both using neural networks.
18:30 - So let's say you have a set of neural networks
18:33 - with weights phi that define this function
18:36 - t that you have on the outside.
18:38 - The discriminator basically.
18:40 - And then you have some parameters
18:41 - that define the generator g.
18:45 - And then you have an f-GAN training objective
18:49 - which is very similar to what we had before.
18:52 - It's again a minimax kind of optimization problem
18:55 - where there is the inner maximization problem over phi.
18:58 - Where you're trying to find a good approximation
19:01 - to the f divergence.
19:03 - By maximizing trying to solve this optimization problem as
19:07 - well as you can.
19:09 - By trying to find weights phi that
19:12 - makes this expression as big as possible.
19:15 - And again, this is no longer cross entropy.
19:18 - But it's something quite similar.
19:22 - And then on the outside, you have a minimization over theta
19:25 - because you're trying to minimize
19:27 - the divergence between the model and the data distribution.
19:31 - So just like in the GAN setting, we
19:33 - have this the fake samples that are
19:35 - coming from this implicit distribution defined
19:37 - by a generator with parameters theta.
19:39 - And you can try to minimize-- choose the parameters theta that
19:44 - minimize this expression.
19:48 - And this-- it's basically the same as what we had in the--
19:53 - if you were to choose the Jensen-Shannon divergence,
19:56 - this would correspond to what we had before.
19:58 - But fundamentally, what's going on here
20:00 - is that there is a generator that's
20:01 - trying to minimize the divergence estimate.
20:03 - And there is a discriminator is trying
20:05 - to come up with the best possible sort of bound on that f
20:10 - divergence.
20:12 - So it's not going to give you exactly maximum likelihood.
20:14 - Because it's an approximation unless you
20:16 - have infinitely flexible kind of discriminators.
20:20 - What people have shown is that, if you were to--
20:23 - in the original f-GAN paper, they basically
20:26 - tested a bunch of different f's for f divergences.
20:28 - And what they've shown is that if you
20:30 - choose the f corresponding to KL divergence, then
20:33 - you tend to get the samples that indeed give you
20:36 - better likelihoods as you would expect.
20:38 - Because you're approximating the KL divergence.
20:41 - But as we kind of discussed, that's
20:43 - not necessarily the one that gives you the best sample
20:45 - quality.
20:46 - And you might be getting better sample quality
20:48 - if you were to choose different f's in that paper.
20:51 -
20:56 - Cool.
20:56 - So that's the kind of high level takeaway.
21:03 - You're not restricted to KL divergence-- exact KL divergence
21:07 - or Jensen-Shannon divergence.
21:09 - You can actually plug-in other f divergences
21:12 - and using the f-GAN training objective.
21:14 - You can still approximately optimize
21:17 - that notion of that divergence.

00:00 -
00:05 - SPEAKER: Now, the other thing that you
00:06 - can do using a very similar machinery
00:10 - is optimize yet a different notion of divergence,
00:15 - which is based on this idea called the Wasserstein GAN.
00:19 - And the motivation for moving beyond f-divergences
00:24 - is that f-divergences are nice they're very powerful,
00:27 - but there are issues when the distributions p
00:31 - and q don't share.
00:34 - They have let's say disjoint support, which
00:37 - can happen, especially early on in training.
00:40 - The samples coming from your generator
00:42 - could be very, very different from the ones
00:45 - that are in the training set.
00:47 - And if that happens, you can have this weird discontinuity
00:51 - where the divergence is a constant maybe infinity
00:55 - or something and then suddenly shifts to the some better value
01:01 - the moment the supports match.
01:04 - And that's a problem because during training you
01:06 - don't get good signal to go in the direction of trying
01:11 - to make the support of your model distribution close
01:15 - to the support of the data distribution.
01:17 - And you can see an example here.
01:20 - Imagine that you have a super simple data distribution where
01:24 - all the probability mass is at 0 and then you
01:27 - have a model distribution where you put all the probability
01:31 - mass at this point theta.
01:32 - Right?
01:33 -
01:36 - So if theta is 0, then the two distributions are the same.
01:40 - But if theta is different from 0,
01:42 - then these two distributions don't share any--
01:46 - the supports are different.
01:50 - And if you look at let's say the KL divergence
01:53 - is going to be 0 if the distributions match and it's
01:58 - going to be infinity for any other choice of theta.
02:01 - So if we're trying to train this generative adversarial network
02:05 - by optimizing theta to reduce the KL divergence,
02:11 - you're not going to get any signal
02:12 - until you hit the exact right value that you're looking for.
02:16 - And if you look at the Jensen-Shannon divergence,
02:18 - you have a similar problem where basically it's
02:22 - 0 if you have the right value and then it's
02:25 - a constant for when you have the wrong value.
02:28 - But again, there is no signal.
02:30 - There is no notion that theta 0.5 is better than theta 10.
02:35 - Ideally, that's something you would
02:37 - want because if you have that then you can do gradient descent
02:40 - and you can try to get to move your theta closer and closer
02:43 - to the value you want.
02:44 - But these sorts of f divergences can have
02:47 - trouble with these situations.
02:52 - And so the idea is to try to think
02:54 - about other notions of distance or divergences
03:00 - that work even when p and q have disjoint support.
03:05 - And the support is just the set of points
03:09 - that have nonzero probability under p or q.
03:14 - And so the kind of one way to do this is to use this thing
03:19 - called the Wasserstein or the Earth-Mover distance.
03:23 - And the intuition is something like this,
03:25 - right, let's say that you have two distributions p and q
03:30 - and you can think of-- the and they are just
03:33 - let's say one dimensional.
03:34 - So you have the densities that I'm showing there
03:37 - and they are just mixtures of Gaussians in this case.
03:40 - And you can ask, how similar are p and q?
03:46 - And one reasonable way of comparing
03:48 - how similar p and q are is to say
03:50 - if you think of the probability mass
03:53 - as being piles of earth or piles of dirt
03:57 - that you have laying out on this x-axis,
04:00 - you can imagine how much effort would it
04:03 - take you if you were to shovel all
04:05 - this dirt from this configuration
04:07 - to this other configuration.
04:11 - And intuitively, the further away
04:13 - you have to move this earth, the more cost you pay because you
04:16 - have to take more time.
04:18 - And p and q, they are both normalized.
04:22 - So the amount of earth that you have on the left
04:25 - is the same as the amount you have on the right.
04:28 - But kind of the more similar p and q
04:30 - are the same that you don't have to do any work.
04:33 - If the probability mass under q is very far
04:35 - from the one under p, then you have to do a lot of work
04:38 - because you have to move all this earth from various points
04:43 - where you have it on the left to the points
04:45 - where you have it on the right.
04:49 - And the good thing about this is that we'll
04:51 - see that it can handle situations where
04:53 - the supports are different.
04:54 - This kind of definition doesn't care if the supports of p and q
04:58 - are disjoint or not.
04:59 - And it defines a very natural notion of distance
05:04 - that varies smoothly as you change the shape of p and q.
05:09 - And the way to mathematically write down
05:16 - this intuition of looking at the cost of transporting earth
05:21 - from configuration p to configuration q
05:24 - is to set up an optimization problem, which looks like this.
05:30 - So the Wasserstein distance between p and q
05:33 - is going to be this infimum, which think of it as the minimum
05:37 - basically.
05:38 - And this infimum is over all joint probability distributions
05:43 - over x and y.
05:47 - You can think of x as being the distribution,
05:50 - p being defined over x and q being defined over y
05:53 - let's say as you look at joint distributions over x and y
05:58 - such that the marginal over x matches p
06:02 - and the marginal over y matches q.
06:07 - And what you do is over all these joint probability
06:12 - distributions that you have here,
06:15 - that you are optimizing over, you look at the expected cost
06:18 - that you get when you draw x and y from this joint distribution.
06:23 - And the cost is the thing that we talked about,
06:25 - which is basically how much effort it
06:27 - takes to go from x to y.
06:30 - And in this case, this is measured with this L1 distance.
06:33 - You can choose other choices, but for now you
06:36 - can think of it basically the absolute value in 1D
06:40 - of x minus y.
06:43 - And you can think of this gamma x, y,
06:49 - which is a joint distribution over x and y
06:53 - as basically telling you how much probability I'm
06:59 - moving from x to y.
07:02 - And so what this is saying is that this condition
07:05 - here that the marginal over x is p
07:08 - of x this is saying that at the beginning
07:11 - you can't move more probability mass than what you started
07:14 - from at x.
07:16 - And the fact that the marginal over y is qy
07:18 - means that the final result, the amount of earth
07:27 - that you find at position y is indeed the one
07:30 - that you want to get in the final configuration, which is
07:33 - the one you're trying to get.
07:35 - And this objective function here is
07:37 - telling you what is the cost of moving earth basically
07:41 - from x to y.
07:42 - So equivalently, you can think of the conditional distribution
07:46 - of y given x as telling you, which fraction of the earth that
07:51 - I have at location x am I going to move to the different y's?
07:54 -
07:57 - And so you can see that then if you look at this expectation,
08:01 - this is telling you in expectation,
08:02 - how much are you going-- how much is this going to cost you.
08:05 - Well, you look at x, you look at the y
08:07 - as you're moving the earth to, you
08:09 - look at the difference between the two
08:11 - and that tells you how much it costs you for a given x.
08:16 - If you take the expectation with respect to y gamma y given x,
08:20 - it's telling you the cost of moving all the probability
08:22 - mass that you have at x to the places you want it to move it
08:26 - to, which because of this constraint here
08:29 - it has to match the final result that you want.
08:33 -
08:36 - And so that's basically the optimization problem
08:41 - that defines this intuition of telling us,
08:46 - how much work do you have to do if you want to move this--
08:49 - we want to morph this probability distribution
08:51 - here into the probability distribution q
08:54 - that you have as an outcome?
08:58 - And just to get a sense of what this
09:02 - looks like in the previous example where we had this data
09:06 - distribution where all the probability mass is
09:08 - at 0 and this model distribution where all the probability
09:11 - mass is at theta, this one, the KL divergence between these two
09:16 - objects is not very useful.
09:18 - But if you think about, what is the earth mover distance here?
09:21 - How much work do you need to do if you
09:23 - want to move all the probability mass from here to here?
09:27 - Yeah, so it's the absolute value of theta technically.
09:32 - And so you can see that now it's starting
09:35 - to be more reasonable in the sense
09:36 - that the closer q theta is to the target, p,
09:43 - the smaller this divergence is, which you might expect
09:47 - might give you maybe a much better learning objective
09:51 - because you have much better gradients.
09:55 - You have a notion of how close you are,
09:57 - how much progress you're making towards achieving
10:00 - your goal to the extent that you can really compute this thing
10:03 - and we'll see how to do that.
10:05 - This would mean this would be a pretty good sort of learning
10:08 - objective to use.
10:10 - Yeah, there is an infinite number of joint distributions
10:14 - that have given marginals.
10:16 - If you think about it, this is actually a pretty mild kind
10:18 - of set of constraints just saying that for every x,
10:24 - the marginal under gamma has to match the distribution at you
10:31 - started from.
10:33 - So this is kind of saying that the-- if you think of gamma x
10:37 - comma y as the amount of earth that is moved from x to y,
10:43 - this is sort of saying that the total amount of earth that you--
10:47 - or actually that is--
10:48 - yeah, that the total amount of earth that you move
10:51 - has to be the amount that you had to begin with.
10:56 - And this is saying that the other constraint is saying
10:59 - that if you look at the amount of earth that you get at the end
11:04 - after you moved everything from all the various x's,
11:07 - it has to match what you want, which
11:09 - is the final result, the final q of y,
11:12 - which is the amount of earth that you want after you've done
11:17 - all this transport, after you've moved all the probability mass.
11:20 -
11:26 - So yeah, there you have two random variables.
11:29 - You can think about many different joint distributions
11:33 - with the same marginals.
11:34 - And if you think about two binary random variables,
11:36 - these two random variables could be independent,
11:38 - they could be highly dependent, and the joint distribution
11:42 - is what tells you how they are related to each other.
11:45 - So there is many joint distributions
11:47 - with the same marginals.
11:49 - And in this case the relationship between them.
11:52 - It's telling you how coupled they are
11:56 - and where you're going to move probability
11:59 - mass from one to the other.
12:01 - Basically what this is saying is it's just the L1 norm, which
12:07 - in 1D you can think of it as just the absolute value of x
12:09 - minus y.
12:10 - And this is just saying that when x and y are far away,
12:14 - they're going to pay a higher cost because transporting
12:18 - from here to Palo Alto is cheaper than from here
12:24 - to San Francisco.
12:25 - And so you can think of if the x-axis is measured in kilometers
12:30 - or something and then you would x minus y
12:32 - is just the distance that you have
12:34 - to travel to go from one point to the other.
12:38 - And so ideally, you would want to choose a gamma such
12:43 - that when you sample from it, x and y are
12:47 - very close to each other.
12:50 - So you minimize the amount of work that you have to do.
12:53 - But that's non-trivial because you also
12:54 - have to satisfy these constraints
12:56 - that at the end of the day you've moved all the probability
12:59 - mass that you have to move and you
13:02 - get this q configuration as a final result, which
13:07 - is this constraint that is saying the marginal over y
13:10 - is q of y.
13:11 - And this constraint is just saying you cannot create earth
13:15 - out of nowhere.
13:16 - You have to move the earth that you started
13:18 - from, you have to go from the configuration
13:21 - that you have on the left, which is p to the configuration
13:23 - that you have on the right, which is q.
13:25 - And these constraints here on the marginals
13:27 - are basically just saying that that's
13:29 - the initial condition, that's the final condition, that's
13:31 - the cost that you incur whenever you move earth from x to y.
13:36 - And so again, basically we want to choose a gamma y
13:39 - given x that puts as much probability mass on y's
13:43 - that are close to x as possible.
13:46 - But then you not always can do it because sometimes you
13:48 - do have to move away.
13:49 - If you have to move probability mass out here
13:52 - and you didn't have any, then you have to take it somewhere.
13:55 - And this optimization problem tells
13:57 - you what's the optimal way of--
13:59 - what's the optimal transport plan
14:01 - that moves the mass from one setting to the other.
14:04 - And again, we're basically in a situation
14:07 - where the original objective function is reasonable,
14:13 - makes sense, it would be good to optimize,
14:15 - but it looks not something we can actually
14:18 - compute because as usual, p and q should be a model and a data
14:23 - distribution.
14:24 - We don't know how to evaluate probabilities
14:26 - according to one or the other.
14:28 - So that doesn't look like something we can optimize.
14:32 - But it turns out that there is a variational characterization
14:37 - or there is a way to basically write it
14:39 - down as the solution of an optimization problem
14:43 - that we can then approximate using some kind of discriminator
14:47 - or some kind of neural network.
14:49 - And it's possible to show that this Wasserstein distance
14:54 - or earth mover distance is equal to the solution
14:57 - to this optimization problem where
15:00 - you have a difference of expectations,
15:02 - one with respect to p and one with respect to q.
15:05 - Again, it's very similar to the GAN setting.
15:08 - And the only difference is that now what we're doing
15:12 - is we're optimizing over functions that have Lipschitz
15:18 - constant 1, which basically means you need to optimize over
15:22 - all functions that basically don't change too rapidly.
15:24 -
15:27 - And so the solution to this optimization problem
15:30 - or this scalar functions f is actually
15:33 - equal to the Wasserstein distance.
15:37 - And notice here we don't have f stars anymore.
15:41 - This is really just the difference in expectations
15:43 - between p and q.
15:45 - And so if you didn't have any constraint,
15:48 - then you could make that thing blow up
15:51 - very easily because you could just pick a point where
15:57 - the probabilities are different and then you could just increase
16:00 - the value of f at that point arbitrarily
16:03 - and then you could make this objective here extremely large
16:10 - or extremely small.
16:12 - But you cannot do it, you cannot choose an arbitrary function f
16:17 - because you have this constraint that basically the shape of f
16:21 - cannot change too rapidly, it has to have Lipschitz constant
16:25 - 1, which basically means that if you go through the graph of f
16:29 - and you take any two points x and y,
16:31 - the slope that you see is bounded by 1 essentially.
16:35 -
16:38 - And again, this optimization problem by itself
16:41 - is not quite something we can solve.
16:43 - But in practice, what you can do is
16:45 - you can approximate the inner this optimization
16:49 - problem over all discriminators that
16:54 - are trying to tell you think about it,
16:56 - what is this objective doing?
16:58 - You're looking for points where the probability mass under p
17:01 - and q is different.
17:03 - So you can find these points then you
17:04 - can increase the value of f and you
17:06 - can get a high value in that difference of expectations.
17:12 - And so you can approximate that problem
17:14 - of trying to find x's that are given
17:17 - different probabilities under model and data
17:19 - by training a neural network, which is, again,
17:23 - going to be some discriminator.
17:26 - And at this point there is, again, no cross-entropy loss.
17:29 - You're just trying to find a network that
17:31 - can take high values on the data points and low values
17:35 - on the fake data.
17:38 - And to enforce the Lipschitzness,
17:40 - enforcing Lipschitzness is hard.
17:43 - But in practice what you can do is as usual
17:45 - you don't want this network to be arbitrarily changing
17:48 - too fast too much.
17:50 - And then in practice what you do is you would either clip
17:53 - the weights or you would enforce a penalty on the gradient
17:57 - of the discriminator so that, again,
18:00 - it cannot change too much, it cannot change too rapidly.
18:04 - So the earth mover distance is this quantity
18:07 - you have on the left so to the extent
18:09 - that you could solve this optimization
18:10 - problem on the right then you would
18:13 - be able to compute exactly the earth mover distance.
18:17 - And intuitively, this function f is
18:21 - telling where there is a discrepancy in probability
18:25 - mass between p and q.
18:27 - So if there are x's that are given different probabilities
18:30 - under p and q, then f will try to choose a large value ideally.
18:36 - But then because of this Lipschitzness constraint then
18:39 - you cannot make it arbitrarily big.
18:40 - you cannot go to infinity.
18:42 - And so you have to somehow be smooth
18:45 - and at the same time try to find differences between p and q.
18:51 - And if you can't solve this one yet,
18:53 - this will give you exactly the Wasserstein.
18:57 - The problem is that in practice you cannot--
18:59 - like before, in the f-GAN setting
19:02 - you can't really optimize that.
19:05 - And so in practice you would use approximations
19:07 - where you just use some sort of discriminator
19:11 - and you try to make sure that the discriminator is not
19:15 - too powerful and you try to restrict basically
19:17 - how powerful the discriminator is
19:19 - by either, for example, trying to reduce,
19:24 - trying to have a penalty term on the gradient with respect
19:28 - to the inputs of the discriminator.
19:29 - So that it cannot change too much.
19:32 - And this doesn't give you bounds.
19:35 - So unlike the f divergence setting,
19:38 - this is just an approximation.
19:39 - It doesn't necessarily give you bounds.
19:42 - Yeah, so they're all based on the very similar idea where
19:44 - you're trying to find a witness function, a discriminator,
19:48 - or some kind of classifier that is trying
19:50 - to distinguish samples coming from p from samples coming
19:54 - from q.
19:55 - You have to restrict what this witness
19:59 - function or this classifier does in some way
20:02 - or you change the way you're scoring
20:04 - what this classifier does.
20:07 - And depending on how you do that,
20:09 - you measure similarity basically in different ways.
20:12 - And if you restrict this discriminator
20:16 - to have a Lipschitz constant of at most 1,
20:19 - then you get Wasserstein.
20:21 - If you use an arbitrary function,
20:23 - but then you score it with respect to that f star,
20:25 - then you get an f divergence, and so forth, yeah.
20:31 - But the main advantage of this is
20:32 - that it's much easier to train.
20:35 - So in practice this is very often used.
20:38 - And you can see an example here where
20:41 - you can imagine a setting where you
20:43 - have real data that is just a Gaussian that is kind of here.
20:47 - So you see all the samples that are
20:49 - coming that are these blue dots that are lying around here.
20:53 - And then you have a model.
20:56 - Let's say you start out with a bad initialization
20:58 - for your generator and most of your samples
21:00 - are, again, a Gaussian, but somehow the means are different,
21:04 - and so all your samples are here, these green dots.
21:08 - And if you get the discriminator,
21:10 - the discriminator will have a very good job
21:12 - at distinguishing between the blue samples
21:14 - and the green samples.
21:16 - And it will be a sigmoid, but it's extremely steep.
21:20 - So basically, everything to the left
21:23 - here will be classified as real.
21:25 - And everything to the right will be classified as fake.
21:28 - But it's almost entirely flat.
21:31 - And so when you think about trying to figure out
21:34 - when you update the generator to try
21:36 - to fool the discriminator, you don't get a whole lot of signal
21:40 - in terms of, which direction should you
21:42 - move these data points?
21:44 - Because the red curve is too flat.
21:49 - And so it's very tricky to actually get this model
21:52 - to learn and be able to learn how to push the fake data
21:56 - points towards the right.
21:58 - But if you think about the Wasserstein GAN critic, which
22:02 - is just the discriminator, it's almost like a linear function.
22:07 - It's this light blue curve.
22:10 - And if you are from the perspective of the generator
22:15 - and you're trying to kind of reduce the same objective
22:24 - function that was being optimized by the critic,
22:27 - you have a much better kind of learning signal
22:31 - to push your data points to the left.
22:34 - And kind of you know that, yeah, this data points out here
22:37 - are much better than the data points out there.
22:41 - I guess, you can even do it in closed form.
22:44 - I don't know if they did it.
22:49 - You could probably also approximate it somehow,
22:53 - but if it's just two Gaussians I think
22:55 - you can do it in closed form.
22:56 - So is the decision boundary, which is not di--
23:04 - well, yeah, I guess you would still go.
23:06 - You would try to, yeah, because it would be the opposite.
23:09 - So you're trying to make it fake.
23:12 - So you would still push towards the left.
23:16 - And from the perspective of the WGAN,
23:18 - you would still try to minimize.
23:21 - From the G, the perspective, you will minimize this expression
23:25 - that you have inside.
23:26 - And so again, you would push the points to the left
23:30 - because the light blue curve goes down.
23:33 - And so I think it's just that it's
23:36 - plotting probability of fake instead of plotting probability
23:38 - of real.
23:39 - So that's why it's going in the wrong direction.
23:41 - You can actually see it here.
23:43 - And it's just that you have basically better learning signal
23:46 - and it's similar to what we were talking
23:49 - about here that if the distributions are too
23:51 - different, then with respect to KL divergence
23:54 - you might not have good enough signal that tells you,
23:58 - oh, if you put all the-- in this case putting the probability
24:02 - mass at one half is better than putting the probability
24:04 - mass at 10.
24:05 - With respect to the Wasserstein, this would show up
24:08 - because there would be a difference between those two
24:10 - settings and one half is closer to the ground truth than 10.
24:14 - And so you would be able to do gradient descent
24:18 - on that objective with respect to theta
24:20 - and you would get closer and closer.
24:22 - With respect to KL, you don't quite see it.
24:24 - And in practice you can also see it here
24:28 - where basically doing optimization from the generator
24:33 - perspective, doing optimization by minimizing
24:36 - the light blue curve is much easier than trying
24:40 - to fool the discriminator in the regular GAN setting
24:45 - because there is vanishing gradients and it's too flat.
24:49 - Yeah, I don't know if you can formally
24:51 - prove that it's more powerful than a regular GAN.
24:57 - They're measuring distance in a different way and I don't know.
25:00 - In general, you could say you would probably
25:02 - have to make some assumptions on p and q
25:04 - to say which one is better and which one is worse,
25:07 - I think in general.
25:09 - I think from this it's more like if you had access
25:14 - to infinitely powerful discriminators I think
25:19 - in that world, I think, it--
25:21 - both would probably work.
25:23 - I think in practice you always have approximations and you are
25:26 - optimizing over restricted families of discriminators
25:30 - and you have this minimax thing where you cannot actually solve
25:32 - the problems to optimality.
25:34 - And it turns out that optimizing the Wasserstein
25:38 - type of objective is much more stable in practice.

00:00 -
00:05 - SPEAKER: The last thing we can briefly talk about
00:07 - is how to actually inferring latent representations in GANs.
00:12 - This is going to be a bit of a shift in terms of the topic
00:15 - but once you train a GAN, you have these latent variables
00:19 - that are mapped to observed variables
00:22 - and you might wonder, it kind of looks like a VAE, to what extent
00:26 - are you able to recover z given x?
00:29 - Let's say if you wanted features or something like that.
00:33 - And one way to do it, the problem
00:37 - is that it's no longer an invertible mapping
00:40 - and you don't have an encoder, right?
00:43 - So in the flow mapping setting, you just invert the generator.
00:48 - So given an x, you figure out what
00:49 - was the z that would be mapped to that x.
00:52 - In the variational autoencoder, you have the inference model,
00:55 - you have the encoder that is doing that job for you.
00:59 - In a GAN you don't quite have it.
01:01 - So one way to get features from a GAN
01:04 - is to actually look at the discriminator.
01:07 - So the discriminator is trying to distinguish real data
01:10 - from fake data, so presumably to do well
01:13 - at the job it has to figure out interesting features
01:16 - of the data.
01:17 - And so you can try to take the discriminator
01:20 - and fine tune it on different tasks
01:23 - or take the representations that you
01:26 - get towards the end of the neural network
01:30 - and hope that those features are actually useful for other tasks.
01:35 - If they were helpful for distinguishing real data
01:37 - from fake data, they might work for other tasks as well.
01:40 -
01:44 - If you want to get the z variables
01:48 - from the discriminator, from the generator
01:50 - kind of like in the VAE, then you
01:51 - need a different learning algorithm.
01:54 - And the problem is that in a regular GAN,
01:58 - you're basically just looking at the x part
02:01 - and somehow we need to change the training
02:03 - objective to also look at the z part and the latent variables.
02:07 - And the way to do it is to basically change
02:11 - the way you set up two sample tests or this likelihood
02:14 - free learning objectives to not only compare
02:17 - the x samples that you get from the model
02:20 - to the real data samples, but to also
02:23 - look at the representations, the kind of z's that produced
02:26 - the samples that you see.
02:29 - And the thing is that when you sample from the model,
02:34 - you get to see both the x and the z part
02:38 - because you're sampling them yourself.
02:40 - But in the data, you only get to see the x,
02:42 - there is no corresponding z.
02:45 - And so the way to do it is to essentially just like in a VAE
02:50 - introduce an encoder kind of network that
02:53 - will map x to the corresponding latent representation z.
02:59 - And so the architecture looks like this,
03:03 - it's called the BiGAN because it goes in two directions.
03:07 - So you have latent features that get mapped to data
03:10 - through the generator and then you
03:12 - have data that gets mapped to latent features
03:14 - through some encoder network.
03:17 - And then the job of the discriminator
03:20 - is to not only distinguish Gz from x, fake samples
03:25 - from real samples, but now the discriminator
03:28 - is going to try to distinguish fake samples with the latent
03:33 - variables from the model, from real samples and latent
03:37 - variables inferred from the encoder.
03:40 - So it's going to work in pairs of inputs x and z,
03:46 - where sometimes the x's are real,
03:48 - sometimes they are generated by the model and same thing.
03:50 - Sometimes the z are real, they are produced from the prior
03:54 - and sometimes they're produced by fitting real data
03:57 - to the encoder.
03:59 - And then basically everything is the same,
04:03 - then you train the generator, trying
04:08 - to fool the discriminator.
04:09 - You train the encoder and you train the discriminator
04:12 - trying to distinguish the samples and to the extent
04:15 - that this works.
04:19 - So the discriminator observes these pairs
04:21 - and the discriminator is trying to do as well as it can at
04:24 - distinguishing these two pairs.
04:27 - And after training, basically you can get the samples from g
04:33 - and you use the encoder to get the latent representations.
04:37 - And that's sort of the idea, it's pretty simple,
04:41 - it's an extension of GANs where you have another mapping which
04:47 - is also deterministic going from data to latent features
04:50 - and then you let the discriminator operate not only
04:53 - on data but on data, latent.
04:56 - And so to the extent that the discriminator cannot
04:59 - distinguish, the z's that are produced by the generative
05:02 - procedure from the z's that are produced by the encoder,
05:05 - then you might expect that the encoder is producing latent
05:08 - representations that are similar to the one that GANs would have
05:11 - used for generating a data point.
05:13 - And so effectively, the encoder is inverting
05:17 - the generative procedure, so it's
05:19 - very similar to a variational autoencoder
05:22 - except that E is a deterministic mapping
05:25 - and is not trained by minimizing KL divergences like in the ELBO,
05:29 - but it's trained by minimizing some kind of two sample
05:32 - test that is being optimized by a discriminator.
05:35 -
05:39 - It's the same sort of high level intuition.
05:43 - Yes, it's the concatenation, so you
05:45 - need to be able to distinguish pairs of real data.
05:47 -
05:50 - Features produced by the encoder from fake data,
05:53 - real features produced from the prior.
05:55 - So you cannot distinguish them, then the features that you get
05:58 - from the encoder E of x are going to be very similar
06:01 - to the z's that were actually used for generating data points.
06:04 - And so that's how the encoder is trained,
06:06 - everything is trained sort of adversarial.
06:09 - In this version there's only two options,
06:12 - but you could imagine a version where
06:17 - you're trying to enforce something stronger
06:18 - or maybe it's more like a cycle consistency
06:22 - that I guess we didn't have time to talk about today.
06:25 - Here there's only two, there is basically samples from the model
06:29 - and corresponding latents versus real data
06:33 - and corresponding latents.
06:35 - That's meant to be on real data, so let's say that then you
06:38 - want to do transfer learning or you want
06:41 - to do semisupervised learning or you
06:42 - want to do clustering or something,
06:44 - how do you get the features from a data point x?
06:47 - You don't use G because you don't know how to invert it,
06:50 - but you've trained a separate model, this encoder
06:52 - model that is basically trained to invert G, and so on.
06:56 - Real data at test time you just use
06:58 - E to get the corresponding latents.
07:02 - Like a VAE, two different pieces that
07:04 - are trained together to fool a discriminator in this case
07:09 - instead of minimizing an ELBO.
07:11 - This is all the same, they are sampled from a prior,
07:14 - so it's the same training as a GAN.
07:16 - So the z part doesn't change.
07:18 - So the z's are from the top half, the z's from the prior
07:24 - and then you pass them through the generator
07:26 - to produce data except that in a VAE
07:29 - that matching is done via KL divergence,
07:31 - here that matching is done adversarially basically.
07:36 - So the outputs of the encoder should
07:39 - be indistinguishable from z's that are sampled
07:42 - from the prior, where indistinguishable is measured
07:45 - not with respect to KL.
07:48 - Now it's measured with respect to a discriminator should not
07:51 - be able to distinguish that the stuff that comes out
07:54 - from the encoder when it's fed real data
07:56 - is different from the real latent variables
07:59 - that you sampled yourself from the prior.
08:02 - So it has the similar flavor to if you remember VAE,
08:05 - we had a very similar kind of intuition
08:07 - that what comes out from the encoder
08:09 - should be indistinguishable from the latent
08:13 - that you generate yourself.
08:15 - In that case, we were enforcing that indistinguishable using KL,
08:19 - here we're using a two sample test discriminator.
08:23 - So kind of like in VAE, you have a x,
08:26 - you feed it through the encoder and you
08:27 - get the corresponding latents and then
08:29 - you do whatever you need to do.
08:32 - Here at inference time if you want to just generate
08:34 - you don't use the encoder but if you want to get features then
08:37 - you still use the encoder just like here.