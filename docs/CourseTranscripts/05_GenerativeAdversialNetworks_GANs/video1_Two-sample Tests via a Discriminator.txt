00:00 -
00:05 - SPEAKER: So the basic idea is that we're
00:08 - going to think about exactly that problem of deciding
00:12 - whether or not two distributions are similar by looking
00:17 - at the samples you get if you're going to sample
00:20 - from one or the other.
00:22 - So you can imagine this setting where
00:24 - I give you a bunch of samples that
00:27 - are sampled from some distribution P,
00:29 - and I give you a bunch of samples
00:31 - that are coming from some distribution Q.
00:34 - And what we can try to do is we can try to figure out
00:41 - if there is a way to tell whether or not
00:44 - P, this distribution that we use to generate
00:47 - the first batch, this group of samples,
00:50 - is actually the same as the distribution
00:51 - that we used to generate the right group of samples.
00:58 - If you can do that, then that could
01:02 - be a good way of comparing probability distributions
01:05 - because if we don't have a way of telling whether or not
01:09 - the distributions are different, then
01:11 - it probably means that the distributions
01:13 - are close to each other.
01:16 - And so that's basically what we're
01:17 - going to do is we're going to use something
01:20 - called a two-sample test, which is basically a procedure
01:23 - for determining whether or not these two groups of samples,
01:29 - S1 and S2, are coming from the same distribu--
01:32 - are generated by the same probability distribution
01:35 - or not.
01:37 - And so it's an hypothesis testing problem,
01:39 - which you might have seen before in other contexts where
01:43 - basically there is a null hypothesis, which is
01:45 - the distributions are the same.
01:47 - So P is equal to Q. So the samples in the first group
01:52 - have the same distribution as the samples
01:54 - in the second group.
01:56 - And then there is an alternative hypothesis,
01:58 - which is that the distributions are different.
02:00 - Yes.
02:02 - You want to compare the distributions because, I guess,
02:04 - we will need a training objective.
02:06 - So we typically use KL divergence
02:09 - to figure out how close our model is to the data.
02:11 - And we said, OK, KL divergence is good but perhaps
02:16 - not ideal in the sense that you can
02:17 - get pretty close in KL divergence
02:19 - and have terrible samples.
02:21 - So maybe there is room for choosing different comparisons,
02:27 - different ways of comparing probability distributions that
02:30 - are closer to perceptual quality of the samples
02:33 - and will allow us to do things differently.
02:36 - And that's the main motivation.
02:38 - So one way to go about it is to say,
02:40 - OK, let's say I have a bunch of samples from P data.
02:42 - Let's say I have a bunch of samples from P theta.
02:44 - How do I compare them?
02:46 - And the most basic thing to do is to say,
02:48 - Can I decide whether or not they are different?
02:50 - Because if I cannot figure out if they are different,
02:53 - then it means that I'm close.
02:56 - They are the same.
02:58 - So if I fail at this hypothesis testing problem--
03:02 - or it's very hard to do well in this task,
03:07 - then it means the distribution are similar.
03:09 - And so I have a pretty good way of--
03:11 - and then I have a reasonable way of comparing.
03:13 - Then maybe I'm doing well on my learning problem.
03:16 - So that's the intuition.
03:18 - How do you do it?
03:19 - Typically what you would do is you would come up
03:21 - with a test statistic which is just a function that you
03:26 - use to compare these two sets.
03:29 - For example, you might try to look
03:31 - at what is the mean of the samples in the first group
03:35 - versus the mean of the samples in the second group.
03:38 - Or you could look at the variance, the sample
03:41 - variance of the samples in S1 versus
03:44 - the sample variance in S2.
03:47 - And presumably, if indeed P is equal to Q,
03:51 - then you would expect the mean of the samples in here
03:54 - to be similar to the means of the samples in here.
03:56 - And you would expect the variances to be similar.
04:00 - So, for example, one statistic you could try
04:04 - is to do something like this where
04:06 - you compute the mean in the first group,
04:09 - you compute the mean in the second group,
04:11 - and then you compare them.
04:13 - And what you could do is you could say, OK,
04:16 - if this statistic is larger than some threshold,
04:21 - then I reject the null hypothesis.
04:24 - Otherwise, I say that H0, so the null hypothesis
04:29 - is consistent with my observation.
04:31 - And as you were saying, there is always some type one and type
04:34 - two error in the sense that T is random
04:37 - because S1 and S2 are random.
04:39 - And so even when P is equal to Q,
04:42 - it's possible that the means are different
04:45 - just because of randomness.
04:48 - It's not going to be super important to what
04:51 - we're going to do.
04:53 - But, yeah, there is no-- it's a hard problem.
04:56 - Even if you had a good statistic,
04:59 - there is still going to be some probability of error.
05:02 - But you could ask the question of, OK,
05:04 - what's the best statistic for determining
05:07 - whether or not to solve this hypothesis testing problem?
05:12 - And you could try to minimize type one and type two errors.
05:14 - There's going to be false positives.
05:16 - There's going to be false negatives.
05:17 - But you can try to choose a statistic that minimizes
05:21 - these types of errors.
05:23 - In practice, one is going to be real data.
05:26 - One is going to be--
05:27 - let's say, S1 is going to be samples from the data
05:29 - distribution, and S2 is going to be samples from our model,
05:33 - but it doesn't have to.
05:34 - It could be two models.
05:35 - It could be-- yeah, this is pretty generic.
05:39 - But the way we're going to actually
05:40 - use it for a generative adversarial network,
05:42 - it's going to be one group of samples are going to be real.
05:45 - One group of samples are going to come from that model.
05:47 - So they're going to be fake.
05:48 - And then we're going to use some statistic
05:51 - to try to determine whether or not
05:54 - these two samples are similar.
05:55 -
06:00 - And, yeah, the key observation here
06:03 - is that at least there is some room for choosing
06:09 - different kind of test statistics,
06:11 - and you can choose some statistics which do not
06:15 - depend on the likelihood of P or Q. For example,
06:18 - if you just look at the means, you don't
06:20 - need to know the probability.
06:23 - You don't need to be able to evaluate probabilities under P.
06:25 - You don't need to evaluate probabilities under Q.
06:28 - You can compute this function just
06:29 - based on a bunch of samples.
06:31 - Yeah, so the question is, Does it
06:33 - solve the overfitting problem?
06:35 - You still have the overfitting problem
06:37 - just like in even if you do maximum likelihood,
06:40 - you can still have overfitting problem.
06:41 - So this does not directly address the overfitting
06:44 - directly although, yeah, you can--
06:48 - sometimes, at least, you can use validation and other things
06:52 - to see what would be the--
06:55 - yeah.
06:55 - So, yeah, so you could try to choose test statistics that
06:59 - are based on the likelihood of the model,
07:02 - but you don't have to.
07:03 -
07:05 - And so, again, the setting of generative
07:11 - modeling with two sample tests is one where we have--
07:15 - there's going to be a bunch of samples
07:16 - that are just going to be coming from the data distribution.
07:20 - Recall that that's all we always assume that somebody is giving
07:23 - us access to a bunch of step samples
07:26 - that are coming from the data distribution,
07:28 - and that's our training set.
07:29 - And so that's going to be the first group of samples.
07:33 - Then just like before, we have a set of models.
07:36 - We have a set of distributions P theta that we
07:39 - are willing to consider.
07:41 - And as long as these distributions
07:44 - are easy to sample from, which is not a terrible requirement
07:48 - because presumably you're going to use the model
07:50 - to generate samples anyways.
07:51 - But as long as you can somehow sample from the model,
07:54 - you can always generate this second set
07:57 - of samples S2, which are just basically
08:00 - samples from the model.
08:03 - And then what you can do is you can try to train--
08:07 - basically the high level idea is going
08:09 - to be instead of trying to find a model P theta,
08:16 - just going to optimize over the set
08:18 - so that we minimize the KL divergence between these two
08:21 - distributions, we are going to try
08:24 - to find a model that tries to minimize this whatever test
08:30 - statistic that we've decided to use
08:32 - to compare two set of samples.
08:37 - And, for example, in the previous example,
08:41 - it could be something like try to make
08:43 - sure that the means of the samples that I produce
08:46 - matches the mean of the samples that I had in the training set,
08:51 - which would not be very useful, but that's the high level idea.
08:57 - And so the problem is that finding a good statistic
09:02 - is not easy.
09:05 - For example, if you were to just try
09:08 - to minimize this kind of test statistic here,
09:12 - you would end up with a generative model that
09:14 - produces samples with the same mean as the training set, which
09:20 - is-- it's the property is desirable,
09:26 - but it's not sufficient.
09:28 - Like, if you just match the mean,
09:30 - you can still produce very bad samples
09:33 - and fool this kind of test statistic.
09:37 - And that's the problem is that in high dimensions,
09:41 - it's very hard to find some good test statistic.
09:45 - And so intuitively, you could say,
09:48 - OK, let's say that you start comparing
09:51 - probability distributions by matching the mean.
09:53 - And so if you compare the means, you
09:55 - would be able to distinguish, let's say,
09:57 - that this green Gaussian is different
09:59 - from this red Gaussian.
10:02 - But then you could say, OK, just matching
10:04 - the means is not sufficient.
10:06 - Here are two Gaussians that have the same mean
10:09 - but different variances.
10:11 - So if you just compare the means,
10:14 - you would not be able to distinguish between those two
10:16 - Gaussians.
10:17 - And then maybe you--
10:19 - and if you match, let's say, the mean and the variance,
10:22 - you could have different distributions
10:25 - that have the same first moments or same mean and same variance,
10:28 - like a Gaussian and Laplace density, same mean,
10:33 - same variances, but different shapes.
10:35 - So you can get a sense that especially the more dimensions
10:41 - you have-- you see they are modeling a lot of pixels
10:43 - or a lot of tokens.
10:45 - There is many different ways in which
10:47 - two or many different things you could look at when you compare
10:52 - two probability distributions.
10:53 - There's many different features that you
10:55 - could try to compare samples with respect to.
10:59 - And so handcrafting test statistic and just say,
11:05 - OK, let's try to train a model based
11:06 - on that is unlikely to work because you're going to match
11:11 - the test statistic, and there's going
11:13 - to be other differences that you didn't
11:14 - think about that actually matter in practice.
11:20 - So what we are going to do is we're going to try to--
11:27 - instead of picking a fixed handcrafted test statistic,
11:32 - we're going to try to learn one to automatically identify
11:36 - in which ways these two set of samples, that one
11:39 - from the data and the one from the model,
11:41 - they differ from each other.
11:44 - Instead of keeping it fixed and just say
11:46 - let's compare the mean, we're going
11:47 - to try to learn what makes these two samples different.
11:54 - And how to do that?
11:57 - Any guess?
11:59 - You've got to try to just basically train a classifier
12:02 - essentially, which in the language
12:04 - of generative adversarial networks
12:07 - is called a discriminator.
12:08 - So what we're going to machine learning,
12:11 - that's exactly what you would do.
12:13 - If you are trying to solve a classification problem,
12:16 - you're trying to figure out what distinguishes
12:19 - the positive class from the negative class, the job
12:22 - of a classifier or, for example, a deep neural network
12:26 - is to identify features that allow
12:27 - you to discriminate and distinguish
12:29 - these two groups of samples.
12:31 - So we're going to use the same idea to figure out
12:34 - what features of the data should we look at to discriminate
12:38 - between the green curve and the red curve, essentially.
12:44 - And so that's basically what we're
12:46 - going to do is we're going to train a classifier--
12:49 - again, it's called a discriminator in this context--
12:52 - to basically classify-- we're going
12:55 - to use a binary classifier, for example, a neural network,
12:59 - to try to distinguish real samples,
13:03 - which are basically the ones from the first set,
13:05 - the one generated by the data distribution, which, let's say,
13:08 - label one, from fake samples, which are the ones generated
13:13 - by P theta by our model, which we can say, for example,
13:16 - label zero.
13:18 - And we can train a classifier to do that.
13:22 - And as a test statistic, we can use the minus
13:26 - the loss of the classifier.
13:28 -
13:32 - Why do we do this?
13:33 - Well, what happens if the loss of the classifier is high?
13:36 -
13:41 - Well, let's say, if the loss of the classifier is low,
13:45 - then it means that you're doing a very good job
13:47 - as distinguishing the two.
13:50 - They are very well separated.
13:52 - Your classifier is doing a very good job
13:55 - at distinguishing these two groups of samples.
13:59 - And so they are very different.
14:01 - So we want the test statistic to be small--
14:03 -
14:06 - to be large, sorry.
14:07 - And if we have a high loss, then the real and the fake samples
14:13 - are hard to distinguish.
14:14 - And so we expect that them to be similar.
14:18 - So it's based on the likelihood of the classifier.
14:21 - It's not based on the likelihood of the generative model.
14:24 - So by doing things this way, we're
14:26 - going to use the likelihood, but it's
14:27 - going to be the likelihood of a classifier.
14:30 - And that's much easier because that's just
14:31 - going to be a likelihood over a basically a binary variable.
14:35 - And so essentially, it doesn't really
14:37 - put any restriction on the kind of neural network
14:39 - you can use as opposed to the likelihood of X
14:44 - over the input, which requires you
14:46 - to either use autoregressive models
14:48 - or invertible neural networks.
14:50 - It puts a lot of restrictions on the kind of architectures
14:52 - you can use.
14:53 - Here, it's just going to be a likelihood
14:55 - over basically a binary random variable, which
14:57 - is the class label.
14:59 - And so all you have to do is you have
15:01 - to have a softmax at the end that maps it to a probability.
15:05 - But then you can do whatever you want with respect to the X.
15:08 - So you can extract whatever features you want of the input
15:12 - X to come up with a good classifier.
15:15 - And there is really no restriction
15:16 - on the kind of architecture.
15:18 - The goal of this classifier, the way
15:20 - we're going to train this classifier
15:22 - is to maximize the two-sample test statistic to basically try
15:31 - to figure out what kind of statistic
15:33 - basically can maximally distinguish basically
15:37 - between the data and the model, which is the same as minimizing
15:41 - the classification loss.
15:42 - So we're going to train the classifier the usual way
15:45 - to minimize the loss because that's also
15:48 - what gives us the most power to basically distinguish
15:51 - these two distributions, these two set of samples.
15:54 - This is the actual statistic.
15:55 - It could be something like this.
15:57 - So it's actually more like a family
15:58 - of statistics, which are all the ones
16:00 - that you can get as you change.
16:02 - You have a classifier.
16:03 - And then you can imagine changing the parameters
16:05 - of the classifier.
16:06 - And then you can think about if you
16:10 - were to try to find a classifier that maximizes
16:15 - this objective function, which is just minimizing
16:18 - cross-entropy, which you approximate based on data
16:22 - because you only have samples from P data,
16:23 - and you only have samples from the model,
16:25 - you end up with something that looks like this, which
16:28 - is going to be the statistic.
16:29 - Remember before we were just taking the mean of X in S1
16:33 - and the mean of X of the samples in S2.
16:37 - Now, we don't just look at the means.
16:38 - Now we look at what the classifier
16:40 - says on these two samples.
16:42 - And that's what we're going to use.
16:45 - And basically as we discussed before,
16:47 - like if the law, which this is just-- this
16:51 - is the negative loss of the classifier
16:53 - because I'm maximizing.
16:54 - So if this quantity is large, then it
17:00 - means that you're doing a good job at separating them,
17:04 - and it means that they are different.
17:07 - And if the loss is--
17:09 - if this quantity is low, then it means
17:11 - that you're very confused.
17:13 - You're not doing a good job at distinguishing
17:16 - them, which supports the idea that probably they are similar.
17:19 - It's not hard.
17:20 - It's hard to distinguish.
17:22 - What we have here is the--
17:25 - the setting is the one we had before where we're saying
17:28 - we're going to use as a statistic.
17:30 - We're going to use a classifier, which
17:32 - we're going to denote as D5 because it's
17:35 - going to be trainable.
17:36 - It's going to be another neural network
17:38 - that we're going to train.
17:40 - And they're going to train this neural network
17:42 - to try to distinguish between the samples in S1
17:46 - and the samples in S2 where the samples in S1
17:50 - are the-- it's just a sample from a group already sampled
17:55 - from the data distribution.
17:56 - And S2 is just a group of samples from the model
17:59 - distribution, P theta.
18:01 - And if you maximize this objective function
18:05 - over the classifier, you are basically
18:07 - trying to do as well as you can.
18:10 - You're basically just training the classifier the usual way
18:13 - by minimizing cross-entropy.
18:15 - And this is going to be our statistic in the sense
18:19 - that the loss of the classifier will tell us how well,
18:23 - how similar basically these two groups of samples are.
18:28 - And basically, yeah, the discriminator, the phi
18:34 - is performing binary classification
18:36 - with the cross-entropy objective.
18:37 - And you can see that you are going to do--
18:41 - to do well here, what you're supposed to do
18:44 - is you're supposed to assign probability one to all
18:47 - the samples that come from P data,
18:49 - and you're supposed to assign probability zero to all
18:52 - the samples that come from the model
18:53 - if you want to maximize that quantity, which, again, is
18:59 - basically what you would do if you were to train a classifier
19:01 - or distinguish the two groups of samples.
19:03 - And that's just like the negative cross-entropy.
19:09 - So for now, P theta is just the--
19:11 - it's just fixed.
19:13 - It's the model distribution.
19:16 - The data distribution is as usual is fixed.
19:19 - You just have a bunch of samples from it.
19:21 - That's S1.
19:22 - And what we're saying is we're going
19:24 - to try to optimize the classifier
19:27 - to do as well as it can at this task of distinguishing
19:31 - between real samples and fake samples
19:35 - because the loss of the classifier
19:37 - will basically tell us how similar the samples that
19:41 - come from the model are to samples
19:43 - that come from the data.
19:45 - Imagine-- do I have it here?
19:48 - Yeah.
19:49 - So imagine that somehow the--
19:53 - maybe-- yeah, OK.
19:55 - Imagine that P-- that the two distributions are the same.
20:00 - So P theta is the same as P data.
20:03 - Then these two samples would come
20:06 - from the same distribution.
20:08 - So the classifier basically cannot do better than chance
20:11 - because you are literally just taking two groups of samples
20:14 - that come from the same distribution,
20:16 - and then there is no way to distinguish them because they
20:18 - are actually coming from the same distribution.
20:20 - So you cannot do better than chance.
20:22 - And so you would have a high loss.
20:27 - On the other hand, if the samples were very different,
20:30 - the classifier would do a pretty good job of separating them,
20:34 - and then in which case the loss would be small.
20:37 - And so based on that, we can come up
20:39 - with a statistic that would basically
20:41 - say based on the loss of the classifier,
20:43 - we're going to decide whether or not
20:45 - the samples are similar or not.
20:47 - So to the extent that you can separate them well using
20:50 - a classifier, then we say that they are different.
20:53 - If somehow they are all overlapping,
20:55 - and there is no way of coming up with a good decision boundary,
20:59 - then we we're saying, OK, then probably the two samples
21:02 - are similar.
21:03 - For now we're just optimizing phi,
21:05 - and that depends on both clearly.
21:07 - You are right when you optimize with respect to theta,
21:10 - you only care about the second term,
21:13 - and indeed the gradients will only involve that term.
21:16 - Yeah, so the notation here is saying--
21:20 - recall that we have a group of samples, S1,
21:23 - that are coming from P data, which is your training set
21:26 - or a mini batch of samples from P data.
21:29 - And then we have another group of samples S2
21:31 - that are coming from a model distribution P theta.
21:33 - And then we have some kind of objective function here
21:37 - that depends on the model and the discriminator.
21:42 - And what we're seeing is that the discriminator
21:45 - is going to try to optimize this quantity which depends
21:50 - on the model and the discriminator,
21:53 - and it's going to try to maximize it.
21:55 - And that's equivalent to basically trying
21:57 - to do as well as it can at distinguishing
22:00 - real samples from fake samples which are coming from P theta.
22:06 - The reason we have this V is that we're
22:10 - going to also try to then optimize
22:11 - this function with respect to theta
22:13 - because we want to train the generative model.
22:15 - And so what we will show up later
22:17 - is basically a minimax optimization problem
22:20 - where we're going to optimize this V both with respect
22:25 - to theta and with respect to phi.
22:27 - So there's going to be a competing
22:29 - game where the discriminator is trying
22:31 - to optimize this V quantity.
22:34 - It is trying to maximize this quantity,
22:36 - and the model is going to try to minimize that quantity
22:42 - because the model is-- we're trying to make it--
22:44 - we're trying to change P theta to fool the discriminator
22:49 - or try to make the classification problem as
22:51 - hard as possible.
22:52 - So later there will be an outer minimization with respect
22:56 - to theta.
22:57 - And that's how we train the model.
22:58 -
23:03 - Cool.
23:04 - And it turns out that the optimal discriminator actually
23:10 - has this form, which makes sense.
23:16 - This is just the--
23:17 - if you just use Bayes rule, and you compute--
23:20 - what is the true conditional probability of a point X
23:27 - belonging to the positive class, which in this case
23:30 - is, let's say, the data distribution real samples?
23:34 - Well, the true conditional distribution
23:36 - is basically the probability that, that point
23:39 - was generated by the data distribution divided
23:42 - by the total probability that, that point was actually
23:44 - generated by either the model or the real data distribution.
23:52 - And so in particular, you can see
23:54 - that if X is only possible according to the data
24:03 - distribution, then the model should assign--
24:06 - the optimal discriminator would assign one
24:09 - because you have basically one over one plus zero,
24:12 - and then it would be one.
24:15 - While, for example, if the two models are the same,
24:20 - so if a point is equally likely to come from P data or P theta,
24:24 - then this quantity should be one half, which makes sense
24:28 - because if X is equally likely under P theta and the P data,
24:34 - then the best you can do is to say one half probability.
24:38 - While if a point is much more likely to have come from P data
24:42 - because this ratio is large, then the classifier
24:45 - should assign high probability to that point.
24:48 - And if a point is very unlikely to have come from P data
24:52 - because the numerator is small, then the classifier
24:56 - should assign low probability to that point.
24:58 - It's true that the KL divergence is
25:02 - going to be minimized when the two distributions are the same.
25:05 - And that's the global optimum of the KL divergence.
25:08 - So whatever we do, we're still going
25:10 - to go towards that global optimum.
25:12 - In practice, you cannot actually reach it.
25:14 - And so really what matters is that if you have an imperfect
25:18 - model, so you cannot really achieve this,
25:22 - the KL divergence will take some non-zero value.
25:25 - This quantity might take some other non-zero value.
25:30 - The argument could be that perhaps among the suboptimal
25:34 - models, you should prefer one that cannot fool
25:40 - a discriminator as opposed to one that gives you high
25:42 - compression because maybe that's more aligned to what you care
25:46 - about.
25:47 - There are actually variants of way of training generative
25:52 - models that are along those lines
25:55 - where we're going to talk a little bit about that
25:58 - when we talk about noise contrastive estimation.
26:00 - I think is going to be pretty similar to what
26:03 - you're suggesting.
26:04 - So if you have access to a likelihood or part
26:07 - of the likelihood, then you can take advantage of it
26:09 - and try to design.
26:11 - But then that defeats the purpose.
26:13 - As what we'll see is that the main advantage of this
26:16 - is that you don't have to have access to a likelihood.
26:18 - The only thing you need is to be able to sample
26:20 - from the model efficiently, which
26:22 - means that you can use essentially an arbitrary neural
26:25 - network to define the generative process, which
26:28 - is a big advantage of this kind of procedure.
26:31 -
26:37 - Yeah, that's what I was saying.
26:38 - If you check, you can see that if P theta is equal to P data,
26:42 - then this quantity is going to be one half for every X, which
26:46 - basically means that the best you can do,
26:50 - the classifier will output 0.5 for every X, which is indeed
26:55 - the best you can do.
26:57 - If the distributions are the same,
26:58 - you cannot possibly do better than chance.
27:01 - So this is when the classifier is maximally confused,
27:04 - basically.
