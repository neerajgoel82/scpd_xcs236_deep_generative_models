00:00 -
00:05 - SPEAKER: The plan is to essentially introduce
00:09 - ways of training energy-based models that
00:11 - do not require sampling during training at least.
00:15 - And so think of them as alternatives
00:19 - to contrastive divergence, which was
00:21 - an approximation to the KL divergence between data
00:25 - and model.
00:26 - So an approximation to maximum likelihood training,
00:29 - that's how we introduced contrastive divergence.
00:32 - But we'll see is the usual trick that
00:35 - is going to be some other divergence, some other way
00:37 - of comparing model to data where the loss function basically
00:43 - does not involve the partition function.
00:46 - And if we train by that instead of by training
00:49 - by approximating the KL divergence then
00:51 - we get much faster training procedures.
00:54 - And so we'll see a few of them.
00:56 - We'll see score matching, which is kind of the key building
00:59 - block also behind diffusion models, noise contrastive
01:03 - estimation and adversarial training.
01:07 - So recall that we have an energy-based model, which
01:11 - is defined like that.
01:13 - And if you take the log of that expression back,
01:17 - we get this sort of difference between the energy, which
01:22 - is the whatever, the neural network you're
01:23 - using to model the distribution and then you have the log
01:26 - partition function.
01:27 - And the key thing is that the score function
01:31 - or the gradient of the log likelihood with respect to x--
01:35 - so note, this is not the gradient
01:37 - of the log likelihood with respect
01:38 - to theta, which are the parameters of the model.
01:41 - This is the gradient with respect to x.
01:43 - So this is basically, how does the probability
01:46 - change if I were to make small changes to the sample itself?
01:52 - Not how the likelihood would change
01:54 - if I were to make changes to the parameters
01:57 - of the neural network.
01:58 - So this is gradient with respect to x, not with respect to theta.
02:02 - This is also a function of x in the sense that at every axis
02:07 - there is going to be different gradients and a function
02:09 - of theta because the log likelihood
02:11 - itself is parameterized by a neural network with weights
02:14 - theta
02:15 - And just what we just saw before,
02:19 - the gradient of the log likelihood
02:23 - does not depend on the partition function.
02:25 - So here I guess it's showing a little bit better
02:28 - than what I was trying to show before,
02:30 - but if you have the log likelihood
02:33 - is the difference of these two terms,
02:34 - the log partition function is the same for every x
02:38 - or it depends on theta, but it does not depend on x.
02:41 - And so when you take the gradient with respect to x,
02:44 - the log partition function doesn't change.
02:47 - And so the gradient is 0.
02:51 - And so that's why we were able to use the score
02:56 - function or the gradient of the log likelihood
02:58 - in the previous sampling procedure.
03:00 - It's easy to compute if you have access to the energy function f
03:05 - theta.
03:07 - And you can see it here, this kind of idea in play.
03:14 - If you have a Gaussian distribution where as usual
03:17 - the parameters would be the mean and the standard deviation,
03:21 - remember, the partition function is this normalization constant
03:25 - that you have in front, the guarantees that the integral
03:29 - of this function is actually 1.
03:31 - If you take the log, you're going
03:32 - to get the log of the normalization constant
03:36 - and then you get the log of this exponential.
03:38 - And then when you take the derivative with respect to x,
03:41 - you get, again, a function of x and the parameters of the model,
03:45 - which is relatively simple.
03:47 - It's just like x minus the mean scaled by the variance.
03:54 - And if you have a gamma distribution,
03:57 - again, as a potentially nasty normalization constant in front.
04:02 - And the moment you take the score, that normalization
04:07 - constant disappears and you get a much simpler function
04:10 - to work with.
04:13 - And so the intuition is that as theta, which is the score,
04:19 - provides you an alternative view of the original function
04:25 - where you are looking at things from the perspective
04:28 - of the gradient instead of looking
04:29 - at things from the perspective of the likelihood itself.
04:33 - So if you imagine you have a P theta, which
04:35 - is just a mixture of two Gaussians let's say in 2D,
04:39 - so there is a Gaussian here and a Gaussian up
04:41 - here, so it's a mixture of two, so you
04:43 - have this fairly complicated level curves,
04:47 - the likelihood is just a scalar function for every x.
04:50 - It gives you a scalar, which is the height of this
04:53 - of this curve where you can imagine you have two bell
04:56 - curves, one center here and one here.
05:00 - The score is basically the gradient at every point.
05:04 - It's a function that every x gives you the gradient
05:07 - of the log likelihood.
05:09 - And so it's a vector field.
05:10 - You can imagine at every point there is an arrow
05:13 - and the arrow tells you, what is the direction that you should
05:16 - follow if you wanted to increase the log likelihood most rapidly?
05:20 - So as expected, you can see that these arrows are pointing
05:23 - towards the means of the Gaussian,
05:25 - which is what you see here in the sense
05:28 - that if you are at a data point and you want to increase
05:31 - the likelihood, you should push it towards the mean
05:35 - if the model is a Gaussian.
05:37 - Well, they are not necessarily fixed,
05:38 - so we're still going to learn them.
05:40 - But when we take gradients with respect to x--
05:44 - and so theta does not depend on x and so
05:47 - when you take the gradient with respect
05:48 - to x, the log partition function disappears.
05:54 - But we're still going to be learning theta.
05:56 - So here, of course, I'm just showing a snapshot
05:59 - where theta is fixed.
06:00 - And theta would represent the means and the variances
06:03 - of these two Gaussians.
06:04 - And if you change those, the score
06:07 - function itself would change.
06:09 - And you can see it here it's still a function of theta,
06:12 - but it's a simple function of theta that does not depend
06:15 - on the normalization constant.
06:16 - So you can compute it without knowing the relative--
06:22 - you don't need to know relative--
06:24 - remember that the gradient is telling you
06:27 - how the likelihood changes if you were
06:28 - to make small changes to x.
06:30 - And we know how to compare the probabilities of two data points
06:33 - in an energy-based models.
06:34 - So it makes sense that it does not depend on the partition
06:38 - function.
06:39 - So the score function as defined is always
06:41 - a vector field in the sense representing the gradient
06:44 - because by definition it's just the gradient
06:46 - of f theta with respect to x.
06:48 - And so in general, f theta would be much more complicated
06:52 - than a mixture of two Gaussians.
06:54 - And so you can imagine that these arrows would be much more
06:57 - complicated and they might-- if you have probability mass spread
07:00 - out in a complicated way, the gradient could be--
07:04 - I mean, it's still going to be a vector field.
07:06 - It might not have that simple structure where it's just
07:08 - pointing you towards these two points,
07:11 - but it's still always going to be a vector field of gradient.
07:15 - So it's a vector field of gradients
07:17 - if it's defined like this because it's actually
07:19 - a conservative vector field because there
07:20 - is an actual underlying energy function.
07:23 - When we talk about score-based models,
07:25 - we'll see that we'll just use an arbitrary neural network
07:27 - to model this.
07:28 - But for now we are assuming that there is an underlying
07:31 - f theta and an energy function and this is just
07:34 - the vector field.
07:35 - So if you like analogies with physics,
07:37 - you can of f theta as being an electric potential
07:42 - and S theta as being the gradient of that, which is
07:48 - like a field basically.
07:50 - An electric field, they describe the same object,
07:54 - but in slightly different ways.
07:56 - And there is no loss of information,
07:58 - we're just thinking of things in a slightly different,
08:01 - taking a slightly different view.
08:03 - There's going to be beneficial from a computational reason
08:05 - because we don't have to worry about the partition function.
08:08 -
08:11 - OK, so how do we do--
08:15 - the key observation is the score function.
08:18 - Gradient of the log likelihood with respect to the inputs
08:21 - is independent of the partition function.
08:23 - And so the idea is that we're going to define a training
08:28 - objective where we're going to compare two probability
08:32 - distributions or two probability densities,
08:35 - p and q, by comparing their respective vector
08:39 - field of gradients.
08:41 - So the idea is that if p and q are similar, then
08:46 - they should also have similar vector field of gradients.
08:50 - If p and q are similar different axis
08:54 - would have similar gradients.
08:57 - So one reasonable way of comparing
08:59 - how similar p and q are is to say,
09:03 - what is the average L2 difference between the score
09:09 - of p and the score of q?
09:13 - So at every point, we look at, what
09:14 - is the direction that you should follow if you wanted to increase
09:17 - the likelihood of p"?
09:18 - Most rapidly, what is the direction
09:20 - that you should follow if you wanted
09:21 - to increase the likelihood of q most rapidly?
09:23 - And we check how different they are.
09:25 - So it's a vector.
09:26 - So to turn it into a scalar we take the norm of this vector
09:30 - and then we're averaging with respect to p in this case.
09:37 - And what I claim is that you can imagine
09:42 - that this is a reasonable loss function because if p
09:46 - is actually equal to q, then the gradients are
09:49 - going to be the same.
09:50 - So gradient of log p is going to be the same as gradient
09:53 - of log q.
09:54 - This vector is going to be 0 everywhere.
09:56 - And the norm is going to be 0.
09:58 - The average is going to be 0.
10:00 - And so the what's called the Fisher divergence between p
10:02 - and q is also going to be 0.
10:05 - So it's kind of a reasonable way of checking how p and q are
10:10 - different from each other.
10:13 - And crucially, the reason we're doing it
10:15 - is that at the end of the day we're
10:18 - interested in training an energy-based model.
10:20 - So let's say p is going to be the data,
10:23 - q is going to be the model.
10:25 - But crucially, this loss function only involves
10:29 - the scores, it only involves this gradient,
10:31 - which we do not depend on the partition function.
10:33 - So this might give us a loss function
10:36 - that is actually very suitable for energy-based models
10:38 - because it does not require you to know the log partition
10:41 - function of the model, that's why we're looking at this.
10:45 - AUDIENCE: [INAUDIBLE]
10:49 -
10:50 - SPEAKER: So it's a different loss function,
10:52 - it's a different way of comparing probability density
10:56 - functions.
10:56 - They are actually related to each other.
10:58 - So in a certain sense the Fisher divergence
11:01 - is kind of the derivative of the KL divergence in a certain way.
11:06 - So if you take two densities and you convolve them
11:08 - with Gaussian noise and you take the derivative of that
11:12 - with respect to the size of the noise,
11:14 - it turns out that that's the Fisher divergence.
11:16 - But it just think of it as a different divergence.
11:20 - It's not going to be as easy, but that's
11:21 - sort of the idea is that let's define a loss in terms
11:25 - of the score because we know how to compute the score
11:27 - and we don't know how to compute the log likelihood.
11:30 -
11:34 - So that's sort of the score matching idea.
11:37 - p is going to be Pdata, q is going
11:39 - to be the energy-based model, which is parameterized
11:42 - by this energy function.
11:45 - And then if you evaluate that fisher divergence
11:49 - between the data density and the model density,
11:52 - you get this kind of thing or equivalently this thing
11:57 - where you take an expectation with respect
11:59 - to the data distribution of the difference between the gradient
12:02 - of the true data generating process
12:06 - and what's given by the model.
12:09 - And so that's basically we're comparing the gradients
12:12 - of the true data distribution with the gradients
12:16 - of the model, which are things we can compute.
12:19 - And even though p theta is an energy-based model,
12:22 - this loss function only depends on the score,
12:26 - which we know we can compute efficiently
12:28 - without having to worry about the normalization constant.
12:31 -
12:35 - So that's the idea.
12:36 - Now, as was pointed out, it feels
12:39 - like it's not very useful because it's still
12:42 - involves the gradient of the log data density, which
12:47 - we don't know, right?
12:48 - It seems like a reasonable loss function, but not
12:50 - exactly one we can't evaluate or optimize
12:53 - because although we have access to samples from Pdata,
12:56 - so presumably you can approximate
12:57 - this expectation with respect to Pdata with samples,
13:01 - it looks like we need to know the gradient of the log data
13:04 - density which is unknown.
13:07 - If we knew what log Pdata is for every x,
13:10 - then we wouldn't have to build a generative model.
13:14 - Yeah, so that's the expression and the problem
13:17 - is that we only have samples from Pdata.
13:19 - And so it looks like that first term the gradient,
13:22 - the score of the data distribution is unknown.
13:26 - So we don't know how to optimize that objective function and try
13:31 - to make it as small as possible as a function of theta
13:33 - because we cannot compute this first term here.
13:36 -
13:39 - We only have access to samples from Pdata,
13:41 - that's the usual setting in a generative modeling problem.
13:45 - But it turns out that you can rewrite this loss function
13:48 - into an equivalent one that does not,
13:51 - no longer depends on the unknown score
13:55 - function of the data distribution
13:57 - by using integration by parts.
14:00 - So just to see how this works, let's start
14:03 - with the univariate case.
14:05 - So x is just a one dimensional scalar random variable.
14:11 - So the gradients are actually just derivatives.
14:16 - And so just because integration by parts
14:18 - is a little bit easier to see that way
14:20 - I'm still using the gradient notation,
14:22 - but these are actually derivatives.
14:23 - And then the we don't have to worry
14:28 - about the norm of the vector because, again, the derivatives
14:31 - are just scalars.
14:32 - And so the squared norm is just like the difference of these two
14:35 - scalars squared.
14:37 - So that's what the loss function looks
14:39 - like when x is just a single scalar random variable.
14:44 - This basically is the same exact expression
14:46 - except that it's no longer a vector.
14:48 - It's just the difference of two scalars
14:50 - and that's what's happening there.
14:54 - And then we can expand this or by just explicitly writing
15:00 - this out as an expectation with respect to the data
15:03 - distribution.
15:04 - So you go through every x that can possibly
15:07 - happen you weight it with the data density
15:09 - and then you look at the difference
15:10 - between the derivatives of the log data distribution
15:14 - and the log model distribution at every point.
15:18 - So you have these two curves.
15:20 - You look at the slopes at every point and you compare.
15:23 -
15:25 - And you can expand the square.
15:29 - It's a square of a difference.
15:31 - So if you expand it you're going to get three terms.
15:33 - You're going to get a blue term, which is just
15:35 - the square of this first gradient of the log data
15:39 - density squared.
15:40 - Then you have the gradient of the log model density squared.
15:44 - And then you have this red term where you have basically the dot
15:49 - product between the cross product between model and data.
15:54 - And you can see that the first term does not depend on theta,
15:59 - so we can ignore it for the purposes of optimization
16:02 - with respect to theta.
16:03 - We can ignore the blue term.
16:05 - The green term is easy it just depends on the model.
16:09 - So again, we're good.
16:10 - The problem is the red term because that one still
16:15 - involves this gradient of the log data density
16:20 - in some non-trivial way.
16:23 - And what we're going to do is we're going to use integration
16:26 - by parts, which is usually, you remember from basic calculus,
16:30 - it's a way to write the integral of the f prime g
16:33 - in terms of the integral of g prime f
16:36 - basically, which function you're taking derivative with respect
16:41 - to.
16:43 - And we apply that to that red term,
16:46 - which is the annoying term.
16:49 - Recall that this is an expectation with respect
16:51 - to the data of the gradient log data density gradient of the log
16:54 - model density.
16:55 - Now, what is the gradient of the log of p data?
17:00 - Gradient of log is the argument of the log,
17:04 - 1 over the argument times the derivative
17:06 - of the argument of the log.
17:08 - So it should look like this.
17:12 - Just by expanding out this gradient of log
17:15 - p data is 1 over p data times the derivative of p data
17:19 - And the reason we're doing it is that now this p data here
17:23 - and this p data here will cancel.
17:27 - And now it looks kind of where we have something where we
17:29 - can apply integration by parts.
17:31 - So this is the derivative of p data times the derivative
17:36 - of the log p model.
17:39 - And we can apply integration by parts
17:42 - and rewrite it in terms of p data.
17:47 - So here we had a derivative of p data.
17:50 - So we rewrite it in terms of the--
17:52 - just the instead of f prime.
17:54 - We go to f.
17:55 - So p data prime it becomes p data.
17:59 - And then we take an extra derivative
18:02 - of the log on the score of the model.
18:05 -
18:08 - And so we've basically rewritten it
18:12 - in terms of an expectation with respect
18:14 - to the data distribution of a second derivative of the model
18:19 - score, essentially.
18:22 - Now, we still have to deal with this the term here fg, which
18:27 - is the integrand evaluated at the two extremes.
18:32 - And under some reasonable assumptions,
18:34 - you can assume that in the limit as x
18:39 - goes to plus and minus infinity, this p data goes to 0,
18:44 - it's a density.
18:45 - So there cannot be too much probability mass
18:47 - at the boundaries.
18:49 - And if you are willing to make that assumption,
18:52 - this simplifies into something that now basically
18:56 - no longer depends on the score of the data density.
18:59 - It only depends on things we can manage.
19:03 - It's still an expectation with respect to the data density,
19:06 - but it only involves the--
19:08 - it no longer involves the score.
19:09 -
19:13 - And so that's basically the trick.
19:16 - If you are willing to assume that this term here is 0,
19:21 - basically the data distribution decays sufficiently fast then
19:26 - you can use integration by parts and you
19:28 - can rewrite this thing, the original score matching loss,
19:33 - recall it had three pieces.
19:35 - If we apply that trick to rewrite the red term
19:42 - into this brown term that we just derived using integration
19:45 - by parts, now we get a loss function that we can actually
19:50 - evaluate and we can optimize as a function of theta.
19:53 - We have the first term, which is constant with respect to theta.
19:56 - So we can ignore it.
19:57 - We have an expectation with respect
19:59 - to Pdata of the derivative squared.
20:02 - And then we have an expectation with respect
20:04 - to Pdata of the second derivative of the log
20:07 - likelihood.
20:10 - And so this is basically what you
20:13 - can write the two expectations as a single expectation.
20:16 - And now we basically derive the loss function
20:19 - that is equivalent up to a constant
20:21 - to where we started from, but now
20:23 - it only involves things we have access to.
20:26 - It only involves the model score and the further derivative
20:31 - of the model score.
20:33 - It's the second derivative of the log likelihood.
20:36 - But again, derivatives are always with respect to x.
20:39 -
20:43 - And so that's kind of where the magic happens.
20:45 - This is how you get rid of that dependence
20:48 - on the score of the data density.
20:49 - and write it down using elementary calculus
20:53 - into an expression, that is now something
20:55 - we can actually optimize.
20:57 - It can evaluate and optimize as a function of theta.
21:00 - So that's at least in the 1D case.
21:05 -
21:09 - And it turns out that there is something you might have seen it
21:14 - in multivariate calculus.
21:16 - There is an equivalent of integration by parts.
21:19 - There's actually Gauss's theorem where you can basically
21:22 - do the same trick.
21:23 - When you have a vector--
21:26 - so when x is a vector and you really have gradients,
21:29 - you can basically use the same kind of trick
21:32 - and you derive something very similar
21:34 - where instead of looking at the square of the derivative,
21:37 - you have the L2 norm of the gradient.
21:42 - And instead of having just the second derivative of the log
21:45 - likelihood, you have the trace of the Hessian of the log
21:50 - probability.
21:51 - So again, you have to look at second order derivatives,
21:53 - but things become a little bit more complicated
21:56 - when you have the vector-valued kind of function.
22:01 - So the Hessian is basically this matrix n by n
22:05 - if you have n variables where you
22:07 - have all the mixed second partial derivatives of the log p
22:14 - theta x with respect to xi, xj for all pairs of variables
22:20 - that you have access to.
22:21 - So again, a theta expansion up to second order if you want.
22:25 - And so that's how you are basically
22:28 - using the same derivation.
22:30 - We're using integration by parts.
22:32 - You, again, write it down in terms
22:34 - of a quantity that no longer depends on the score of Pdata.
22:40 - And that's an objective function that we can now optimize.
22:45 - If you're willing to approximate this expectation with a sample
22:49 - average, we always have access to samples from Pdata.
22:53 - So we can approximate that expectation using samples.
22:57 - Then you get algorithm or a loss that looks like this.
23:01 - You have a samples of data points
23:05 - that you sample from Pdata training data.
23:08 - And then you can estimate the score matching loss
23:11 - with the sample mean, which would look like this.
23:16 - So you go through individual data points.
23:20 - You evaluate the gradient of the energy at each data point.
23:24 - You look at the square of the norm of that of that vector.
23:27 - And then you need to look at the trace of the Hessian of the log
23:32 - likelihood, which is this, again, Hessian
23:36 - of f theta in this case, which is the model.
23:41 - And then this is now a function of theta
23:45 - that you can try to optimize and minimize with respect to theta.
23:49 - You recall we're trying to minimize.
23:51 - This is equivalent up to a shift independent from theta.
23:55 - It's equivalent to the fisher divergence.
23:57 - So if you're able to make this as small as possible
24:00 - with respect to theta, you're trying
24:02 - to match the scores of the data distribution and the model
24:05 - distribution.
24:07 - This still has issues with respect
24:09 - to very high dimensional settings
24:11 - like the trace of the Hessian.
24:13 - I know it requires higher order differentiation
24:16 - and it's somewhat expensive, but there's
24:19 - going to be ways to approximate it.
24:21 - The key takeaway is that it does not require you to sample
24:24 - from the energy-based model.
24:26 - This is the loss where you just need to training data,
24:29 - you evaluate your neural network,
24:31 - and you need to sample from the energy-based model
24:34 - during a training loop, which is key
24:37 - if you want to get something efficient.
24:39 -
24:41 - And the last function actually have--
24:44 - this is you just brought up then indeed the Hessian is tricky.
24:48 - But it has a reasonable flavor.
24:52 - If you think about it, what is this loss saying?
24:55 - You're try to minimize this quantity as a function of theta.
24:58 - So what you're saying is that you should look at your data
25:01 - points and you should look at the gradient of the log
25:03 - likelihood evaluated at every data point
25:06 - and you're trying to make that small, which basically means
25:09 - that you're trying to make the data points
25:11 - kind of stationary points for the log likelihood.
25:14 - So the data points should either be local maxima or local minima
25:18 - for the log likelihood because the gradients at the data points
25:22 - should be small.
25:23 - So you should not be able to somehow perturb the data points
25:27 - by a little bit and increase the likelihood by a lot
25:30 - because the gradients should be very small
25:33 - evaluated at the data.
25:34 - That's kind of what this piece is doing.
25:37 - And this piece is say trying to loosely trying
25:41 - to make sure that the data points are local maxima instead
25:44 - of local minima of the log likelihood.
25:47 - And to do that you need to look at the second order derivative
25:50 - and that's what that term is doing, which
25:53 - is kind of very reasonable.
25:54 - It's saying if you want to fit the model,
25:56 - try to choose parameters so that the data points are local maxima
25:59 - somehow of the log likelihood.
26:03 - And that can be evaluated just by looking
26:05 - at first order gradients and second order gradients.
26:09 - Yeah, so that's essentially what we're
26:11 - going to do is we're going to--
26:13 - there's two ways of doing it.
26:14 - One is to I guess something called slice score matching
26:18 - where you're kind of taking random directions
26:21 - and you're checking whether the likelihood goes
26:23 - up or down along those directions, which
26:25 - is the same as if you know about the Hutchinson trick
26:28 - for estimating the Hessian, it's basically the same thing where
26:32 - it's an estimator for the trace of a matrix that
26:35 - looks at a random projection around the random direction
26:39 - around it.
26:40 - And the other thing is denoising score matching,
26:42 - which also has this flavor of adding a little bit of noise
26:46 - and checking whether the likelihood goes
26:47 - up or down in the neighborhood of a data point.
26:50 - And so it has that flavor basically
26:54 - and those things are going to be scalable with respect
26:57 - to the dimension.
26:59 - So the question is, has the Hessian an analytical form?
27:01 - If theta is a neural network you can't, there is no close min.
27:06 - You have to use autodiff to basically compute it.
27:10 - The problem is that it needs many
27:11 - backward passes because you're not
27:15 - computing just a single partial derivative,
27:17 - you're computing n partial derivatives with respect
27:20 - to every input because you have to compute
27:24 - all the diagonal elements of the Hessian
27:26 - and we don't of an efficient way of doing
27:28 - it other than doing backprop basically
27:31 - n times, which is also expensive when n is large.
27:35 -
27:39 - But the good thing is this avoids sampling
27:43 - and this is going to be the key building block also for training
27:47 - diffusion models.
27:49 - I just proved you that this is equivalent to the fisher
27:53 - divergence.
27:53 - And the fisher divergence is 0 if and only
27:55 - if the distributions match.
27:58 - So even though, yeah, you might think that this
28:01 - is not quite doing the right--
28:02 - it's not quite the right objective,
28:05 - in the limit of infinite data this would be giving you
28:08 - exactly the-- if you were to optimize it globally,
28:11 - this would give you exactly the data distribution because it's
28:15 - really just the equivalent up to a shift to the true Fisher
28:18 - divergence that we started with, which is this thing here,
28:24 - which is 0 only basically if the densities match.
28:26 -
28:30 - Cool.
28:31 - Now, the other cool technique that you
28:33 - can use for training-- yeah, so that's the takeaway so far,
28:38 - is that approximations to KL divergence around the lead
28:43 - require sampling, too expensive.
28:45 - But you can-- if you are willing to instead measure similarity up
28:49 - here using this Fisher divergence, then again,
28:52 - you get a loss function that is much more,
28:55 - that is very suitable for training energy-based models
28:57 - because it does not require you to--
29:00 - even though it looks tricky to compute and optimize,
29:03 - it actually can be rewritten in terms of something that
29:06 - only depends on the model and you can optimize
29:08 - as a function of theta.

00:00 -
00:05 - SPEAKER: Now, there is another way
00:07 - of training energy-based models, which
00:09 - is going to be somewhat loosely similar to generative
00:13 - adversarial networks, which is essentially a way
00:17 - to fit an energy-based model by instead of contrasting data
00:24 - to samples to them from the model,
00:26 - we're going to contrast the data to samples from some noise
00:31 - distribution, which is not necessarily
00:34 - the model distribution itself.
00:37 - And so that's how it works.
00:40 - You have the data distribution.
00:42 - And then there's going to be a noise distribution, which
00:44 - is any distribution you can sample from
00:47 - and for which you can evaluate probabilities.
00:52 - And what we're going to do is we're essentially
00:56 - going to go back through the GAN idea of training a discriminator
01:00 - to distinguish between kind of data samples and noise samples.
01:07 - So far there is no energy-based models, just
01:09 - the usual GAN-like objective.
01:15 - And the reason I'm bringing this up
01:18 - is that if you have the optimal discriminator, then
01:21 - you would somehow get this density
01:23 - ratios between the noise distribution and the data
01:26 - distribution.
01:28 - So recall that if you train a discriminator optimally
01:32 - by minimizing cross-entropy, and so if you're
01:35 - trying to discriminate between real data and samples
01:40 - from the noise distribution, what
01:44 - is the optimal discriminator?
01:46 - It has to basically give you the density ratio.
01:48 - For every X, it has to be able to know
01:50 - how likely X is under data and how likely X is under the noise
01:55 - distribution--
01:57 - so useful recap for the midterm.
02:01 - This is the optimal discriminator
02:04 - is the density ratio between--
02:06 - for every X, you need to figure out
02:08 - how likely it is under the data versus how likely it
02:10 - is under the data and the alternative noise distribution.
02:16 - And the reason I'm bringing this up
02:19 - because what we could try to do is we
02:23 - could try to basically parameterize
02:27 - the discriminator in terms of our generative model, which
02:31 - could be an energy-based model.
02:33 - So we know that the optimal discriminator
02:36 - has this form, P data over P data plus noise distribution.
02:40 - And so we could try to just define a discriminator.
02:44 - So instead of having your whatever MLP, whatever
02:48 - neural network you want to discriminate between data
02:52 - versus noise, we're going to define
02:55 - a special type of discriminator where
02:59 - when we evaluate the probability of X being real,
03:04 - being real data, we get the number.
03:06 - Instead of just feeding X through a neural network
03:09 - arbitrarily, we get it by evaluating the likelihood of X
03:14 - under a model P theta versus the probability
03:20 - under the noise distribution, which again we're
03:24 - assuming is known because we're generating the noise
03:26 - distribution, the noise data points ourselves.
03:31 - And so the good thing is that if you could somehow come up
03:36 - with the optimal discriminator that distinguishes data
03:39 - versus noise, we know that the optimal discriminator will
03:44 - have this form.
03:45 - And this has to match the P data or P data plus noise.
03:51 - And so you can see that somehow if this classifier is
03:54 - doing very well at distinguishing data from noise,
03:57 - it has to learn--
03:59 - basically P theta has to match P data.
04:01 -
04:04 - So the classifier is forced to make decisions
04:07 - based on the likelihood of X under P theta.
04:10 - And then if it's able to make good decisions,
04:13 - then this P theta has to match the data distribution basically.
04:17 -
04:21 - That's essentially the trick that we're leveraging here.
04:26 - And then what we're going to do is
04:28 - we're going to actually parameterize the P theta using
04:30 - an energy-based model.
04:31 -
04:35 - But that's the key idea.
04:38 - Instead of using an arbitrary neural network
04:41 - as the discriminator as you would do in a GAN,
04:44 - we're defining a discriminator in terms
04:46 - of another generative model.
04:48 -
04:52 - And the idea is that by training the discriminator the usual way
04:57 - by minimizing cross-entropy loss,
04:59 - we're forcing it to learn a P theta that matches the data
05:05 - distribution because that's the only way it can do well
05:09 - at this binary classification task.
05:11 - It really needs to know which x's are likely under P data
05:15 - to get good cross-entropy loss.
05:18 - And that's only possible when P theta matches P data.
05:23 - And we're going to see that this is suitable
05:26 - when P theta is defined up to a constant, when P theta is going
05:29 - to be an energy-based model.
05:32 - So, well, yeah, maybe let me skip this
05:37 - since we're running out of time.
05:38 - But you can also use the classifier
05:40 - to correct the noise distribution.
05:42 - But for now, let's assume that P theta is an energy-based model.
05:48 - So we're going to parameterize P theta
05:51 - in that previous expression in terms of an energy usual trick.
05:56 - Let's define up to a constant.
05:58 - And what we're going to further do is in general,
06:05 - this normalization constant Z theta
06:07 - is a function of the parameters F theta,
06:10 - and it's a complicated function because we
06:13 - don't know how to compute that integral, that
06:15 - sum over all possible things that can happen.
06:18 - So what we're going to do is we're
06:21 - going to treat Z theta as being an additional trainable
06:24 - parameter.
06:27 - So not only we're going to optimize F theta,
06:30 - but we're going to treat Z theta itself
06:32 - as an additional trainable parameter which is not
06:35 - explicitly constrained to take the value of the normalization
06:38 - constant.
06:40 - So you're going to be some other scalar parameter
06:42 - that we can optimize over.
06:45 - So if you do that, then basically the density model
06:49 - that we're going to use in the classifier now depends on theta
06:52 - and depends on Z.
06:56 - And then we just plug this.
06:58 - The idea is that basically if we plug-in this expression
07:04 - into the classifier, into the discriminator,
07:07 - and we train the discriminator the usual way by minimizing
07:11 - cross-entropy, we know that under the optimal parameters,
07:15 - this classifier will have the density model that we're
07:21 - using to build the classifier will have
07:23 - to match the data distribution.
07:25 - And what this means is that the optimal theta and the optimal Z
07:30 - are going to be such that the energy-based model is
07:34 - equal to the data distribution.
07:36 - But crucially now Z is just a learnable parameter.
07:41 - It happens to be the correct partition function in the limit
07:45 - because you take the integral of both sides with respect to X,
07:50 - you're going to see that the integral
07:52 - of this optimal energy-based model
07:55 - is equal to the integral of the data, which
07:57 - is one by definition.
07:59 - So even though we treat Z as a learnable parameter,
08:05 - in the limit of learning an optimal classifier,
08:09 - this learnable parameter that is not
08:11 - constrained to be the actual partition function
08:14 - will take the value of the true partition function of the model
08:19 - because that's what like the optimal classifier
08:21 - should do if it does really well with this binary cross-entropy
08:26 - classification loss.
08:28 - The loss function which ends up being--
08:33 - let's see-- something like this.
08:35 - So if you plug it in, recall we're basically
08:38 - saying we're going--
08:40 - instead of picking an arbitrary neural network
08:43 - for the discriminator like in a GAN,
08:45 - we're going to pick a neural network that
08:46 - has a very specific functional form
08:49 - so that when you evaluate what is the probability that X
08:52 - is real, you have to get it through this kind
08:54 - of computation.
08:55 - But you have an energy-based model that tells you
08:58 - how likely X is under the model where both F theta and Z are
09:04 - learnable parameters.
09:06 - And then if you just multiply numerator and denominator by Z,
09:10 - you get an expression that, again, as it should,
09:14 - depends on F theta and Z and the noise distribution, which
09:17 - is known, Pn.
09:19 - The noise distribution is, again,
09:20 - something that we are deciding.
09:22 - You can pick whatever you want as long as you can sample
09:25 - from it and you can evaluate probabilities
09:27 - under the noise distribution.
09:30 - And then literally what we do is we still
09:33 - train the classifier by doing binary classification
09:37 - with cross-entropy loss.
09:40 - Just like in a GAN, we have data.
09:42 - We have real data.
09:43 - We have fake data which is generated
09:45 - by this noise distribution, which we decide ourselves.
09:48 - So this is different from a GAN.
09:50 - The fake data is coming from a fixed noise distribution.
09:54 - So we're contrastive.
09:56 - We're contrasting the real data to fake samples generated
10:00 - by the noise distribution, and we're
10:02 - training the classifier to distinguish between these two.
10:06 - And the classifier has this very specific functional form
10:10 - where it's defined in terms of an energy-based model
10:14 - where the partition function is itself a learnable parameter.
10:19 - And then we optimize this with respect
10:21 - to both theta and Z trying to do as well as we can
10:25 - at this classification task.
10:27 - What happens is that in theory, this
10:31 - works regardless of what is the noise distribution.
10:34 - In practice, what you want is you want a noise
10:37 - distribution that is very close to the data distribution
10:41 - so that the classifier is really forced
10:43 - to learn what makes for a good sample,
10:47 - what makes for the real--
10:48 - what kind of structures do the real samples have.
10:52 - At the end of the day, what you learn is
10:53 - you learn an energy-based model.
10:55 - So you learn an F theta, and you learn a partition function.
10:58 - And in the limit of infinite data, perfect optimization,
11:03 - then if you optimize this loss perfectly,
11:06 - the energy-based models matches the data distribution.
11:09 - And the partition function, which
11:11 - is just the value of these learnable parameters Z
11:13 - that you get actually is the true partition function
11:17 - of the energy-based model.
11:18 - So even though we're just training it
11:20 - in an unconstrained way, so there is no relationship here
11:23 - between theta and Z, it just so happens
11:27 - that the best thing to do is to actually properly normalize
11:30 - the model where Z theta becomes the partition function
11:34 - of the energy-based model.
11:36 - So in principle, this does the right thing.
11:39 - In practice, it heavily depends on how good
11:42 - the noise distribution is.
11:44 - So there is no generator.
11:47 - The generator is fixed.
11:48 - Or you can think of it as being fixed.
11:49 - So the noise distribution would be the generator,
11:52 - and that's fixed.
11:54 - We are training a discriminator, but it's
11:56 - a very special discriminator.
11:58 - So you are not allowed to take x and then fit it
12:01 - through a ConvNet or an MLP and then
12:04 - map it to a probability of being real versus fake.
12:07 - You have to get the probability by using this expression.
12:11 - There is only a discriminator.
12:12 - Once you've trained it, you can extract an energy-based model,
12:16 - which is the f theta, from the discriminator.
12:19 - So in this flavor, which is the simpler version,
12:22 - the noise distribution is fixed.
12:24 - We'll see soon, if we have time, in a few couple
12:26 - of slides that indeed it makes sense to change the noise
12:29 - distribution in trying to adapt it and make it
12:32 - as close as possible to the data or the current best
12:36 - guess of the model distribution.
12:38 - So, yeah, that's an improvement over this basic version
12:42 - of things where the noise distribution is fixed for now.
12:45 - So we're assuming that the noise distribution is something you
12:48 - can sample from efficiently.
12:49 - So you can always basically get some stochastic gradient ascent
12:56 - here on this.
12:57 - Once you train them-- so the learning is fine.
12:59 - It's efficient.
13:00 - As long as Pn is efficient to sample form,
13:03 - you never have to sample from P theta.
13:06 - Once you've trained a model, you have an EBM.
13:09 - So if you want to generate samples from it,
13:12 - you have to go through the MCMC Langevin.
13:15 - So at inference time, you don't get any benefit.
13:18 - This is just at training time.
13:19 - This loss function does not involve sampling from the model.
13:23 - It's fair game in the sense only to the point-- to the extent
13:26 - that in the limit, you will learn the partition function.
13:30 - In general, you will not.
13:32 - So the solution to this optimization problem
13:35 - will give you a Z. In practice, that is not the true partition
13:38 - function of the model-- is just going to be an estimate.
13:41 - And you're going to end up with an energy based model that
13:43 - is suboptimal because you're short of the Z
13:47 - that you estimated is not the true partition
13:50 - function for that model.
13:52 - So when you have finite data, imperfect optimization,
13:56 - there are-- you pay a price for this approximation.
13:59 - But in the limit of things being perfect,
14:04 - this is not an issue, basically.
14:06 - Yeah, so if you have infinite data,
14:10 - and somehow you're able to perfectly optimize over
14:13 - theta and Z, then we know that the optimal solution
14:16 - over theta and Z will be one where this matches the data
14:20 - distribution.
14:21 - And so the only way that for that to happen is for Z star
14:26 - to be the true partition function
14:28 - of that energy-based model.
14:30 - But in practice, this is not going to happen.
14:32 - So you just get an estimate.
14:34 - If it's not a true partition function,
14:36 - you still have an energy-based model
14:38 - for which there is going to be a real partition function.
14:42 - It's just not the one you've estimated.
14:43 - So F theta still defines a valid energy-based model.
14:47 - It's just that the partition function for that model
14:50 - is not going to be the solution to this optimization
14:54 - problem over Z. So it's not going to satisfy the constraint.
14:58 - But there's going to be a partition function for that F
15:00 - theta.
15:01 - So that's going to be a valid energy-based model.
15:03 - So it turns out that they are actually very much related.
15:06 - And then if the noise distribution
15:09 - is like what you get by perturbing data
15:13 - by adding a little bit of Gaussian noise essentially,
15:16 - then this turns out to be exactly denoising
15:18 - score matching.
15:19 - So it very much depends on the noise distribution
15:23 - that you choose.
15:24 - But there are instances where this becomes exactly a score
15:28 - matching.
15:29 - So I don't think it's fair to say that this is always bad.
15:31 - It's just different thing.
15:33 - So either you do contrastive divergence
15:36 - where you would sample from it.
15:38 - And so in some sense, it involves the partition function
15:40 - in the sense that you would estimate the gradient of the log
15:43 - partition function by using samples from the model,
15:45 - but that's also too expensive.
15:47 - Or that's exactly what we're doing right now.
15:49 - Let's come up with a training objective that does not depend
15:51 - on the partition function.
15:52 - So it's going to be efficient.
15:54 -
15:58 - Cool.
15:59 - And then, yeah, so then for numerical stability,
16:03 - let me see what do I have here.
16:05 - So, yeah, that's the objective.
16:07 - And then you plug-in the expression for the discriminator
16:13 - in here, and you get a loss that looks like this.
16:15 - And you have the log of a sum of two things.
16:19 - So for numerical stability, it's actually easier
16:22 - to use the LogSumExp trick where the log of e of theta plus XPN,
16:29 - which is what you have in the denominator, it's
16:32 - more numerically stable to write as a LogSumExp.
16:36 - But then practically speaking, the implementation
16:39 - is very simple.
16:40 - You start with a sample, a batch of data points.
16:44 - You have a batch of noise samples, and you just do--
16:49 - basically you have this classifier
16:53 - which has a very specific functional form
16:54 - and just you evaluate the cross-entropy loss
16:57 - of that classifier on this mini batch which happens
17:00 - to have this functional form.
17:03 - And then you optimize it as a function of theta and Z.
17:09 - And that's just basically what we had before.
17:11 - And so you're evaluating the loss
17:15 - of the classifier over these two batches of real and fake
17:19 - or real and samples from the noise distribution,
17:22 - then you try to maximize these as a function of theta and Z,
17:27 - and, yeah, stochastic gradient ascent with respect to theta
17:30 - and Z. And, again, key thing, you don't need to sample
17:35 - from the model.
17:36 -
17:38 - And you can see that the dependence on Z
17:41 - is non-trivial in the sense that sometimes it's
17:44 - not optimal to just make Z as small as possible or as
17:47 - big as possible.
17:49 - It depends on Z on some non-trivial way.
17:50 - And so there is some interesting learning happening here
17:54 - over both theta and Z.
17:59 - But at the end of the day, yeah, you
18:01 - end up with an estimate of the energy of the model f theta
18:05 - and an estimate of the log partition function.
18:07 - And everything can be trained without using samples
18:11 - from the energy-based model.
18:13 - So it looks a lot like a GAN, Generative Adversarial Network,
18:19 - in the sense that in both cases you are training a discriminator
18:22 - with binary cross-entropy.
18:23 - So that part is the same.
18:25 -
18:27 - Both are likelihood-free.
18:30 - We don't have likelihoods in EBM, so it better be.
18:33 - There is never a need to evaluate likelihoods
18:36 - under the EBM or under the data distribution
18:40 - because we don't have either of them.
18:42 - So it's all just like a standard cross-entropy loss basically
18:45 - on a classification task reduced to a discriminative modeling,
18:49 - generative model to discriminative
18:51 - classifier training.
18:54 - The key difference is that in the GAN,
18:57 - you are actually have a minimax optimization where you are also
19:01 - training the noise or you're training the generator.
19:03 - Here we are not.
19:04 - So here this is table, is easy to train.
19:07 - The noise distribution is fixed, and you're just
19:10 - maximizing that objective function as a function of theta.
19:13 - It's non-convex, but there is no minimax.
19:15 - There is no instability.
19:17 - It's actually relatively stable to train.
19:19 -
19:22 - And the kind of slight difference
19:26 - is that in noise contrastive estimation,
19:28 - you need to be able to evaluate the likelihoods
19:30 - of the contrastive samples that you generate from the noise
19:33 - distribution while in a GAN, you just need to be able to sample
19:37 - from the generator.
19:38 - So if you look at the loss here, we
19:40 - need to be able to evaluate-- when we generate
19:43 - from Pn from the noise distribution,
19:44 - we also need to be able to evaluate how
19:46 - likely these noisy samples are.
19:49 - In a GAN, you don't have to.
19:50 - You just need to be able to generate them fast.
19:53 - So that's slightly different.
19:55 - And when you're training an NCE model,
20:01 - you just train the discriminator.
20:02 - And then from the discriminator, you
20:04 - get an energy function which defines an energy-based model.
20:07 - While in a GAN, you're actually training deterministic sample
20:12 - generator.
20:13 - So the outcome of the learning is going to be different.
20:17 -
20:20 - And maybe the last thing that I'll say is that kind of what
20:23 - was suggested before is that it might make sense
20:27 - to adapt the noise distribution as you go during training.
20:33 - And so instead of keeping a fixed noise distribution,
20:37 - we can try to learn it jointly with the discriminator.
20:41 - So recall we need an energy-- we need a noise distribution
20:44 - that we can sample from efficiently
20:46 - and we can evaluate probabilities over efficiently.
20:49 - And so the natural candidate is a flow-based model for this.
20:54 - And intuitively, we're training the noise distribution
20:59 - to make the classification problem as
21:00 - hard as possible so that the noise distribution is close to P
21:06 - data.
21:07 - And so the flow contrastive estimation
21:10 - is basically this idea where the noise distribution
21:12 - is defined by a normalizing flow with parameters phi.
21:18 - And then it's basically the same,
21:21 - except that now the discriminator
21:24 - depends on the noise distribution, which
21:25 - is a flow model.
21:26 - So it will depend on the parameters of the flow.
21:29 - A flow model you can sample from efficiently.
21:31 - You can evaluate likelihoods efficiently so
21:33 - it fits with this API.
21:36 - And then now we optimize the discriminator over theta
21:40 - and Z the usual way by noise contrastive estimation.
21:44 - And then what they propose is to train the flow model
21:50 - in a minimax way so it goes back to GANs in some way by train
21:56 - the flow model to confuse the discriminator as
21:59 - much as possible.
22:01 - So that's their proposal.
22:04 - In the end, they use the flow model.
22:06 - So here are some samples.
22:07 - And they are actually generated from the flow model
22:10 - although technically, they get both.
22:12 - They get an energy-based model, and they get a flow model,
22:14 - and they show that for some things,
22:16 - you're better off using the energy-based model.
22:19 - But, yeah, you get both at the end of the day.
22:21 - Yeah, so basically, noise contrastive estimation
22:24 - where the noise distribution is a flow that
22:26 - is learned adversarially recall that the inside--
22:31 - this max here inside is basically the loss
22:34 - of a discriminator in a GAN.
22:36 - It tells you how confused the discriminator is and so--
22:40 - well, not how confused, how not confused.
22:42 - And so by minimizing it, you're trying
22:46 - to make the life of the discriminator
22:48 - as hard as possible.
22:49 - And so you're learning something by minimizing a two-sample test
22:52 - essentially.
22:53 - And so it's the same as the usual GAN training.

00:00 -
00:05 - SPEAKER: Today, we're going to start talking about score based
00:09 - models or diffusion models.
00:11 - And we're going to see that which is a state of the art,
00:16 - class of generative models for images, video, speech, audio,
00:21 - a lot of different continuous data modalities.
00:24 - This is the way to go.
00:25 - And we'll see it's going to build
00:27 - on some of the techniques we talked about
00:30 - in the last lecture.
00:33 - So first of all, usual picture here.
00:38 - The overview of what we're talking about in this class,
00:44 - we've talked a lot about different model families
00:48 - and we've seen two main classes of generative models.
00:52 - We've seen likelihood based models
00:56 - where basically the key object you're working with
00:59 - is the probability density or the probability mass function.
01:02 - So the model is basically just a function
01:04 - that takes as input some x.
01:07 - And maps it to some scalar which is how likely
01:09 - is that x according to the model.
01:12 - And we've seen that probability mass functions or probability
01:15 - density functions are tricky to model.
01:17 - Because they have to be normalized.
01:19 - They have to integrate to one.
01:21 - So we've seen that one way to get there is
01:23 - to use autoregressive models.
01:25 - Another way to get there is use flow models.
01:27 - But that's always constrains the architectures you can use.
01:32 - And the alternative way to go around it is to sort of, well,
01:39 - give up in some sense on the normalization.
01:42 - And use variational tricks to essentially evaluate
01:46 - the likelihood.
01:47 - So we've seen variational autoencoders.
01:49 - And we've seen energy based models
01:51 - where you have to deal with this normalization constant that
01:54 - normalizes the probability density.
01:57 - And we've talked about a bunch of techniques
01:59 - to try to get around the fact that you
02:03 - have to evaluate Z theta and maybe avoid
02:05 - likelihood based training.
02:07 - And various ways of training energy based models.
02:10 - And then the pros here is that you
02:14 - can do maximum likelihood training which
02:15 - in principle is great.
02:17 - It's a lot that you can monitor.
02:19 - You can see how well it goes.
02:21 - It's optimal in a certain sense.
02:23 - You can compare models.
02:25 - But you have to deal with the restricted architectures.
02:28 - You can't plug-in an arbitrary neural network
02:30 - to model the likelihood.
02:33 - The alternative way to go about this
02:35 - is to just model the sampling process.
02:37 - So this is kind of an implicit generative model like a GAN
02:40 - where we're just going to describe
02:42 - the way you produce samples.
02:44 - For example, you feed random noise through a neural network.
02:48 - Essentially, any neural network, you
02:50 - can pick as the generator defines a valid sampling
02:53 - procedure.
02:55 - The problem is that given a sample, given an output
02:57 - from this network, evaluating how likely the model is
03:01 - to generate that is very hard.
03:03 - And so you have to give up on likelihoods again.
03:06 - And although these models tend to work pretty well.
03:11 - The key problem is that you can't train them
03:13 - in a very stable way.
03:14 - You have to do minimax optimization
03:16 - and that's a problem.
03:19 - And so what we're going to talk about today is
03:22 - a different way of representing probability distributions,
03:26 - probability densities that deals with the score.
03:29 - That's what these models are going to be.
03:31 - They're going to-- they're called
03:32 - score based generative models.
03:34 - And this is only going to be applicable to probability
03:39 - density functions.
03:40 - So continuous kind of random variables.
03:43 - But when we're dealing with continuous random variables,
03:45 - then we can start thinking about working
03:48 - with the gradient of the log density
03:53 - instead of working with the density itself.
03:56 - So we've seen that in a likelihood based model.
03:59 - You would normally work with p of x.
04:02 - And score based model instead, the object that you work with
04:06 - is the gradient of the log density.
04:08 - And the gradient again, is with respect to the inputs
04:11 - is not with respect to the parameters of your model.
04:16 - And that's the score function.
04:18 - And we've seen this in the previous lecture.
04:21 - But the idea is that it provides you
04:24 - an alternative interpretation of the probability density
04:28 - function.
04:29 - You can alternatively think of the PDF
04:31 - as a function that maps every point to a scalar which
04:35 - is non-negative.
04:36 - So you can think of it as the height of some surface
04:40 - over this 2D space.
04:41 - In this case, it's a mixture of two Gaussians.
04:43 - And the score is just a function that is vector valued.
04:48 - At every point, it gives you the gradient of the log density.
04:51 - And so it's kind of like a vector field
04:53 - where at every point, you get the arrow is telling you,
04:57 - what's the direction that you should
04:58 - follow if you want to increase the log likelihood most rapidly?
05:03 - And these two are sort of like equivalent views.
05:06 - So if you like again analogies with physics,
05:09 - this is kind of like describing a physical system
05:12 - in terms of electric potentials or electric fields that
05:15 - kind of like are the same.
05:17 - But computationally, it might be advantageous
05:20 - as we'll see to work with one versus the other.
05:25 - And in particular, the main challenge
05:30 - that we talked a lot about in this course when
05:33 - modeling probability density functions is
05:35 - that you have to make sure that these PDFS are normalized.
05:39 - So you need to figure out a way of parameterizing curves
05:42 - that are ideally flexible.
05:45 - And they can have arbitrary shapes
05:47 - as you change or as complicated as possible of a shape
05:51 - as you can get by changing the parameters
05:53 - of your neural network.
05:54 - But somehow you need to make sure
05:55 - that the total area under the curve is fixed.
05:59 - It's equal to one.
06:00 - So you have a normalized object or some way of somehow computing
06:04 - the area under the curve for any choice of the parameters.
06:08 - And that's potentially tricky as we've seen.
06:11 - Often, what it means is that you have
06:14 - to choose very specific architectures that
06:17 - allow you to basically either guarantee
06:19 - that the area under the curve is one
06:21 - or somehow like in a normalizing flow
06:24 - that you can compute it efficiently.
06:27 - And now if you think about the score, in the one case,
06:32 - the score is just--
06:33 - this is the gradient, is just the derivative
06:35 - of the function you see on of the log of the function
06:38 - you see on the left.
06:40 - And the function on the right no longer
06:44 - needs to satisfy any normalization constraint.
06:48 - So, and it's potentially much simpler to work with.
06:52 - You see here, this relatively complicated curve
06:55 - on the left and the corresponding score
06:57 - function on the right is potentially much easier
07:02 - to work with.
07:02 - So the intuition behind a score based model
07:05 - is that instead of modeling data using the density,
07:08 - we're going to model data using the score.
07:11 - So that's going to be the object that we're
07:13 - going to use to define our model family.
07:16 -
07:19 - And we've seen that this is useful in the context of energy
07:25 - based models.
07:26 - Energy based models are one way of defining
07:29 - very flexible probability density functions by saying,
07:33 - OK, I'm going to pick an arbitrary neural network.
07:35 - I'm going to make it non-negative.
07:36 - And then I'm going to renormalize
07:38 - by dividing somehow computing the total area under the curve.
07:42 - And then dividing by the number to get a valid probability
07:45 - density function.
07:47 - Super flexible, the problem is that if you
07:50 - want to do evaluating likelihoods involve the log
07:53 - partition function.
07:54 - So if you want to do maximum likelihood training,
07:57 - you have to go through either somehow estimate
07:59 - a partition function.
08:01 - Or you need to do contrastive divergence things
08:04 - where you have to sample from the model, which is expensive.
08:08 - On the other hand, which is something
08:10 - you don't want to do-- on the other hand what we're
08:12 - seeing is that we can train energy based models
08:15 - by matching-- instead of trying to match basically the density
08:22 - ratios using KL divergences.
08:24 - We can try to fit our energy based model to-- by trying
08:28 - to make sure that the vector--
08:29 - the corresponding vector field of gradients.
08:31 - So the scores of the model match the scores
08:35 - of the data distribution.
08:37 - And so, and recall that this was basically the Fisher divergence.
08:44 - And we were able to do through integration by parts.
08:48 - We were able to rewrite this objective function
08:50 - into one that basically only involves the score.
08:55 - Which as we've seen in the last lecture,
08:59 - does not require you to compute the partition function.
09:02 - So the score here.
09:05 - The critical thing to notice here
09:07 - is that the score function, the gradient of the log density
09:11 - according to the model, when you take the log of an EBM,
09:15 - you get your neural network.
09:16 - And then you get the log partition function.
09:18 - Critically, the log partition function does not depend on x.
09:21 - It's the same for every point.
09:22 - It's just the area under the curve.
09:24 - No matter where you are, the area under the curve
09:26 - is the same.
09:27 - And so when you take the gradient with respect to x,
09:29 - that's 0.
09:30 - And so we can compute this model score
09:34 - in terms of the original energy of the model.
09:38 - So in this expression here, we can basically compute this term
09:42 - efficiently without having to deal with the normalization
09:46 - constant.
09:48 - And so we have this expression.
09:51 - If you want to do score matching for an energy based model,
09:53 - you have that loss which you can in principle optimize.
09:58 - And try to minimize as a function of theta.
10:02 - And now you might wonder.
10:05 - I mean, can we only do score matching for EBMs.
10:08 - And if you think about it.
10:11 - It's a-- if you look at the loss,
10:14 - it's something that is well defined
10:16 - for any model family, rightt?
10:20 - As long as you're able to compute
10:22 - this gradient with respect to x of the log density
10:25 - according to the model, then you can do score matching.
10:28 - And you can train a model by minimizing the Fisher
10:31 - divergence.
10:33 - So in particular, what other model families can we apply?
10:38 - Score matching too, well, we can certainly
10:40 - apply it to continuous autoregressive models.
10:43 - If you can compute the log density,
10:45 - you can probably also differentiate through that
10:48 - and compute the score.
10:49 - You can do it on a normalizing flow models.
10:52 - Again, we can compute the log likelihood.
10:56 - And so we can also compute the score.
10:58 - Although, perhaps it doesn't make a lot of sense because you
11:01 - have access to the likelihood.
11:03 - So you might as well train these models by maximum likelihood.
11:05 - But in principle, you could apply score matching
11:08 - to these models.
11:09 - And you could train them that way as well.
11:12 - So but you could also wonder.
11:15 - I mean, what's the most general model family that we
11:17 - can train using score matching?
11:20 - And you can think that while you can certainly
11:24 - apply it to autoregressive models, to flow models,
11:28 - you can think of EBMs as a generalization where
11:33 - autoregressive models and flow models are
11:35 - special kind of EBMs where the partition function is guaranteed
11:38 - to be 1.
11:40 - But perhaps, there is something even larger.
11:43 - We can even optimize over an even broader
11:45 - set of model family.
11:50 - And that's the idea behind a score based model.
11:54 - Instead of modeling the energy, we're basically directly going
11:59 - to model the score function.
12:01 - So we're going to define our model family by defining--
12:08 - by basically specifying the corresponding vector
12:12 - field of gradients.
12:14 - So the model is not going to be a likelihood,
12:16 - the model is not going to be an energy.
12:18 - The model is going to be a vector valued function or a set
12:22 - of vector valued functions as you change theta as you change
12:24 - your neural network, you're going
12:26 - to get different vector fields.
12:29 - And that's what we're going to use to describe basically
12:33 - the set of possible distributions
12:36 - that we are going to be fitting to our data
12:39 - distribution in the usual way.
12:43 - And so the-- basically that the difference
12:47 - with respect to an EBM is that we're not
12:49 - going to model necessarily the energy
12:50 - and then take the gradient of it.
12:52 - Instead, we're going to directly think
12:54 - about different kinds of vector fields that we can get.
13:00 - And we can parameterize using a neural network.
13:03 - And in this case the neural network
13:04 - is a vector valued function for every x s theta.
13:09 - The estimated score at that point
13:11 - is a vector with the same number of dimensions as the input.
13:15 - So as theta is really a function from rd to rd.
13:19 - So if you have d dimensions, the output of this neural network
13:23 - will also have d dimensions.
13:24 - Because that's how however many coordinates
13:28 - you need to specify one of these arrows at every point.
13:32 -
13:35 - And so that's basically the kind of very high level story here.
13:43 - As usual we want to fit a model to a data density.
13:48 - So there is a true underlying data density that is unknown.
13:51 - We assume we have access to a bunch of samples from the data
13:54 - density.
13:56 - And then what we're going to try to do
13:58 - is we're going to try to find some function in our model
14:05 - family.
14:05 - So we're going to try to choose parameters theta
14:08 - or we're going to try to choose some vector field of gradients.
14:12 - That is hopefully as close as possible to the vector
14:15 - field of gradients of the original data density.
14:19 - So that's going to be the learning objective
14:23 - and try to choose parameters theta such
14:25 - that the corresponding function-- vector valued
14:28 - function that we get matches the true vector field of gradients
14:32 - of the data density.
14:34 - The only thing we have access to are
14:36 - samples and so we don't have access to the true density.
14:38 - And so we're never going to be able to achieve this perfectly.
14:41 - And there is a learning element in the sense
14:44 - that we only have access to a bunch of samples.
14:47 - So we need to make sure we're not overfitting.
14:50 - And we need to make sure that there's
14:53 - going to be some limits to how well we can do this.
14:56 - But it's that you have the same problem even if you have a--
15:00 - if you're training by maximum likelihood.
15:02 - You're only given samples.
15:03 - You can try to get as close as possible to the empirical data
15:07 - distribution hoping that by fitting the samples,
15:12 - you're also fitting the true underlying data density.
15:15 - So we're going to have the same problem in the sense
15:18 - that we only have samples.
15:19 - We have limited data.
15:21 - But the main difference is that instead
15:23 - of trying to fit one of these scalar function
15:27 - that is giving us the likelihood.
15:29 - We're going to try to fit this vector valued function
15:32 - that is giving us the gradient of the log likelihood
15:36 - essentially.
15:37 - In both cases, it's a hard problem.
15:39 - I would say that even if you work with likelihoods,
15:41 - you don't just want to put probability
15:43 - mass around the training data.
15:46 - Because you want the model to generalize to unseen data
15:51 - that it hopefully coming from the same distribution as the one
15:54 - you've used for training.
15:55 - But you don't want to just fit the training distribution.
15:59 - If you're fitting a model over a training set of images,
16:04 - you don't just want to put probability mass
16:06 - around the images that you have in the training set.
16:08 - You want to spread it out and you need to be able to say,
16:11 - oh, there is other parts of the space where
16:13 - I need to put probability mass even though I have not seen them
16:16 - during training.
16:17 - And so we have a similar problem.
16:20 - To some extent, kind of the gradient and the function
16:23 - are essentially the same thing.
16:25 - So if you have the gradient, you can integrate it
16:27 - and you can get the function.
16:29 - And because everything has to be normalized.
16:32 - So you know that the, I mean, you can get the function up
16:35 - to a constant and we know what the value of that constant
16:37 - needs to be because it has to be normalized.
16:39 - So in some sense, it's just as hard as the original problem.
16:44 - As we'll see, there's going to be
16:46 - issues that are very specific to training with the Fisher
16:49 - divergence.
16:51 - That makes it so that doing things,
16:55 - this vanilla or approach will not quite work
16:57 - and we'll need to do a bunch of different things
16:59 - to actually make it work in practice.
17:02 - But so far, it's more like a--
17:04 - up to here, I'm just saying that it's
17:06 - going to be a different representation of the models we
17:09 - are willing to consider.
17:10 - I even said, how are we going to do the training?
17:13 - And how do we prevent overfitting?
17:15 - And so forth.
17:16 - So the idea would be that potentially, the vector
17:22 - field that you model might not be the gradient of a scalar
17:25 - function.
17:25 - So it might not necessarily be a conservative vector field.
17:29 - So you can imagine that here if you do things this way,
17:34 - f theta is a scalar function which is the potential.
17:40 - If you think about in physics term, there is a potential.
17:43 - Maybe an electric potential and that's a scalar.
17:46 - And you get the vector field by taking the gradient of that.
17:50 - So it's a way of parameterizing a set of vector fields
17:54 - that they need to satisfy certain properties because they
17:58 - are the gradients of a scalar function.
18:02 - Here, I'm saying, oh, I'm no longer even
18:04 - going to restrict myself to gradients of scalar function.
18:08 - I'm going to allow myself to just have arbitrary vector
18:12 - fields.
18:12 -
18:15 - There might not be an underlying scalar function such
18:20 - that this vector field is the gradient of that function.
18:24 - That's the sort of high level idea,
18:26 - we're going to try to fit directly score models to data.
18:31 - So the problem is this, you're given IID samples from our data
18:35 - density which is unknown.
18:37 - Usual learning setting, our training set of samples
18:42 - from some unknown data distribution
18:44 - and you want to try to estimate the score of this data
18:47 - distribution.
18:49 - And so we're going to think about model family which
18:52 - is going to be a set of vector valued functions.
18:56 - Parameterized by neural networks as you change theta,
18:58 - you change the shape of the vector field.
19:01 - And the goal is to choose parameters so that the vector
19:04 - fields are similar.
19:08 - So you can imagine the first question is, how do we
19:11 - compare two vector fields.
19:14 - So there's going to be the true vector
19:17 - field of gradients corresponding to the data density.
19:20 - There's going to be an estimated vector field of gradients.
19:23 - How do we compare them?
19:26 - A reasonable way to do it is to basically overlap these two
19:30 - vector fields at every point.
19:32 - There is going to be a true gradient, an estimated gradient.
19:35 - And we can look at the difference between the two.
19:38 - And average this over the whole space.
19:43 - And if you do that, you get back the Fisher divergence
19:48 - that we talked about before.
19:51 - So if you go through every x, you
19:54 - look at the true gradient at that point
19:55 - according to the data density.
19:57 - You look at the estimated gradient at that point
19:59 - according to the model.
20:01 - There's going to be some difference,
20:03 - you look at the norm of that vector,
20:05 - you average with respect to the data density.
20:09 - And that's going to be a scalar value that tells you
20:12 - how far away your model is from the true vector
20:16 - field of gradients of the data distribution.
20:20 - So if you can get this quantity to zero as a function of theta,
20:24 - then that the vector fields match
20:26 - and you have a perfect model.
20:28 - And so trying to minimize this as a function of theta
20:31 - is a reasonable learning objective.
20:33 - And we know that even though it looks like something that you
20:37 - cannot possibly optimize because it depends on this unknown
20:41 - quantity here.
20:43 - Recall, we only have access to samples.
20:46 - We can do integration by parts and you
20:48 - can rewrite it in terms of an objective that
20:51 - only depends on your model.
20:54 - And it still involves an expectation with respect
20:56 - to the data.
20:57 - But you can approximate that using the sample average.
21:03 - So in order to train this kind of model,
21:06 - you need to be able to evaluate s theta efficiently.
21:11 - And we need to somehow be able to compute
21:17 - this trace of the Jacobian.
21:20 - Which is basically the sum of all a bunch
21:22 - of partial derivatives.
21:26 - And then there is the question of, well,
21:29 - do we need this core model to be proper to correspond
21:35 - to the gradient of some energy function?
21:37 - And we'll see that that's actually not
21:39 - really needed in practice.
21:43 - So the most straightforward way of kind
21:49 - of parameterizing the score would
21:52 - be to just pick a vector value in the neural network.
21:55 - So let's say you have three inputs and three outputs.
21:59 - Because we at every point, this neural network
22:02 - has to estimate the gradient which
22:03 - is a vector, which is the same dimension as the input.
22:08 - And then we need to be able to basically evaluate
22:12 - this loss which involves the norm
22:14 - of the output of the neural network
22:18 - and the trace of the Jacobian.
22:22 - So to evaluate the first term which
22:25 - is just the norm of the output, it's easy.
22:28 - Basically what you do is you just do a forward pass
22:31 - and then you can compute as theta,
22:33 - and then you can also compute the squared norm as theta.
22:39 - The more complicated piece is the trace of the Jacobian.
22:44 - So the Jacobian is basically this matrix
22:48 - where you have basically all the partial derivatives
22:53 - or all the gradients of every output with respect
22:57 - to the inputs.
22:59 - So the first term up here is the partial derivative
23:03 - of the first output with respect to the first input.
23:06 - And then you have all these partial derivatives
23:12 - that you have to deal with.
23:15 - And the problem is, we're trying to compute
23:20 - the trace of this matrix which is basically
23:23 - the sum of the elements of the diagonal.
23:25 - And so what you need to do is you
23:27 - need to be able to compute the partial derivative
23:30 - of the first output with respect to the first input.
23:33 - And then you need to compute this element here
23:37 - on the diagonal.
23:38 - You need to compute the partial derivative of the second output
23:41 - with respect to the second input.
23:43 - And then you need to compute the partial derivative
23:46 - of the third output with respect to the third input.
23:50 - Then you have to sum up these three numbers.
23:54 - Because you need to sum up these three elements
23:57 - on the diagonal of this matrix.
24:01 - And although, we can do back propagation.
24:04 - So you can compute these derivatives
24:06 - relatively efficiently.
24:09 - Naively doing this would require a number of back propagation
24:15 - steps that scales linearly with the number of dimensions
24:18 - that you have.
24:20 - And we don't know if there is a more efficient way
24:23 - of basically doing this.
24:24 - But the only way basically we know how to do it
24:27 - is essentially extremely inefficient
24:30 - when the number of dimensions grows.
24:33 - And is very large.
24:36 - And so even though this loss does not
24:40 - involve partition functions, it still
24:42 - scales pretty poorly with the dimensionality of the data.
24:46 - Yeah.
24:47 - So IBMs are even worse because in an IBM,
24:51 - you would need to do one more backprop to get the score.
24:56 - And then one more to get these derivatives.
25:00 - So an IBM would even be even more expensive.
25:04 - These at least saves you one backpropagation
25:08 - because you are already modeling the gradient of something.
25:11 - But it's still expensive.
25:14 - Yeah.
25:15 - So you have the hessian of f theta.
25:17 - So when you take the first gradient with respect
25:19 - to x of f theta, you get essentially s theta.
25:24 - And then you have to do the Jacobian of s theta.
25:28 - So you need to do second order basically derivatives
25:32 - in that case.
25:33 - So it's even more expensive.
25:34 - They have to be the same here because you're
25:36 - modeling the score, which is the gradient of the log likelihood.
25:41 - And so that has to be the same dimension as the input.
25:44 - Yeah.
25:45 - We're modeling a joint distribution
25:47 - over a set of random variables.
25:50 - And if some of them are missing, computing marginals
25:54 - might be expensive.
25:56 -
25:59 - Cool.
26:00 - So this vanilla version which is something
26:04 - we briefly mentioned also in the last lecture, if you recall.
26:06 - We said, OK, this is avoids the partition function.
26:10 - But you doing integration by parts is still expensive.
26:14 - Because of this Hessian term or trace
26:17 - of the Jacobian in this case and so
26:19 - we need more scalable approximations
26:21 - that work in high dimensions.
26:24 - And that's what we're going to talk about next, which
26:27 - is how to get this to scale to high dimensional settings
26:31 - where basically this d is large.

00:00 -
00:05 - SPEAKER: There's kind of two approaches
00:06 - that we're going to talk about.
00:08 - The first one is called the denoising score matching.
00:12 - And the idea is that instead of trying
00:14 - to estimate the gradient of the data,
00:17 - we're going to try to estimate the gradient of the data
00:21 - perturbed with noise.
00:23 - So you can imagine that there is a data distribution that
00:26 - might look like this.
00:27 - And then there's going to be a noise
00:31 - perturbed data distribution shown in orange denoted q
00:35 - sigma where we're basically just adding noise to the data
00:40 - or convolving the data density, in this case,
00:43 - with a noise distribution q sigma of x tilde given
00:48 - x, which might be something like a Gaussian in this case.
00:52 - We're kind of smoothing the original data density
00:56 - by essentially adding noise.
01:00 - Then it turns out that if you're estimating
01:04 - the score of this distribution that you get after adding noise
01:07 - is a lot easier computationally.
01:10 - And so to the extent that you choose the noise
01:14 - level to be relatively small, this
01:16 - might be a reasonable approximation.
01:19 - If you don't add too much noise, then this yellow density
01:22 - will be pretty close to the blue one.
01:24 - And so this course that you estimate for the yellow density,
01:29 - the noise-perturbed density are going
01:31 - to be pretty close to what you want because basically,
01:39 - q sigma is going to be pretty close to the original data
01:41 - density when sigma is small.
01:44 - That's the high-level idea.
01:47 - And so it works like this.
01:50 - You have a data density which could be over images
01:53 - and then you add noise to the images
01:55 - by using this Gaussian kernel q sigma.
02:00 - And then you get a new distribution over images
02:03 - plus with noise and we're going to try
02:05 - to estimate the score of that.
02:08 - And the way we're going to try to fit
02:14 - our model to this noise-perturbed data density
02:17 - is again using the Fisher divergence.
02:20 - But now instead of doing a Fisher divergence between model
02:23 - and data, we do Fisher divergence
02:25 - between model and this noise-perturbed data density.
02:32 - So it's the same thing as before,
02:34 - except we replace Pdata with q sigma, which is data plus noise,
02:40 - basically.
02:43 - And then which is just this.
02:46 - So the expectation is just this integral with respect
02:50 - to q sigma.
02:52 - So just like before, the norm of the difference
02:55 - between the estimated gradient and the true gradient,
02:58 - except that now instead of using the real-data density
03:01 - we use this q sigma which is the noise-perturbed data density.
03:07 - And then we do just when we're doing integration
03:11 - by parts, we expand this square and get three terms.
03:17 - We get the norm of the first term,
03:20 - the norm of the second term, and then we
03:22 - have this inner product between the two pieces, the red term,
03:29 - which is going to be the complicated one.
03:31 - Basically, just like in the integration by part trick,
03:35 - you can see that the blue term does not depend on theta,
03:39 - so we can ignore it.
03:40 - The green term depends on theta in an easy way.
03:46 - So it's just basically the usual thing.
03:49 - And a complicated piece is the red one.
03:53 - Or we have this dot product between the score
03:56 - of the noisy data and the estimated score.
04:00 - So q is defined as--
04:03 - yeah, basically, you get a sample from q sigma
04:06 - by randomly drawing from the data,
04:10 - randomly drawing some Gaussian noise and adding it to the data.
04:14 - So if we achieve that that is going
04:15 - to be tractable in the sense that we're
04:18 - going to get rid of that trace of the Jacobian term.
04:21 - So we're going to get a loss function that
04:24 - is going to be scalable in high dimensions.
04:26 - So that's going to be the--
04:29 - we're doing this because the trace of the Jacobian
04:32 - was too expensive.
04:34 - This introduces an approximation because you're
04:36 - no longer estimating the score of the data density
04:39 - or estimating the score of this other thing,
04:42 - but it turns out that we're going
04:43 - to be able to do it much more efficiently.
04:46 - It's going to reduce this problem to denoising.
04:49 - So basically, this score-matching objective will
04:54 - end up being equivalent to the problem of--
04:57 - given this x tilde, try to remove noise
05:01 - and try to estimate the original image you started with,
05:04 - basically.
05:07 - It's going to be mathematically equivalent.
05:10 - Basically, we are going to rewrite this red term in a sum--
05:13 - where do I have-- my cursor here,
05:15 - we're going to rewrite this red term in some way
05:17 - and we're going to show it's going
05:18 - to be equivalent to denoising.
05:22 - So OK.
05:23 - So we can order the blue term.
05:24 - It doesn't depend on theta.
05:25 - Then we have this green term, which is easy,
05:27 - and then we have this red term, which is tricky,
05:31 - but we're going to rewrite it.
05:33 - So focusing on the red term, it looks like this.
05:37 - And just like in the integration by part kind of trick,
05:42 - we can write the gradient of the log as 1
05:46 - over the argument of the log times the gradient
05:48 - of the argument of the log.
05:50 - This is the basic.
05:52 - I just basically expanded the gradient of the log of q sigma.
05:58 - And now you see that this q sigma here and q sigma
06:01 - down here will cancel with each other.
06:05 - And so we end up with something a little bit simpler,
06:08 - which is the dot product basically
06:10 - between the gradient of the noise-perturbed density
06:16 - and the gradient and the score model at every point.
06:20 - And now we can write the expression
06:24 - for q sigma, which is just this integral.
06:29 - Basically, the probability of any particular sigma x tilde
06:33 - is going to be the probability of sampling any data point
06:37 - x times the probability of generating x tilde by adding
06:42 - noise to x, basically.
06:46 - Just think about the sampling process.
06:48 - What is the probability of generating an x tilde?
06:50 - You have to look at every possible x
06:52 - and you have to check what was the probability of generating
06:55 - x tilde by adding noise to x.
06:58 - And that's basically what this integral here is giving you.
07:02 - It's just the definition of q sigma
07:03 - that we had in the previous slide.
07:07 - And now we can see that this is linear,
07:11 - so we can push the gradient inside the integral.
07:17 - And that's where things become a lot simpler because now you
07:19 - see that now we are getting a gradient of this Gaussian
07:25 - density basically and we no longer have
07:28 - to deal with the gradient of the data density, basically.
07:33 - And now we can further push out the--
07:39 - well, now we can use again this trick
07:42 - here that the gradient of the log of q
07:44 - is 1/q times the gradient of q.
07:46 - And we can rewrite the gradient of the transition
07:48 - of the Gaussian density as a q times the gradient of log q.
07:54 - Because if you take the gradient of log q,
07:56 - you're going to get the gradient of q times 1 minus q,
07:59 - and so these two things are obviously the same.
08:03 - And now you push the expectation out
08:06 - and we basically have an expression
08:08 - that looks very much like the original one
08:12 - that we started with.
08:13 - But we no longer have to deal with this gradient of the log
08:16 - data density perturbed with noise,
08:18 - but we have to look at the gradient
08:20 - of this conditional distribution of x tilde
08:22 - given x, which is just a Gaussian density.
08:29 - And so overall, basically, we've rewritten
08:33 - this complicated object up here into something
08:36 - that is a little bit simpler because now it
08:38 - involves only the gradient.
08:40 - It basically involves the score of this q
08:43 - sigma of x tilde given x, which is just going to be Gaussian.
08:46 -
08:49 - And so bringing it together, this
08:51 - is what we started with estimating the score of the data
08:55 - density perturbed with noise.
08:59 - You could write it this way.
09:01 - And through this algebra that we just did,
09:03 - we could also rewrite the red term in terms of this.
09:08 - And now you can basically see that essentially, you
09:18 - can write it as the square difference between s theta
09:21 - and the gradient of this Gaussian transition kernel
09:24 - that we have here because when you
09:27 - take the square of this term, it would give you the red one.
09:29 -
09:32 - The square term of this one will give you this brown term.
09:35 - That way we're subtracting out.
09:37 - And then the dot product between these two
09:39 - is exactly this red term that we just derived.
09:44 - So all in all, basically, what we've shown
09:47 - is that if you want to estimate the score of the q
09:50 - sigma, the noise-perturbed data density,
09:53 - it's basically equivalent to trying
09:55 - to estimate the score of this transition
10:00 - kernel, this Gaussian density that we
10:02 - use to add noise across different x's
10:07 - and different x tildes that are sampled from the noise
10:10 - distribution.
10:13 - So a lot of algebra, but basically, up to constants
10:19 - we can rewrite the score-matching objective
10:22 - for the noise-perturbed data density
10:24 - into a new score-matching objective
10:26 - that now involves terms that are relatively easy to work with.
10:31 - And in particular, if you look at this expression,
10:37 - it turns out that this gradient of the log of q sigma x
10:41 - tilde given x is easy to compute because that's just a Gaussian.
10:45 - So q sigma x tilde given x is just
10:48 - a Gaussian with mean x and standard deviation and variance
10:55 - sigma squared identity.
10:57 - So that's just a squared exponential.
11:01 - When you take the log it just becomes a quadratic form.
11:04 - When you take the gradient you just get a relatively--
11:08 - basically, an expression that looks like that.
11:12 - And so when you plug in this expression in here,
11:18 - you get something easy to work with.
11:23 - Maybe I don't have it here, but basically, you
11:27 - end up with an objective that no longer involves
11:31 - traces of the Jacobians.
11:33 - It's an L2 loss between a theta compared to this x tilde minus x
11:40 - over x over sigma squared, which is basically
11:44 - a denoising objective as we will see
11:48 - in our next couple of slides.
11:51 - So the key takeaway here is you don't
11:54 - have to estimate the trace of the Jacobian anymore.
11:59 - If you're willing to estimate the score, not
12:03 - of the clean data, but if you're willing to estimate
12:05 - the score of this q sigma which is data plus noise.
12:08 -
12:11 - So practically, the algorithm is something like this.
12:15 - You have a minibatch of data points sampled from the data.
12:18 - You perturb these data points by adding Gaussian noise.
12:24 - So literally just add noise to each xi with the variance sigma
12:31 - square.
12:33 - And then you just estimate the denoising score-matching loss,
12:37 - which is just based on the minibatch, which is just
12:40 - the loss on these data points and it's just
12:43 - basically this expression.
12:45 - And recall that if this q sigma is Gaussian,
12:50 - then the loss looks something like this.
12:54 - And so it has a very intuitive kind of interpretation
12:59 - because what we're saying is that what this score model needs
13:03 - to do at every data point, x tilde.
13:05 - So the score model is being evaluated at these noisy
13:09 - data points x tilde.
13:10 - And for each data point, what the score model is trying to do
13:14 - is it's trying to estimate the noise that was
13:17 - added to xi to produce x tilde.
13:21 - Yeah.
13:22 - So you'd want sigma to be as small as possible
13:25 - because you want q sigma to be as close as possible to Pdata.
13:30 - On the other hand, the variance goes to infinity of this loss
13:35 - as sigma goes to 0.
13:37 - So you can't actually choose sigma to be too small.
13:40 - So in practice, you need to try to choose the sigma as
13:43 - small as possible such that you still optimize the loss,
13:48 - but there is always an approximation.
13:50 - But that's the trade-off.
13:51 - You don't have Hessians or traces of the Jacobian anymore,
13:56 - but you're not estimating the score of the clean data,
13:59 - you're estimating the score of the noisy data.
14:03 - We're no longer estimating--
14:04 - I mean, we're changing the goalpost.
14:06 - We're no longer estimating the--
14:08 - you can think of this as basically
14:10 - a numerical approximation of it.
14:12 - In some sense, we're adding Gaussian noise
14:14 - and we're trying to estimate derivatives
14:16 - through a finite difference kind of thing, basically.
14:21 - That's one way of deriving.
14:24 - The same thing if you like that sort of approximation route,
14:28 - it has the flavor of basically estimating the derivatives
14:34 - through a perturbation.
14:35 - When is this loss 0?
14:37 - Maybe I have it on the next slide.
14:41 - Yeah.
14:42 - So if you think about it, the loss function looks like this,
14:48 - so the original loss function was this,
14:50 - and then we were able to rewrite it as this.
14:52 - And so what are you doing?
14:54 - You're starting with a clean image,
14:56 - then you add noise to generate x tilde, then look at this loss.
15:01 - What we're saying is that the score model
15:04 - takes x tilde as an input.
15:06 - And to make this L2 loss as small as possible,
15:10 - you're trying to match this x minus x
15:13 - tilde, which is exactly the noise that we added.
15:16 -
15:18 - And so to make this loss as small as possible,
15:22 - s theta has to match the vector of noise
15:25 - that we added to this image.
15:29 - So that's why it's a denoiser because it gets to see x tilde
15:34 - and it needs to figure out, what do I subtract to this x tilde
15:38 - to get back a clean image?
15:41 - Yes.
15:42 - That's called Stein unbiased risk estimator.
15:46 - That's the key trick that is used.
15:48 - You can still evaluate the quality of an estimator
15:51 - without actually knowing the ground truth in some sense.
15:55 - So the axis and the x tilde you're generating them yourself,
15:58 - but s theta doesn't see the clean data.
16:00 - So x theta only sees the noise data
16:02 - and you're trying to predict the noise.
16:04 - This is not restricted to Gaussian noise.
16:07 - If you look at the math, the only thing
16:08 - you need to be able to compute is these gradients of--
16:13 - basically, as long as the distribution
16:16 - that you use to add noise that you can compute likelihoods
16:19 - and you can get the gradient in closed form, then
16:21 - you can get a denoising loss for that.
16:25 - You're going to end up estimating the score.
16:30 - We are estimating the score of q sigma, right,
16:33 - which is if you're adding Gaussian noise,
16:36 - it's going to be basically theta plus Gaussian noise.
16:39 - If you add another kind of perturbation,
16:43 - you're going to get another type of perturbed data
16:49 - and you're estimating the score of that.
16:52 - So you're right.
16:53 - We're not estimating the score of the clean data density,
16:56 - we're estimating the score of the data plus noise.
17:00 - The hope is that you need to add just a small amount of noise
17:04 - so that if sigma is small enough that these images are
17:08 - indistinguishable from the clean ones, then
17:11 - the approximation is not too bad.
17:14 - And what we gain by doing that is that it's much more scalable.
17:18 - That's the magic of denoising score matching
17:20 - that basically these two objectives are equivalent
17:24 - up to a constant.
17:25 - So by minimizing the bottom one, the denoising,
17:28 - you are actually also minimizing the top objective
17:32 - where you're really estimating the score of the distribution
17:37 - of the data convolved basically with Gaussian noise,
17:39 - the smoothed version of the data density.
17:43 - Even though, you can just work at the level
17:45 - of the individual conditionals.
17:47 - That's the beauty of these denoising score matching.
17:50 - And another way to say it maybe is that denoising is not
17:52 - too hard as a problem.
17:54 - And so we have pretty good neural networks
17:57 - that can do denoising.
17:58 - And so to some extent, we've reduced the problem
18:01 - of generating images to the problem
18:03 - of denoising, which is a relatively easy task
18:08 - for our neural network.
18:09 - So to the extent that you can do well at denoising,
18:12 - you're going to do well at estimating the score.
18:15 - And we know that the score is basically, to some extent,
18:18 - equivalent to having a likelihood.
18:22 - So we haven't yet talked about how
18:24 - do you actually generate samples from these models,
18:26 - but essentially, we'll do MCMC.
18:28 - And so after all these steps, we've
18:32 - reduced generative modeling to denoising,
18:34 - which is an easy task, probably one of the easiest tasks
18:38 - that you can think of.
18:40 - It doesn't have to be Gaussian as long as the machinery like,
18:43 - yeah, basically, as long as you can compute
18:46 - this gradient of whatever distribution
18:50 - you use to add noise, the math works out.
18:54 - And really if you think about what happened in the proof,
18:57 - really the only thing that matters
18:59 - is that the gradient is linear.
19:01 - Gradient is a linear operator and so
19:03 - this whole machinery works.
19:05 - So here we've seen the score matching reduces the denoising.
19:08 - So estimating the score is the same as estimating
19:11 - the noise that was added to the data point.
19:14 - And so the reason this is true or another way
19:17 - to think about this is that there is something called
19:21 - Tweedie's Formula, which is basically
19:24 - an alternative way of deriving the same result, which
19:27 - is kind of like, it's telling you
19:31 - that indeed as was suggested by you, that the optimal denoising
19:38 - strategy is to basically follow the gradient
19:40 - of the perturbed log-likelihood.
19:43 - So you can imagine that if you had a data density that
19:47 - only has three images.
19:51 - So it's kind of like three deltas.
19:53 - And this is like a toy picture, but just
19:57 - for visualization purposes, you can
19:59 - imagine that if you add noise to these three images,
20:02 - you're going to get a density that looks like this.
20:05 - And then you can imagine-- let's say you're trying to denoise,
20:09 - and what we've just shown is that the best way to denoise
20:13 - is to follow the gradient.
20:15 - So if somehow somebody gives you a data point to the left here,
20:20 - how should you denoise it?
20:22 - You should follow the gradient to try
20:23 - to go towards high-probability regions, which makes sense.
20:29 - If you're trying to denoise, try to change the image,
20:31 - and push it towards high-probability regions.
20:34 - And in fact, the optimal denoising strategy
20:37 - is to take the noisy sample and follow a step
20:40 - plus with the right scaling, but basically,
20:43 - follow the gradient of the log of perturbed data density.
20:48 - So for these results, the denoising score-matching stuff
20:54 - it's still true.
20:57 - What it's good about Gaussian is the following.
21:02 - Maybe I will clarify.
21:04 - So essentially, what you can look at
21:07 - is there is the data, clean data,
21:12 - and then there is the noisy data and then there
21:14 - is the posterior distribution of the clean data
21:17 - given the noisy data.
21:19 - And we know the definition of the noisy data distribution.
21:25 - And basically, Tweedie's Formula is telling you
21:28 - that the expected--
21:31 -
21:35 - given a noisy image x, the expected value
21:39 - of the clean image is given by this expression.
21:43 - And so if you want to minimize the L2 loss,
21:45 - the best thing you can do is to output
21:47 - the conditional expectation of x given x tilde.
21:51 - And so from that perspective, you want to follow the gradient.
21:56 - And this particular version of the formula
22:00 - is only true for Gaussians.
22:03 - The other way to make things efficient
22:07 - is to take random projections.
22:10 - We still have time.
22:10 - So another alternative way of coming up
22:15 - with an efficient approximation to original score-matching loss
22:21 - that does not involve traces of the Jacobians
22:24 - is to basically take random projections.
22:28 - So you can imagine at the end of the day, what we're
22:30 - trying to do is we're trying to match the estimated vector
22:34 - field to the true vector field.
22:36 - And if these vector fields are really the same then
22:41 - they should also be the same if we project them
22:44 - along any kind of direction.
22:47 - So you can take this direction and this direction,
22:51 - and you can project the arrows along that direction.
22:55 - And if the vector fields are the same then
22:58 - the projections should match.
23:02 - And so in particular if these projections are just
23:04 - x's aligned, then individual components of these vectors
23:08 - should match.
23:08 -
23:11 - And the idea is that working on the projection space
23:16 - is going to be much more efficient because now it's
23:18 - going to be a one-dimensional problem.
23:21 - And so that's basically a variant
23:26 - of the Fisher divergence, which we call the sliced Fisher
23:30 - divergence, which is exactly what we had before.
23:32 - But before comparing the data to the model gradients,
23:38 - we project them along a random direction v.
23:43 - So you randomly pick a direction v, and then at every data point,
23:48 - you compare the true gradient and the estimated gradient
23:52 - along this direction v. And note that after you take this dot
23:58 - product, these are scalars.
24:00 - So these are no longer vectors, they are scalars.
24:04 - And it turns out you can still do integration by parts.
24:08 - And you end up with an objective function that looks like this.
24:13 - And it still involves the Jacobian,
24:17 - but crucially now it involves basically
24:19 - Jacobian vector products, which are basically
24:23 - directional derivatives and are things
24:25 - that you can estimate using backpropagation efficiently.
24:28 - So the second term is just the usual thing.
24:33 - It's efficient.
24:33 - It's just the output of the network times dot
24:37 - product with a random vector, so that's efficient to evaluate.
24:41 - Now we have something that looks like this.
24:43 - We have this Jacobian matrix left multiplied by this vector v
24:48 - and right multiplied by the same vector v.
24:51 - And it turns out that basically this thing is just
24:54 - like a directional derivative and that's
24:57 - something you can compute with backpropagation efficiently.
25:00 - So if you think about it, this is the expression
25:04 - we started with, which you can equivalently
25:06 - write as the gradient of the dot product.
25:10 - And that's something that you would compute like this.
25:13 - So you have a forward pass that computes as theta, then
25:19 - you take the dot product with v and that gives you a scalar.
25:24 - Now you do a single backpropagation
25:26 - to compute the gradient of that scalar with respect
25:29 - to all the inputs.
25:31 - And then you take another dot product to get back
25:35 - the derivative or the quantity.
25:39 - And so this can basically be done roughly
25:42 - at the cost of a single backpropagation step.
25:45 - The v is sampled from some distribution.
25:49 - And let me see if I have it here.
25:53 - So this is what it would look like.
25:55 - So with sample data, for every data point,
25:57 - you would randomly sample a direction
26:00 - according to some distribution.
26:03 - And then you just optimize this objective function,
26:07 - which as we've seen is tractable to estimate
26:10 - and it does not involve a trace of the Jacobian.
26:14 - And there's a lot of flexibility in terms
26:18 - of choosing this pv like how do you choose the directions.
26:21 - And you can choose for example Gaussian or Rademacher vectors.
26:26 - And they both work in theory.
26:28 - Then the variance can vary, but basically, there's
26:32 - a lot of flexibility in terms of choosing
26:34 - these random directions.
26:37 - Before you have to compute the partial derivatives
26:39 - of every output with respect to every input.
26:41 - So you need a d back props.
26:44 - And here you can do a single one because it's basically
26:46 - a directional derivative.
26:47 - Basically, these are essentially unbiased estimators
26:51 - of the original objective.
26:52 - You can also think of it that way.
26:54 - There is variance that you're introducing because you're
26:57 - comparing projections of the vectors
27:00 - instead of comparing the vectors fully,
27:03 - which is what the original score-matching loss would do.
27:08 - So that's the price you pay, basically.
27:10 - And you can use variance reduction techniques
27:12 - to actually make things more stable in practice.
27:16 - Different distributions.
27:18 - What you can do is you can take--
27:20 - if you are willing to pay a little bit more computation
27:24 - cost, you can take multiple random projections
27:26 - per data point.
27:28 - You can just try to match, not just
27:31 - sample of v1, x1 along direction v1,
27:34 - but you can take a bunch of them and then average them.
27:38 - And so there is a natural way of reducing variance
27:40 - by taking more projections, but then it becomes more expensive.
27:43 - Eventually, if you take n projections, where
27:46 - n is the dimensionality, and you compare
27:48 - on every single coordinate, it goes back to the original one.
27:52 - And you are free to choose something in between.
27:54 - In practice, one projection works.
27:57 - Here there is no noise.
27:59 - The advantage of this is that you are actually
28:01 - estimating the score of the data density as opposed
28:04 - to the data density plus noise.
28:06 - Yeah.
28:06 -
28:10 - And here you see some plots kind of showing
28:14 - that if you do vanilla score matching, how long it
28:17 - takes per iteration as a function of the data dimension.
28:20 - It can go up to 300 to 400 dimensions
28:22 - and then you run out of memory.
28:24 - This was a few years ago, but it kind of
28:26 - scales poorly linearly with respect to the dimension.
28:29 - And if you have these sliced versions
28:31 - they are basically constant with respect to the data dimension.
28:35 - And in terms of PF model quality, it actually performs.
28:41 - Not super important what this graph means,
28:43 - but it's what you get with sliced versions
28:45 - of score-matching matches.
28:46 - It's pretty much what you would get
28:48 - with the exact score-matching objective.
28:52 - You still need to do the integration by parts trick.
28:55 - This one you don't know it.
28:56 - So the original loss would basically just at every x,
29:00 - you take the dot product of the true gradient, the estimated
29:03 - gradient, and then you square the difference.
29:07 - You can't evaluate that loss because it
29:10 - depends on the true gradient which you don't know.
29:12 - But then you can do integration by parts
29:14 - and you can rewrite it as this thing, which
29:18 - is what we had before and it no longer depends
29:21 - on the true score.

00:00 -
00:04 - SPEAKER: Well, let's say that somehow you've
00:06 - used the real vanilla score matching or denoising
00:12 - score matching, or SLI score matching,
00:14 - and you're able to train your neural network as theta so
00:19 - that the estimated vector field of gradients
00:22 - is close to the true vector field of gradients of the data
00:25 - density.
00:27 - The question is, How do you use this?
00:30 - You no longer have access to a likelihood.
00:33 - There is no autoregressive generation.
00:37 - How do you generate samples?
00:40 - And so the intuition is that the scores are basically
00:45 - telling you in which direction you
00:47 - should perturb a sample to increase its likelihood most
00:52 - rapidly.
00:54 - And so you could imagine a basic procedure
00:58 - where an MCMC kind of procedure like
01:01 - what we talked about before where you initialize particles
01:07 - at random.
01:08 - And here I'm showing multiple particles,
01:10 - but you could imagine sampling x0 based
01:15 - on some initial distribution.
01:17 - Then you could imagine repeatedly
01:19 - kind of taking this update where you're basically
01:24 - taking a step in the direction of the estimated gradient, which
01:28 - you just do gradient ascent on using the estimated
01:32 - scores to decide the direction.
01:36 - And if you do that, you're going to get something
01:39 - like this where the particles kind of will all
01:43 - converge in this local optima, the local maxima hopefully
01:48 - of this density, which is kind of right.
01:54 - Imagine you start with random noise, which is an image which
01:58 - is pure noise, and then you follow the gradient
02:00 - and then until you reach a local optimum where
02:03 - you can no longer improve.
02:05 - We know that that's not the right way to generate a sample.
02:08 - The right way to generate a sample
02:10 - is to follow the noisy gradient.
02:12 - That's what we call Langevin MCMC, which is exactly
02:16 - the same procedure, except that we also
02:19 - add a little bit of Gaussian noise at every step.
02:22 - And if you do that, then you'll see
02:25 - that we'll actually generate--
02:27 - when you run it for long enough, this procedure
02:29 - is guaranteed to produce samples from the underlying density.
02:35 - So remember that this vector field corresponded to a density
02:39 - where we have a lot of probability mass
02:41 - here, a lot of probability mass there.
02:43 - And indeed, if you look at the distribution of these particles,
02:46 - they're going to have the right distribution
02:49 - because what we've seen is that these longevity dynamics
02:53 - sampling is a valid MCMC procedure in the limit.
02:57 - So it's a way of sampling from a density when you only
03:01 - have access to the score.
03:03 - So we know that if you initialize your particle--
03:07 - it doesn't matter how you do it--
03:09 - and then you repeat this process of following the noisy gradient
03:17 - in the limit of small step, sizes,
03:19 - and infinite number of steps, this
03:22 - will give you a sample from the underlying density.
03:26 - So literally all we're doing is replacing the true score
03:29 - function with the estimated score function.
03:34 - And basically that's one way of generating samples.
03:40 - Your first estimated score by score
03:42 - matching, trying to match this neural network output arrows,
03:48 - output gradients that are close to the true one,
03:51 - and then you just follow the directions.
03:53 -
03:55 - And to the extent that you've done a good job
03:58 - at estimating the gradient and to the extent
04:01 - that this technical conditions are satisfied,
04:04 - this would produce a valid sample.
04:06 -
04:09 - And so that's basically the full picture.
04:13 - The full pipeline is you start with data,
04:15 - you estimate the score, and you generate samples
04:17 - by basically following the score which kind of corresponds
04:23 - to removing noise because we know that the score is telling
04:27 - you the direction that you should follow
04:29 - if you want to remove noise.
04:30 - And so back to what we were discussing before,
04:33 - it has a little bit of this flavor
04:34 - of removing noise and then adding noise because that's what
04:40 - Langevin is telling you to do.
04:41 -
04:45 - And unfortunately, if you just do this, it doesn't work.
04:49 - So this is what you get if you use this procedure.
04:52 - You try to train a model on MNIST,
04:54 - even simple data sets like MNIST, CelebA, CIFAR-10.
04:57 - It just doesn't work.
04:59 - And this is what the Langevin procedure looks like.
05:04 - You start with pure noise, and then it gets stuck somewhere.
05:10 - But it doesn't produce good samples.
05:15 - And there are several reasons for this.
05:19 - One is that basically data tends to-- real world data tends
05:24 - to basically lie on a manifold.
05:26 -
05:30 - And if the data is really on a manifold,
05:32 - the score might not be defined.
05:35 - And you can see this intuitively.
05:36 - Like imagine you have a density that is over concentrated
05:40 - on a ring, as you make the ring thinner and thinner,
05:45 - the magnitude of the gradient gets bigger and bigger,
05:48 - and at some point it becomes undefined.
05:52 - And so that's a problem.
05:55 - And indeed real data tends to lie
05:58 - on low dimensional manifolds.
06:01 - Like, if you just take MNIST samples,
06:03 - and then you take the first 595 PCA components,
06:09 - so you project it down on a linear manifold
06:12 - of dimension 585, there is almost no difference.
06:17 - So basically means that indeed, even just
06:24 - if you restrict yourself to linear manifolds,
06:26 - that you can get PCA.
06:27 - There is almost no loss.
06:29 - And if you take CIFAR-10, and you
06:30 - take a 2,165 dimensional manifold,
06:35 - again, almost no difference after you project the data.
06:39 - So it seems like indeed that's an issue.
06:45 - And you can see if you look at the training
06:47 - curve on CIFAR-10, that's the sliced score matching loss.
06:51 - It's very, very bumpy, and it doesn't quite train.
06:56 - The other issue which was hinted at before
07:00 - is that if you think about it, we're
07:03 - going to have problems in the low data density regions
07:06 - because if you think about points that
07:09 - are likely under the data distribution,
07:12 - we're going to get a lot of samples from those regions.
07:18 - If you think about the loss, the loss
07:20 - is an expectation with respect to the data distribution
07:23 - of the difference between the estimated
07:24 - gradient and the true gradient.
07:27 - But with this expectation, we're approximating it
07:32 - with a sample average.
07:34 - And most of our samples are going to come--
07:36 - let's say, are going to be up here
07:38 - and are going to be down here.
07:40 - And we're never going to see samples in between.
07:44 - And so if you think about the loss,
07:45 - the neural network is going to have
07:47 - a pretty hard time estimating the gradients in between.
07:52 - And you can see here an example where we have the true data
07:55 - scores in the middle panel and the estimated data
07:59 - scores on the right panel.
08:00 - And you can see that the arrows, they
08:02 - match pretty well at the corners where we're going
08:05 - to see a lot of training data.
08:07 - But they're pretty bad the moment you
08:09 - go away from the high data density regions.
08:13 - Yeah, that's how you find it.
08:16 - And I guess one way you're trying
08:18 - to find stationary points, you're
08:19 - trying to maximize, I guess, the log likelihood.
08:22 - And it's not obvious how you would do it.
08:25 - You could do gradient ascent and try to find a local maximum.
08:30 - But the problem is that the gradient is not
08:32 - estimated accurately.
08:34 - If you imagine randomly initializing a data point,
08:37 - very likely you're going to be initializing it
08:39 - in the red region, and then you're
08:42 - going to follow the gradients.
08:43 - But the gradients are not accurate
08:44 - because they're estimated very inaccurately.
08:46 - And then your Langevin dynamics procedure
08:48 - will get lost, basically.
08:51 - That's kind of what happens is that if you think
08:56 - about those particles, a lot of those particles starts out here.
08:59 - And you're going to follow these arrows,
09:01 - but the arrows are pointing you in the wrong direction.
09:04 - So you're never going to be able to reach these high data density
09:09 - regions by following the wrong instructions somehow.
09:14 - You could try to initialize one of the data points.
09:16 - The problem is that still then it's not
09:19 - going to mix, which is what's going to come up next.
09:21 - But even though Langevin dynamics in theory converges,
09:26 - it can take a very long time.
09:28 - And you can kind of see the extreme case here
09:30 - where if you have a data density that
09:34 - is kind of like a mixture of two distributions
09:37 - where the mixture weights are pi and 1 minus pi, but crucially
09:42 - p1 and p2 have disjoint support.
09:44 -
09:48 - And so basically, you have probability pi, pp1
09:55 - when you are in A, and you have 1 minus pi p2 when you are in B.
09:59 - So there's two sets that are disjoint,
10:01 - and you have a mixture of two distributions that
10:03 - are with disjoint supports.
10:04 - Think of a mixture of two uniform distributions
10:07 - with two disjoint supports.
10:11 - If you look at the score function,
10:13 - you'll see it has this expression.
10:17 - It's just the log of this in the support
10:21 - of the first distribution and the log of this
10:23 - in the support of the second distribution.
10:26 - And you can see that when you take the gradient with respect
10:28 - to X, the pi disappears.
10:33 - So it does not depend on the weight
10:36 - that you put of the two mixture modes.
10:40 - And so that's the problem here is
10:42 - that the score function does not depend on the weighting
10:45 - coefficient at all.
10:46 - So if you were to sample just using the score function,
10:49 - you would not be able to recover what is the relative probability
10:52 - that you assign to the first mode versus the second mode.
10:56 - This is kind of an extreme case of Langevin
11:00 - not even mixing, basically.
11:03 - And, yeah, basically if you're not Langevin,
11:07 - it will not reflect pi.
11:08 - And here you can see an example of this where the true samples,
11:12 - they are--
11:13 - there is more samples up here than down here.
11:16 - So this p1 is maybe, I don't know, 2/3 of them are up here,
11:22 - and one third are down here.
11:24 - If you just run Langevin, you kind of
11:26 - end up with half and half.
11:28 - So it's not reflecting the right weight.
11:32 - And that's basically an indication that, again, Langevin
11:35 - is mixing too slowly.
11:38 - And then what we'll see in the next lecture is a way to fix it.
11:42 - That will actually make it work.
11:44 - And that's the idea behind diffusion models, which
11:47 - is to essentially figure out a way
11:49 - to estimate these scores more accurately all over the space
11:55 - and get better guidance.
11:56 - And that will actually fix this problem,
11:59 - and we'll get to the state of the art
12:01 - certainly diffusion models.

00:00 -
00:05 - SPEAKER: All right.
00:06 - So the plan for today is to continue
00:09 - our discussion of score-based models,
00:11 - and we'll see how they are connected to diffusion models.
00:15 - And we'll kind of see some of the state-of-the-art stuff that
00:20 - currently has been used to generate images, videos,
00:22 - some of the things we've seen in the very first introductory
00:26 - lecture.
00:28 - So brief reminder, this is kind of the usual roadmap
00:33 - slide for the course.
00:34 - Today, we're talking about diffusion models
00:38 - or score-based models.
00:39 - You can think of them as one way of defining this model
00:43 - family by parameterizing the score
00:45 - and then learning the data distribution by essentially
00:49 - using some kind of score matching loss.
00:51 - And we've seen that, yeah, that's
00:55 - kind of the key underlying idea is
00:58 - that to represent the probability distribution,
01:01 - we're going to use a neural network, which is vector valued.
01:07 - So for every point, it gives you a vector.
01:10 - And that vector is supposed to represent
01:13 - the gradient of the log likelihood at that point.
01:18 - So you can think of it as a vector field,
01:21 - like the one you see here, that is parameterized
01:23 - by some neural network.
01:25 - And so as you change the weights,
01:26 - you get different vector fields.
01:29 - And we've seen that it's possible to fit
01:33 - these models to data by doing score matching.
01:36 - So we've seen that the kind of machinery
01:40 - that we talked about in the context of energy-based models
01:43 - can be applied very naturally to these settings.
01:46 - And so there is a way to fit the estimated gradients
01:52 - to the true gradients by minimizing
01:54 - this kind of loss, which only depends on the model.
01:58 - This is the thing we derived by doing integration by parts.
02:02 - And it's a principled way of fitting the model.
02:06 - The issue is that it's not going to work in practice if you're
02:10 - dealing with high-dimensional settings because
02:12 - of this trace of the Jacobian term
02:15 - that basically would require a lot of back propagation steps.
02:19 - So it's not going to work if you're
02:21 - trying to save model images.
02:24 - And so in the last lecture, we talked about two ways
02:26 - of making score matching more scalable.
02:29 - The first one is denoising score matching,
02:31 - where the idea is that instead of trying
02:34 - to model the score of the data distribution,
02:36 - we're going to try to model the score
02:38 - of this noise-perturbed data distribution.
02:41 - And typically the way we obtain this noise-perturbed data
02:46 - distribution is by starting from a data point and then applying
02:50 - this kind of perturbation kernel,
02:52 - which gives you the probability of error--
02:55 - given that you have a clean image x,
02:56 - what is the distribution over noisy images x tilde?
03:01 - And it could be something as simple
03:02 - as, let's add Gaussian noise to x.
03:06 - And it turns out that estimating the score
03:08 - of this noise-perturbed data distribution
03:11 - is actually much more efficient computationally.
03:14 - And so the usual kind of score matching loss
03:17 - where you do regression, some kind
03:20 - of L2 loss between the estimated score
03:23 - and the true score of the noise-perturbed data density.
03:27 - That's kind of the key difference here.
03:28 - We're no longer estimating the score of p data.
03:30 - We're estimating the score of q sigma.
03:33 - It turns out that it can be rewritten
03:35 - in terms of the score of this transition kernel, perturbation
03:40 - kernel, q sigma of x tilde given x, which is just, let's say,
03:44 - a Gaussian.
03:45 - And so in the case of a Gaussian distribution,
03:49 - this objective function basically
03:51 - corresponds to denoising because the--
03:56 - yeah.
03:56 - Basically, the score of a Gaussian
03:58 - is just like the difference from the mean, essentially.
04:01 - And so you can equivalently think
04:05 - of denoising score matching as solving a denoising problem,
04:08 - where what you're doing is you're sampling a data point,
04:11 - you're sampling a noise vector, and then you're
04:14 - feeding data plus noise to the score model as data.
04:19 - And the goal of the score model is to try to estimate z
04:22 - essentially, which is the amount of noise
04:24 - that you've added to the clean data x.
04:28 - And so there's this equivalence between learning
04:31 - the score of the noise-perturbed data density
04:35 - and performing denoising.
04:37 - And as you see, this is much more efficient
04:39 - because we no longer have to deal with traces of Jacobians.
04:43 - This is-- everything is-- it's a loss that you can efficiently
04:47 - optimize as a function of theta.
04:50 - And so the pros is, yeah, it's much more scalable.
04:53 - It has this intuitive kind of correspondence to denoising,
04:58 - meaning that probability architectures that work well
05:00 - for denoising are going to work well for this kind of score
05:03 - estimation task.
05:05 - The negative side of this approach
05:09 - is that we're no longer estimating the score
05:11 - of the clean data distribution.
05:13 - We're now estimating the score of this noise-perturbed data
05:17 - density.
05:18 - And so we're kind of shifting the goal post here
05:22 - because we're no longer estimating
05:24 - the score of the true data density,
05:26 - but we're estimating the score.
05:27 - Even if we're doing very well at solving this problem,
05:30 - even if we can drive the loss to 0, we don't overfit,
05:33 - everything works well, we're no longer
05:35 - estimating what we started out with,
05:38 - but we're estimating this noise-perturbed data density
05:42 - score.
05:43 - And then, we've seen the alternative is
05:46 - to do some kind of random projection,
05:47 - and that's the sliced score matching approach, where
05:51 - essentially instead of trying to match
05:53 - the true gradient with the estimated
05:56 - gradient at every point, we try to just match their projections
06:01 - along some random direction v. And so at every point,
06:05 - we sample a direction vector v.
06:08 - Based on some distribution, we project the true score,
06:12 - the estimated score at every point.
06:14 - After the projection, you get scalars.
06:17 - And then you compare the projections.
06:19 - And if the vector fields are indeed the same,
06:21 - then the projections should also be the same.
06:24 - And it turns out that, again, this objective function
06:27 - can be rewritten into one that only
06:30 - depends on your model, kind of the same integration
06:32 - by parts trick.
06:34 - And now this is something that can be evaluated efficiently,
06:40 - can optimize efficiently as a function of theta,
06:44 - because essentially it only involves
06:46 - directional derivatives.
06:48 - And so it's much more scalable than vanilla score matching.
06:53 - It also estimates the score of the true data density,
06:55 - as opposed to the data density plus noise.
06:59 - But it's a little bit slower than denoising score
07:03 - matching because you still have to take derivatives, basically.
07:09 - So that's sort of where we ended kind of last lecture.
07:14 - And then the other thing we talked about is
07:17 - that how to do inference.
07:20 - And we said, well, if you somehow
07:23 - are able to estimate the underlying vector
07:26 - field of gradients by doing some kind of score matching,
07:30 - then there are ways of generating samples
07:32 - by using some kind of Langevin dynamics procedure, where
07:35 - you would basically--
07:36 - These scores are kind of telling you in which direction
07:39 - you should go if you want to increase
07:41 - the probability of your data point,
07:42 - and so you just follow these arrows,
07:44 - and you can generate samples, basically.
07:48 - And what we've seen is that this didn't actually
07:51 - work in practice, this variant of the approach.
07:54 - It makes sense but it doesn't work for several reasons.
07:57 - One is that, at least for images,
08:00 - we expect the data to lie on a low dimensional manifold,
08:04 - meaning that the score is not really a well-defined object.
08:09 - We have this intuition that we're not
08:12 - expecting to be able to learn accurate scores
08:15 - when we're far away from the high data density regions.
08:19 - If you think about the loss, it depends on samples
08:22 - that you draw from the data distribution.
08:24 - Most of the samples are going to come
08:26 - from high-probability regions.
08:29 - When you're far away, you have an object that looks nothing,
08:32 - let's say, like an image, you've never seen these things
08:35 - during training, it's unlikely that you're
08:37 - going to be able to estimate the score very accurately.
08:41 - And that's a problem because then kind of Langevin dynamics
08:44 - depends on this information to find high-probability regions.
08:47 - And so you might not-- you might get lost,
08:49 - and you might not be able to generate good samples.
08:52 - And then, yeah, we've seen that there
08:55 - are issues with the convergence speed of Langevin dynamics.
08:57 - It might not even converge if you
09:00 - have zero-probability regions somewhere.
09:02 - It might not be able to go from one region of the state--
09:05 - the space of possible images to another one.
09:08 - And so that's also an issue.
09:11 - And so what we are going to see today
09:14 - is that there is actually a very simple solution
09:16 - to all of these three issues that we just talked about.
09:21 - And that basically involves adding
09:23 - noise, adding, let's say, Gaussian noise to the data.
09:27 - And to see this, we notice that, well, one issue
09:32 - is that if the data lies on a manifold,
09:35 - then the score is not really defined.
09:37 - But the moment you add noise to the data,
09:40 - then it becomes kind of supported over the whole space.
09:43 - Noisy data, you are adding noise,
09:48 - so any possible combination of pixel values
09:51 - has some probability under this noise-perturbed distribution.
09:55 - And so even though the original data
09:57 - lies on a manifold, the moment you add noise,
10:00 - you fall off the manifold, and it becomes
10:02 - supported over the whole space.
10:03 -
10:06 - Score matching on noisy data will
10:10 - allow us to basically estimate the score much more accurately.
10:14 - This is kind of some empirical evidence showing
10:17 - if you try to do score matching on CIFAR-10 on clean images,
10:20 - the loss is very, very bumpy.
10:22 - You're not learning very well.
10:24 - But the moment you add noise to the data, tiny little amount
10:27 - of noise to the data, with some tiny little standard deviation,
10:31 - then the loss converges much more nicely.
10:33 -
10:37 - And it solves the issue of the fact
10:42 - that score matching is not accurate in low-data density
10:46 - regions.
10:47 - But remember, kind of the intuition
10:50 - was that most of your data points are going to come from--
10:54 - let's say if your data is a mixture of two Gaussians,
10:56 - one here and one here, most of the data will be--
10:59 - the samples that you see during training
11:01 - are going to come from this region or this region, the two
11:04 - corners, where the data is distributed.
11:07 - And as a result, if you try to use data fit a score model,
11:13 - there is a true score model in the middle-- there
11:15 - is a true score in the middle.
11:16 - There is an estimated score on the right
11:18 - it's going to be accurate around the high-data density regions.
11:21 - It's going to be inaccurate the moment you go far away.
11:24 - But if you think about adding noise,
11:28 - again, it's kind of a good thing for us
11:31 - because if you add noise to the data,
11:33 - then it's going to-- the samples of the noise-perturbed data
11:39 - densities are going to be, again, kind of spread out all
11:41 - over the space.
11:43 - And so what happens is that now if you think about where you're
11:47 - going to see your samples during training, if you add
11:49 - a sufficiently large amount of noise,
11:51 - the samples are going to be all over the space.
11:54 - They're going to be kind of spread around the whole space.
11:59 - And what this means is that if you are willing to add noise
12:02 - to your data, and you add a sufficiently large amount
12:04 - of noise, then we might be able to estimate the score accurately
12:10 - all over the space.
12:13 - And now, of course, this is good because it
12:16 - means that we might be able to get
12:19 - good information from our Langevin dynamics sampler.
12:23 - Like if we are relying on these arrows
12:25 - to go towards high-probability regions,
12:27 - Langevin dynamics will probably work if we do this.
12:31 - The problem is that we're no longer kind of approximating.
12:35 - We're no longer-- if you do Langevin dynamics
12:39 - over these estimated scores, you're
12:42 - going to be producing samples from the noisy data
12:44 - distribution, so you're going to be generating images
12:47 - that kind of look like this instead of generating
12:51 - images that look like this.
12:53 - So that's kind of the tradeoff here.
12:56 - Yes, we're going to be able to estimate the score more
12:58 - accurately, but we are estimating
13:00 - the score of the wrong thing.
13:01 - Before, what we were doing is we were estimating the score
13:04 - of the noisy data distribution.
13:06 - And so here, if you were to do this,
13:08 - yeah, you would be using the other score matching.
13:10 - You would solve a denoising problem.
13:12 - You will learn the score of the noisy data distribution.
13:15 - Now you follow that score, and you are producing noisy samples.
13:19 - On the one hand, you'd like sigma, the amount of noise
13:22 - that you add, to be as small as possible
13:24 - because then you're learning the score of the clean data.
13:27 - So presumably if you follow those scores,
13:29 - you're going to generate clean samples.
13:31 - On the other hand, if you do that,
13:33 - we're not going to be expected to learn the score very
13:37 - accurately.
13:38 - And so that's the dilemma that we have here, basically.
13:41 - You could use denoising score matching to estimate the score.
13:44 - In fact, that's what we would end up doing.
13:47 - So you could-- if you were to use denoising score matching,
13:51 - you would take data, you would add noise,
13:53 - you would solve the denoising problem.
13:55 - What you end up learning is the score
13:57 - of the perturbed data density.
14:00 - So you end up learning this.
14:02 - But that's not this, which is what you wanted.
14:07 - It's not the score of the clean data density.
14:10 - So in particular, if you were to then follow those scores
14:15 - that you have here, you would produce samples
14:18 - that according to their noise-perturbed data density,
14:22 - in particular, the images would look like this, not like this.
14:25 - So as I said, how you do this, or you could even
14:29 - do sliced score matching here or vanilla
14:31 - score-- you could do sliced score matching, for example,
14:34 - not vanilla, but you could do sliced score matching here
14:36 - to estimate this.
14:39 - Denoising score matching would be a much more natural choice
14:42 - because it's faster, and it automatically
14:44 - gives you the score of a noise-perturbed data density.
14:47 - So here I'm just saying even you were
14:50 - able to estimate the scores, they are not what you want.
14:53 - Using denoising score matching would be a very natural way
14:56 - of estimating these scores.
14:58 - And that's what we're actually going to do.
15:00 - If you recall from the last slide
15:03 - we were saying, OK, if you were to do a PCA of the data,
15:05 - and you keep a sufficiently large number of components,
15:09 - you reconstruct the data almost perfectly, which basically means
15:13 - that the different pixels in an image,
15:15 - they are not linearly independent.
15:17 - They kind of-- once you know a subset of them,
15:20 - you get all the others automatically,
15:22 - which basically means that the images lie
15:25 - on some kind of plane, essentially, which
15:28 - is sort of what I'm visualizing here with this shape.
15:32 - So not all possible pixel values are actually
15:37 - valid in the data distribution, essentially.
15:40 - There is some kind of constraints,
15:41 - which you can think of it as encoding this kind of curve.
15:44 - And all the images that we have in the data,
15:47 - they lie on this surface or this curve
15:50 - in a high-dimensional space.
15:52 - And so the score is not quite well
15:55 - defined because what does it mean to go off the curve,
15:58 - then kind of the probability is 0 the moment
16:00 - you go off the curve.
16:01 - And so it can explode, basically.
16:05 - The moment you add noise, then basically
16:07 - any combination of pixel values is valid
16:10 - because there's always some probability
16:11 - of adding the right amount of noise
16:13 - such that that combination was possible.
16:16 - So if you imagine data that lies on a plane or that kind
16:20 - of surface, and then you add noise,
16:22 - you're kind of moving the value by a little bit,
16:26 - and then it's no longer lies on that plane,
16:28 - or no longer lies on the surface.
16:30 - So you're breaking that constraint
16:33 - that held for the real data no longer holds
16:37 - for noise-perturbed data, and that helps estimating
16:41 - the gradient more accurately.
16:43 - Yeah, so noise does become more stable,
16:45 - but you do have this problem that, as you said--
16:48 - if you add sufficiently small amount of noise--
16:51 - it's going to be more or less.
16:53 - There is not really a discontinuity.
16:55 - So yeah, you add a very small amount of noise,
16:58 - your noise-perturbed data distribution
16:59 - is very close to what you wanted, so that's great.
17:02 - But you're not really solving the problems
17:04 - that we have here, basically.
17:06 - That's what I have in the next slide here.
17:08 - That's the question, how much noise do we want to add?
17:12 - Do you want to add a very little small amount of noise?
17:14 - Do you want to add a lot of noise?
17:16 - That is kind of-- if you think about the different amount
17:21 - of noise that you can add, you're
17:23 - going to get different sort of trade-offs.
17:26 - You can imagine that there is the real data density.
17:30 - There is the real scores.
17:31 - And if you try to estimate them using score matching,
17:34 - there's going to be a lot of error,
17:36 - kind of in this region, as we discussed.
17:40 - Then you could say, OK, now, I'm going
17:42 - to add a little bit of noise.
17:43 - So I'm no longer estimating the right thing,
17:46 - so there's going to be a little bit of error
17:48 - everywhere because I'm estimating
17:50 - noise-perturbed scores instead of true scores,
17:53 - but my estimation starts to become a little bit better.
17:57 - And then you can add even more noise.
18:00 - And then at some point, you are doing a great job
18:03 - at estimating the scores.
18:04 - But you're estimating the scores of something
18:06 - completely wrong because you added
18:08 - too much noise to the data.
18:11 - And maybe that's the extreme where you add a ton of noise,
18:14 - you've completely destroyed the structure
18:17 - that was there in the data.
18:19 - So what you're estimating has nothing
18:21 - to do with the clean images that you started from,
18:24 - but you're doing a very good job at estimating the score
18:26 - because it becomes very easy.
18:29 - So those are the things that you need to balance.
18:32 - We want to be able to estimate the score accurately,
18:34 - and so we would like to add as much noise as possible
18:36 - to do that.
18:37 - But at the same time, adding noise
18:40 - reduces the quality of the things
18:42 - we generate because we're estimating
18:44 - the score of a noise-perturbed data density.
18:47 - So we're no longer estimating what
18:49 - we wanted, which is the thing that we have up here,
18:52 - but we're estimating the score of a data distribution
18:55 - with a lot of noise added.
18:57 - The noise you can estimate any of this.
18:59 - You can use the noisy score matching
19:00 - to estimate the score of any of these slices.
19:03 - But it's going to perform--
19:06 - it might become very bad if the amount of noise that you add
19:09 - is very small.
19:10 - And so that's kind of what you see here.
19:13 - Well, this is maybe the clean score,
19:15 - or maybe this is a little bit of noise,
19:18 - denoising score matching is not going to work very well.
19:21 - Now, denoising score matching is a way
19:23 - of estimating the score of a noise-perturbed data density
19:27 - with any amount of noise that you want.
19:30 - The question is, how much noise do you want to add?
19:32 - You'd like to add as little noise as possible
19:34 - because you want to estimate something
19:36 - close to the real data.
19:38 - But the more noise you add, the better estimation becomes.
19:43 - And so that's kind of the problem,
19:45 - that you want to trade off these two things,
19:46 - and it's not clear how to do that.
19:50 - So this is perhaps another way to think about it.
19:53 - Imagine that somehow the data lies on this curve,
19:56 - and this is just like a curve in a 2D space.
20:01 - Most of your samples are going to be
20:02 - close to that to that thick line that we have here.
20:08 - What's happening?
20:09 -
20:12 - And so if you were to estimate the score far away
20:16 - from the black curve, it's going to be fairly inaccurate.
20:20 - Then you can imagine, OK, let's add a lot of noise, sigma 3.
20:24 - Then most of the samples are going
20:26 - to be pretty far away from the from the black curve.
20:30 - And so we're going to get pretty good directional information
20:34 - when you're far away from a clean sample.
20:37 - But it's going to be inaccurate the moment you get closer
20:40 - to the real--
20:42 - where the real data lies.
20:45 - And then you can imagine a setting
20:47 - where you have an ensemble of different noise levels.
20:52 - You're not just considering a single noise level,
20:54 - but you are considering many of them
20:57 - so that you are able to get good directional information,
21:01 - both when you're far away and when
21:02 - we are a little bit closer and a little bit
21:04 - closer to the real data distribution.
21:08 - And that's kind of the main underlying
21:11 - idea of a diffusion model or score based model.
21:14 - The key idea is that we're not just
21:16 - going to learn the score of the data,
21:19 - or we're not just going to learn the score of the data
21:22 - plus a single amount of noise, but we're
21:24 - going to try to learn the score of the data perturbed
21:27 - with different kinds of amounts of noise.
21:31 - That's the intuition.
21:33 - And so specifically, we're going to consider
21:36 - different amounts of noise, sigma 1, sigma 2, all the way
21:39 - to sigma l.
21:41 - And we're going to use something called annealed Langevin
21:45 - dynamics to basically generate samples.
21:49 - And the basic idea is that when we start, when we initialize
21:54 - our Langevin dynamics procedure, there's
21:57 - probably going to be very little structure in the samples.
22:00 - They don't look like natural images.
22:03 - And so what we can do is we can follow the scores that
22:07 - were estimated for the data distribution, plus a lot
22:11 - of noise.
22:13 - And for a little bit, if you were to keep running this thing,
22:16 - then you would be able to generate samples from the data
22:20 - distribution, plus a lot of noise,
22:22 - which is not what we want.
22:24 - But what we can do is we can use these samples to initialize
22:29 - another Langevin dynamics procedure, where
22:31 - we've decreased the amount of noise by a little bit.
22:36 - And then you, basically, keep running your Langevin dynamics
22:39 - procedure following the scores, corresponding
22:42 - to the data density plus a smaller amount of noise sigma 2.
22:47 - Then you decrease it even more, and you initialize--
22:51 - because you got closer and closer to the high-data density
22:54 - regions, then we know that now we
22:58 - are starting to see more structure in the data.
23:00 - And so we should follow the score for the data density plus,
23:05 - let's say, a very small amount of noise.
23:07 - And then, again, you kind of follow the arrows,
23:10 - and then you're generating samples that we actually--
23:13 - like the ones we actually want because at this point,
23:16 - we kind get the best of both worlds
23:17 - because at the end of this procedure,
23:20 - we're generating samples from data plus a very
23:24 - small amount of noise.
23:26 - But throughout the sampling procedure,
23:28 - we are always kind of getting relatively accurate estimates
23:31 - of the scores because we are considering
23:34 - multiple kind of noise scales.
23:36 - So at the very beginning, where there
23:38 - was no structure in the data, we were
23:40 - following the score corresponding
23:41 - to data plus a lot of noise.
23:43 - And then as we add more and more structure to the data,
23:46 - because we are moving towards higher-probability regions
23:48 - by following these arrows, then we can afford to reduce the--
23:54 - basically consider the gradients of data
23:57 - that was perturbed with smaller amount of noise.
24:01 - And this procedure will get us the best of both worlds
24:05 - because Langevin dynamics is never lost.
24:07 - We're always following a pretty accurate estimate
24:10 - of the gradient.
24:12 - But at the same time, at the end,
24:14 - we're able to generate samples for a distribution of data
24:18 - plus noise, where this noise level sigma 3
24:21 - can be very, very small.
24:22 - So this final samples that you produce
24:24 - are going to be almost clean.
24:26 - Yeah, so typically people use 1,000.
24:29 - That's the magic number.
24:30 - But then we'll talk--
24:32 - in the second part of the lecture,
24:33 - we'll talk about an infinite number of noise levels.
24:35 - So the natural way to do things is
24:37 - to actually consider continuous number, like an infinite number.
24:42 - And that's kind of it gets you the best of--
24:45 - a lot of structure, a lot of interesting things.
24:47 -
24:51 - So that's sort of like the intuition.
24:53 - And you can see here another example of,
24:57 - what happens if you were to just run Langevin dynamics?
25:01 - It kind of gets has this problem where you're
25:05 - seeing too many particles down here because it doesn't
25:09 - mix sufficiently rapidly.
25:11 - And even though there should be more probability mass up here,
25:14 - meaning that more particles should end up here,
25:17 - they're just kind of too many down here
25:20 - because the arrows are basically-- you're not
25:22 - estimating things accurately.
25:24 - And if you do annealed Langevin dynamics,
25:27 - so you use this procedure where you
25:29 - run multiple lines of dynamics chains
25:31 - corresponding to different amounts of noise,
25:34 - then it ends up giving you the right sort of distribution,
25:36 - where you see there is many fewer particles down here
25:40 - representing the fact that there should be less probability
25:43 - mass down there.
25:44 -
25:47 - And yeah, here is another example showing this,
25:51 - but let me skip.
25:52 - So what does it mean in practice?
25:55 - What it means in practice is that in order to do this,
25:58 - you need to be able to estimate the score, not just of the data
26:04 - density, not just of the data density
26:06 - plus a certain fixed amount of noise.
26:09 - But you need to be able to jointly estimate
26:12 - the score of the data plus different amounts of noise
26:16 - levels, various amounts of noise levels.
26:19 - So you need to be able to know what is the score of data
26:22 - plus a lot of noise.
26:23 - You need to be able to know what is the score of data
26:26 - plus a little bit less noise, all the way
26:28 - down to a very, very small amount of noise
26:30 - added to the data where it's almost kind of the true data
26:36 - density.
26:37 - And that's fine because if you do a annealed Langevin
26:40 - dynamics-- even though this score is only ever going
26:43 - to be estimated accurately close to the high-data density
26:46 - regions, we still have that problem.
26:48 - This score here is not going to be estimated
26:51 - accurately everywhere.
26:52 - It's only ever going to be estimated accurately
26:55 - when you're very close to, let's say, a real image.
26:58 - But that's fine because we're using this Langevin dynamics
27:00 - procedure, and we're only going to use this score
27:03 - model towards the end of the sampling, where we already
27:06 - have a pretty good guess of the kind of images we want.
27:09 - While this score here, which is data plus a ton of noise,
27:13 - is going to be estimated pretty accurately.
27:16 - Everywhere, it's going to be good at the beginning
27:18 - of the sampling, but we don't want
27:20 - to just keep following that because we
27:21 - want to be able to sample from something close
27:24 - to the clean data distribution.
27:28 - And so to make things efficient, what we would do
27:32 - is we would have--
27:34 - you could potentially train separate score networks,
27:38 - one for every noise level.
27:40 - If you have, let's say, 1,000 noise levels, that
27:43 - would mean 1,000 different neural networks, kind
27:46 - of training each one being trained
27:49 - on a different kind of vector field.
27:53 - To make things more efficient in practice, what you can do
27:56 - is you can have a single neural network that
27:59 - takes an additional input parameter sigma, which
28:03 - is basically just the amount of noise that we're considering,
28:06 - and the single neural network will jointly
28:09 - estimate all these different vector fields.
28:12 - So when you fit in a large value of sigma here as an input,
28:16 - then the network knows that it should be estimating the vector
28:20 - field for, let's say, data plus a lot of noise,
28:23 - while when you fit in as an input a small value of sigma,
28:26 - then the network knows that it needs
28:28 - to estimate the vector field for data density
28:31 - plus a small amount of noise.
28:34 - And so this is just basically a way
28:36 - to kind of make the computation a lot more efficient because we
28:39 - have now a single model that is trained to solve
28:42 - all these different estimation problems.
28:45 - It's going to be worse than just training 1,000 separate models,
28:49 - but it's going to be much more efficient because we're just
28:51 - training a single neural network at that point.
28:55 - Yeah, so what we're learning here,
28:57 - so this vector fields are not necessarily conservative,
29:00 - unless you parameterize the network in a certain way.
29:03 - You could potentially parameterize the network such
29:06 - that it's the gradient of an energy function.
29:09 - Actually, it doesn't hurt performance
29:10 - too much if you do that, but it doesn't actually seem to help.
29:14 - So in practice, you can just use a free form
29:16 - kind of neural network that goes from, say, images to images,
29:20 - and that's not a problem.
29:23 - But you're right that it's not necessarily
29:25 - the gradient of a potential of an energy function,
29:27 - and so weird things can happen.
29:29 - Where if you follow a loop the probability can go up or down,
29:33 - even though if there was really an underlying energy,
29:36 - it shouldn't change the probability.
29:39 - So that could be a problem.
29:41 - But in practice, it works.

00:00 -
00:05 - SPEAKER: What is the model look like?
00:07 - So the model is going to be a single neural network that
00:09 - will try to jointly estimate all these scores.
00:12 - How do we actually learn it?
00:14 - We're going to learn it by denoising score matching.
00:17 - So there's going to be this noise conditional score
00:19 - network, which is going to be a network that jointly estimates
00:24 - all these vector field of scores.
00:26 - And how should we train this?
00:29 - You could do sliced score matching.
00:32 - It's much more natural to just use denoising score matching
00:35 - since denoising score matching already
00:36 - gives you the score of a noise perturbed data density,
00:40 - you might as well directly use that.
00:43 - So since we were trying to model scores of data plus noise,
00:49 - we might as well just directly use
00:51 - denoising score matching because that's
00:53 - a little bit more efficient.
00:55 - And then the loss function is going
00:57 - to be kind of a weighted combination of denoising score
01:01 - matching losses because we want to jointly solve, let's say,
01:06 - 1,000 different tasks.
01:08 - And so the loss function might look something
01:10 - like this where we have this nice conditional score
01:14 - network that takes as input a data point and a noise level
01:18 - and tries to estimate the score of the data distribution
01:22 - perturbed with that noise level at that point x.
01:26 - And we want to train this core network to perform well
01:30 - across different noise levels.
01:32 - So if you have the l noise levels,
01:34 - this noise network, this noise conditional core network
01:37 - should be able to solve all these different regression
01:39 - problems as well as possible.
01:42 - And so there is a lambda sigma I parameter here
01:46 - that basically controls how much you
01:47 - care about estimating accurately the scores at different noise
01:51 - levels.
01:52 - Yeah, so the loss would look like this.
01:55 - So the data is clean as you said.
01:56 - And then you add noise corresponding to the sigma
02:01 - that you care about, and then you
02:02 - try to denoise that data point.
02:04 - And so if you think of it from the denoising perspective,
02:07 - we're not just learning to denoise
02:09 - data that has been perturbed with a fixed amount of noise,
02:12 - but we're basically learning a family
02:14 - of denoisers, each one working with different amounts of noise.
02:19 - So there is a denoiser that works
02:20 - when the data is corrupted with a very large amount of noise,
02:24 - and there's going to be a denoiser that works when
02:26 - the data is corrupted with a smaller amount of noise
02:29 - all the way down to almost no noise being added to the data.
02:33 - And these are different denoising problems
02:35 - and equivalently corresponding to estimating
02:38 - the scores of different noise perturbed data distributions.
02:44 - Yeah, so when the noise is very large,
02:47 - basically denoising is very hard.
02:50 - If I add an infinite amount of noise, all the structure
02:54 - and the original data is lost, the best you can do
02:57 - is to basically output the average image, essentially.
03:00 - That's the only thing you can do.
03:02 - There is no information in x tilde about x.
03:06 - So the best thing you can do is to basically
03:08 - if you're trying to minimize an L2 loss, is to predict the mean.
03:11 - And you can imagine that if that's the only thing you have,
03:15 - you're not going to be able to generate good images.
03:17 - But because you know also how to denoise images
03:20 - with less amounts of noise, then if you know all this score
03:24 - models, then you can do that annealed Langevin dynamics
03:27 - procedure, and you can actually generate clean images
03:30 - at the end.
03:31 - So, yeah, we'll get to what lambdas makes sense.
03:35 - In theory, like if you had infinite capacity,
03:37 - this model is arbitrarily powerful.
03:39 - It doesn't even matter because it could
03:41 - solve perfectly each task.
03:43 - In practice, it matters how you weight this different score
03:48 - matching losses.
03:49 - We'll get to that.
03:51 - And, yeah, but the loss function basically looks like this.
03:56 - It's a mixture of denoising score matching objectives
04:00 - across all the different noise levels that we are considering.
04:04 - It's true that they are all related to each other.
04:06 - So in theory, if you know the score at a particular noise
04:10 - level, you can in theory recover the score of different noise
04:15 - levels.
04:16 - But it's not just a scaling.
04:18 - So there is something called the Fokker-Planck equation.
04:21 - And if you were to solve that equation,
04:24 - which is just the PDE at the end of the day,
04:26 - but if you were able to solve that PDE,
04:29 - that tells you basically how the scores
04:31 - are related to each other.
04:33 - There are papers.
04:34 - We've done a number of--
04:36 - we have a number of papers trying
04:38 - to enforce that condition because in some sense,
04:41 - this is just treating all these tasks as being independent.
04:45 - But we know that they are related to each other
04:47 - as you were suggesting.
04:48 - And so you might be able to do better if you tie together
04:53 - the losses because you know how to go from one solution
04:56 - to the other solution.
04:57 - In practice, it doesn't seem to help a lot.
05:00 - But you're right.
05:01 - There is something called-- yeah,
05:02 - if you could solve the Fokker-Planck equation,
05:04 - you could go from, yeah, go from any score
05:07 - to any other score at least in continuous time.
05:10 -
05:14 - Cool.
05:14 - So now we need-- we have several choices to make.
05:17 - We need to choose what kind of noise scales
05:21 - are we going to consider.
05:22 - So we need to decide on what is the maximum amount of noise
05:26 - that we're going to add.
05:27 - We need to decide the minimum amount of noise
05:31 - that we're going to add, and we need
05:33 - to decide how to step in between these two extremes, essentially.
05:38 - And for the maximum noise scale, you probably
05:42 - want to choose it to be roughly the maximum pairwise distance
05:47 - between any two data points.
05:49 - And the idea is if you have two images in the data, x1 and x2,
05:56 - you want the amount of noise to be sufficiently large
05:59 - so that basically it's possible to go from one data
06:04 - point to the other data point if you were to add noise,
06:08 - essentially.
06:08 - So if you start from data point one,
06:10 - and you were to add a sufficiently large amount
06:12 - of noise, there should be a reasonable probability
06:15 - of generating a data point two and equivalently
06:20 - going back on the other on the other direction.
06:24 - And this basically ensures that at the beginning when
06:27 - you do start out your lingerie in dynamics procedure
06:31 - with a lot of noise, that's going to mix--
06:33 - it's going to explore the space pretty efficiently because there
06:38 - is a way to go basically from any point to any other point.
06:42 - That's the intuition for this choice.
06:48 - The minimum noise scale, you probably
06:50 - want it to be sufficiently small so that the image plus noise
06:56 - is hard to distinguish from just a clean image.
07:00 - So the minimum noise scale should be very, very small.
07:06 - And the other thing to decide is how you go from the maximum
07:15 - to the minimum.
07:16 - So how do you interpolate between these two extremes?
07:19 - And again, the idea is that if you think about that Langevin
07:22 - dynamics procedure, we want to make sure
07:25 - that these different noise scales have sufficiently overlap
07:31 - so that when you initialize the Langevin dynamics chain
07:34 - corresponding to the next noise level,
07:36 - you're starting with something that makes sense.
07:40 - And so if you imagine that you have this spheres where they are
07:45 - increasingly corresponding to what you would get
07:49 - or the probability as you increase the amount of noise
07:52 - that you add, and you go sigma 2, sigma 1, sigma 3, essentially
07:59 - what you want to make sure is that when you have, let's say,
08:02 - data plus noise level sigma 2, there
08:06 - should be sufficiently overlap with the data points
08:10 - you expect when you have data plus noise level sigma 3 so
08:15 - that when you use the samples that
08:17 - were obtained by running Langevin dynamics with sigma 2
08:20 - noise levels, and you use them to initialize the Langevin chain
08:24 - corresponding to noise level sigma 3,
08:26 - you have something that makes sense
08:29 - because if there is no overlap--
08:31 - So you go very--
08:32 - you have like a drastic reduction in noise level.
08:35 - Let's say, you go from a lot of noise to very little noise
08:38 - after running your Langevin chain for the large amount
08:41 - of noise, you're not going to get a good initialization
08:45 - for the next noise level.
08:47 - But if there is a decent amount of overlap between these noise
08:51 - levels, then you might expect this annealed Langevin dynamics
08:57 - procedure to actually work pretty well.
09:00 - We are deciding how to do sigmas right.
09:03 - And so what you can do is you can actually
09:04 - work out a little bit of math about what makes sense.
09:09 - And it makes sense according to this heuristic
09:12 - to basically use some kind of geometric progression
09:16 - between the different noise levels.
09:19 - This ensures that there is sufficiently
09:22 - overlap between the different shells
09:25 - that you get as you increase the-- or you decrease the amount
09:28 - of noise that you add.
09:30 - And this is a heuristic.
09:32 - It's not necessarily the only valid choice,
09:34 - but this is the first one that we did that seemed to work.
09:39 - The other thing we can decide is the weighting factor.
09:45 - Remember, we're jointly solving all these estimation problems
09:48 - corresponding to different noise levels.
09:50 - How much should we care about the different noise levels which
09:56 - is controlled by this lambda sigma I hyperparameter that
09:59 - decides how much weight do you put on the different components
10:03 - of the loss?
10:05 - And so how do we choose this weighting function?
10:09 - The idea is that you want to balance
10:11 - the importance of the different components in the loss.
10:16 - And a reasonable heuristic that, again, works well in practice
10:20 - is to choose that to be equal to the amount of noise
10:24 - that you're adding at that level.
10:26 - It's because the size of the arrow
10:28 - changes also, the way it's scaled.
10:32 - Do I have it here?
10:34 - So the loss would look something like this.
10:36 - And basically, yeah, essentially you actually
10:43 - end up carrying the same about the different noise levels
10:47 - if you do that choice because of the various scaling factors.
10:53 - So remember there is a sigma I here.
10:54 -
11:00 - Again, it's like a choice.
11:02 - Other choices can work as well.
11:04 - But this is the thing that we did.
11:08 - Yeah, so epsilon theta here is basically a noise prediction
11:12 - because it's basically literally just estimating
11:14 - the noise that was added.
11:16 - There is different parameterization
11:18 - where you might want to predict as we discussed when the noise
11:23 - level is very high, then you might want to predict the mean.
11:27 - So there are different ways of parameterizing
11:29 - this, what the network is supposed to output.
11:32 - This is kind of the most--
11:33 - the simplest one where you're just predicting.
11:35 - It's a noise prediction kind of task.
11:37 -
11:41 - And so the final loss looks something like this.
11:45 - You're basically sample a mini batch of data points,
11:48 - then you sample a mini batch of noise indices.
11:53 - So you basically equivalently choose
11:56 - a bunch of noise levels, one per data point, let's say,
12:02 - uniformly across the different noise scales
12:05 - that we're willing to consider that could be--
12:07 - maybe L here could be 1,000 if you have 1,000 different noise
12:11 - levels.
12:13 - And then what you do is you sample noise IID, one noise
12:20 - vector per data point.
12:22 - And then you basically train this core network
12:27 - to solve the denoising problem for each data point,
12:31 - essentially, with the weighting function that we had before.
12:35 -
12:39 - And then you basically just do stochastic gradient descent
12:42 - on this loss trying to essentially find
12:46 - parameters theta that minimize this denoising loss which
12:50 - is equivalent to essentially estimating
12:53 - the scores of the data density perturbed
12:57 - with these various noise levels as well as you can.
13:02 - And so basically, everything is just
13:07 - as efficient as training a single score model
13:11 - because everything is amortized, and there's a single score
13:14 - network that is jointly trained to estimate the score of data
13:19 - plus noise at different noise intensities.
13:24 - And so the final thing looks like this.
13:28 - You have data, clean data, and then you have noise at the end.
13:32 - And so there is going to be correspondingly kind
13:35 - of different versions of the data distribution
13:42 - that has been perturbed with increasingly large amounts
13:45 - of noise going from clean data, mediumly perturbed
13:49 - data, all the way to data plus a ton of noise
13:52 - where basically then the structure is completely
13:55 - destroyed.
13:55 - So visually, you can think of clean data, data
13:58 - plus a little bit of noise, more noise, more noise, more noise,
14:01 - all the way to huge amount of noise
14:04 - where you don't even recognize what you started from.
14:07 - So during learning, I guess, there is always going to be--
14:10 - I mean, it's a scalar, and so there's
14:11 - always going to be an ordering.
14:12 - And so you're always going to be considering--
14:14 - you're going to always going to go from the smallest amount
14:16 - to the largest amount of noise.
14:17 - During inference, it might make sense
14:20 - to do different things than what I described
14:24 - is annealed Langevin dynamics where you would
14:26 - start from a lot of noise and you go all the way
14:29 - to clean data.
14:30 - There are versions where you might not want to do this.
14:33 - And so you might want to go from noise to data
14:37 - and then maybe a little bit of noise and then go back.
14:42 - So, yeah, there is a lot of flexibility
14:43 - then at the inference.
14:46 - At training time, you need to estimate all of them.
14:48 - And there's always going to be a mean,
14:49 - and there's going to be a max.
14:50 - And then you can think about how you, yeah, how you space them.
14:57 - All of them-- there is a single model
14:59 - that is jointly trained to solve all these denoising tasks.
15:05 - And in fact, you can get better performance
15:07 - if you're willing to consider multiple.
15:09 - Like some of the state of the art models that are out there,
15:12 - they don't have a single one.
15:13 - They might have a few of them.
15:15 - And because they can afford to train a bunch of different noise
15:19 - models, it might make sense to do it
15:21 - because this is purely like a computational trick.
15:24 - If you could, it would be better to jointly separately estimate
15:28 - the scores for every noise level.
15:31 - OK, so then there's going to be different noise levels.
15:34 - For every noise level, there's going
15:36 - to be a corresponding data density plus noise, which we're
15:39 - going to denote p sigma 1.
15:41 - And this is the same as the Q sigma
15:42 - that we had at the beginning.
15:43 - So you can think of it as data becoming increasingly corrupted
15:48 - as you go from left to right.
15:51 - For each one of these noise levels,
15:53 - there's going to be a corresponding score model.
15:57 - So all these scores are going to be different
15:59 - because we are adding increasingly large amounts
16:01 - of noise.
16:02 - And then there's going to be a single neural network that
16:05 - is jointly trained to estimate all these vector fields.
16:09 - And the network takes the data x where you
16:11 - are in this plot and the sigma.
16:14 - And it will estimate the score for that noise level.
16:17 -
16:20 - And we jointly train them by this mixture of denoising score
16:24 - matching objectives, and which is just
16:29 - a mixture of this usual score matching loss.
16:32 - And then we do, again, Langevin dynamics to sample.
16:36 - So what you would do is you would initialize your particles
16:39 - at random.
16:40 - Presumably that is pretty close to just pure noise.
16:45 - So we initialize our particles here,
16:48 - and then we follow this course for p sigma 3
16:52 - which is data plus a lot of noise.
16:55 - We're getting a little bit closer to the higher data
16:57 - density regions.
16:58 - And then we use these samples to initialize a new Langevin
17:02 - dynamics chain for the data density plus a little bit
17:05 - less noise.
17:07 - We follow this arrows again, and you
17:11 - see that the particles are moving
17:13 - towards the high probability regions of the original data
17:15 - density.
17:16 - So they're becoming more and more structured in some sense.
17:19 - And then we use these particles to initialize another Langevin
17:22 - chain corresponding to an even smaller amount of noise.
17:27 - And at the end, we get samples that
17:31 - are very close to the original data density
17:34 - because sigma 1 is very, very small.
17:36 - So this is almost the same as clean data.
17:40 - And throughout the process, we are getting good information,
17:43 - good directional information from our score models
17:46 - because we're using the corresponding noise
17:50 - level in the data density although the arrow is
17:54 - pointing you in one direction because it's deterministic.
17:58 - Recall the Langevin dynamics is following the arrow,
18:00 - but it's also adding noise at every step.
18:03 - And so regardless of where you are,
18:05 - if you run it for a sufficiently long amount of time,
18:10 - you might end up somewhere completely differently.
18:13 - So even though you initialize the particle in the same point,
18:16 - maybe you initialize it here, because of the randomness
18:19 - in the Langevin dynamics, you might end up
18:21 - in completely different places, meaning
18:23 - that you're going to generate completely different,
18:26 - let's say, images.
18:26 -
18:30 - And so that's the procedure near Langevin dynamics.
18:34 - And here you can see it in how it works.
18:39 - This is kind of what happens if you
18:41 - follow that exact procedure on real data sets.
18:46 - Let me play it again.
18:47 - So you see how you start from pure noise,
18:50 - and then you're kind of following this arrows,
18:53 - you're following these gradients,
18:54 - and you're slowly basically turning noise into data.
18:59 -
19:02 - Here you can see examples on some image data
19:04 - sets, MNIST CIFAR-10, so forth.
19:07 - And you can see that it has this flavor going from noise to data
19:11 - by following these score models.
19:15 - How many steps?
19:16 - Ideally, you would want to do as many as possible.
19:18 - In practice, that's expensive, so you might want
19:21 - to do 1, or 2, or maybe 10.
19:24 - Each step is expensive because it
19:27 - requires a full neural network evaluation.
19:29 - And so that's again, a hyperparameter.
19:33 - You should do as many as you can.
19:35 - But the more you do, the more expensive
19:37 - it is to generate a sample.
19:39 - These are all generated yeah, yeah, yeah,
19:41 - through the Langevin dynamics, annealed Langevin dynamics
19:45 - procedure.
19:46 - Oh, yeah, and MNIST is basically a data set
19:49 - of handwritten digits.
19:50 - They look like that.
19:51 - And then you have people's faces.
19:52 - And you have CIFAR-10 samples.
19:54 - So these are pretty good kind of samples that
19:57 - were generated by this model.
19:58 - They have the right structure.
19:59 - And the model is able to generate reasonable
20:02 - looking images.
20:04 - It's not theoretically grounded.
20:05 - I think one key advantage is that if you think about it,
20:09 - they can be much more expensive at the inference time.
20:12 - Imagine you're running a Langevin dynamics chain which
20:15 - might involve evaluating a neural network 1,000 times,
20:18 - 10,000 times.
20:20 - So if you think of it from that perspective,
20:22 - you have a very deep computation graph
20:25 - that you're allowed to use at inference time.
20:28 - But at training time, you never have to actually unroll it.
20:31 - So that's the key insight that you're
20:34 - allowed to use a lot of computation at inference time.
20:37 - But it's not very expensive to train because it's not
20:40 - trained by like, again, by generating a full sample
20:43 - and then checking how should I change my parameters to make
20:47 - the sample better.
20:49 - It's trained very incrementally to slightly improve
20:53 - the samples by a little bit, and then
20:56 - you keep repeating that procedure at inference time,
20:58 - and you get great samples.
21:01 - They are trained in a very different way.
21:03 - There is no two sample test.
21:04 - They are trained by score matching.
21:06 - The architectures are different.
21:08 - The amount of compute that you use
21:09 - during training and inference time are different.
21:12 - So there's a lot of things that change.
21:16 - Hard to say there's no theoretical argument
21:18 - for why this is better.
21:19 - But in practice, yeah, it seems to be dominating.
21:22 - It's also much more stable to train
21:23 - because it's all like just score matching, so no minimax.
21:27 - Yeah, now if you look at certain kind of metrics
21:30 - that we'll talk about more in a few
21:32 - lectures, but this was the first model that was actually
21:36 - able to beat GANs back then where
21:39 - the state of the art on image generation.
21:42 - That was the first hint that these kind of models
21:44 - could actually outperform GANs despite a lot of years
21:48 - and years.
21:49 - And lots of resources that were spent in optimizing GANs,
21:52 - this thing was actually able to improve sample quality according
21:56 - to some metrics.
21:58 - And indeed these are different kinds of data sets.
22:01 - So scaling up the model a little bit,
22:04 - it can generate pretty reasonable faces
22:06 - of people and monuments and things like that.
22:11 - Yeah, we'll talk about the metrics more in future lectures.
22:15 - But there are metrics that try to quantify
22:17 - how good the samples are, how closely they
22:19 - match the-- they relate to how visually appealing
22:24 - the samples are.
22:25 - And they are automated, so there's no human in the loop,
22:27 - and they correlate with how good the samples are.
22:31 - Not super important what they are exactly,
22:33 - but the important bit was that these
22:36 - are the first model that was actually competitive with GANs.
22:39 - And that's prompted a lot of the follow-up work
22:43 - on really scaling these models.

00:00 -
00:05 - SPEAKER 1: And you might also wonder,
00:07 - because I think it was also kind of brought up here,
00:09 - like what does the noise levels that you use during inference
00:14 - have to match the ones that you see during training,
00:17 - can we use more noise levels, less.
00:20 - And so it's pretty natural to think
00:22 - about what happens if you have an infinite number of noise
00:24 - levels.
00:26 - Right now, we have kind of the clean data
00:28 - distribution, which let's say is just a mixture of two Gaussians.
00:32 - So here yellow denotes high probability density
00:35 - and blue denotes low probability density.
00:38 - And then so far, what we said is that we're
00:41 - going to consider multiple versions of this data
00:43 - distribution perturbed with increasingly large amounts
00:47 - of noise.
00:47 - So sigma 1, sigma 2, sigma 3, where
00:50 - sigma 3 is a very large amounts of Gaussian noise.
00:54 - So that's kind of the structure in the data is completely lost.
00:57 - So if you start out with a distribution,
00:59 - it's just a mixture of two Gaussians.
01:01 - After you add a sufficiently large amount of noise,
01:04 - you are left with just pure noise essentially.
01:08 - And you could imagine using maybe here I
01:11 - have 3 different noise levels.
01:13 - You could imagine--
01:16 - And you can always plot these densities.
01:19 - So you have most of the probability mass
01:22 - is here and here, because it's a mixture of two Gaussians.
01:25 - Then you can see that the probability mass spreads out
01:28 - as you add more and more Gaussian noise to the data.
01:32 - And now you might wonder well, what
01:34 - happens if we were to consider multiple noise levels that
01:38 - are kind of in between?
01:40 - What happens if we add a noise level that
01:44 - is in between 0 and sigma 1?
01:46 - Then you're going to get a density that
01:47 - is kind of in between these 2.
01:50 - And in the limit, you can think about what
01:52 - happens if you were to consider an infinite number of noise
01:56 - levels going from 0 to whatever was the maximum amount.
02:00 - And what you're going to get is an infinite number
02:04 - of data densities perturbed with increasingly large amounts
02:09 - of noise that are now indexed by t, where
02:12 - t is a kind of random variable continuous variable
02:17 - from 0 to the kind of maximum that you have
02:21 - on the right on the other end.
02:23 - So each slice here, each vertical slice
02:27 - is basically the density of data convolved
02:33 - where we've added noise corresponding
02:37 - to this continuous index t.
02:40 - Right?
02:41 - And so you can see how I got here.
02:44 - We started out with a finite number of noise levels,
02:48 - where all the probability mass was here.
02:50 - And then it spreads out.
02:52 - And then you can think about a finer kind
02:55 - of interpolation and finer, and finer,
02:57 - until you have something continuous.
03:00 - And so you go from pure data at time 0, here on the left,
03:04 - to pure noise on the other extreme, where
03:10 - kind of corresponding to the maximum amount of noise
03:13 - that you're adding to the data.
03:16 - So now instead of having 1,000 different versions of the data
03:21 - perturbed with increasingly large amounts of noise,
03:23 - you have an infinite number of data densities
03:28 - that have been perturbed with increasingly
03:31 - large amounts of noise.
03:32 -
03:35 - And so you can think of what we were doing before as kind
03:38 - of selecting 1,000 different slices here and modeling
03:43 - those data distributions.
03:45 - But really there is an infinite number of them.
03:49 - And that perspective is actually quite useful as we'll see.
03:54 - And so you have this kind of sequence of distributions.
04:02 - p0 is just the clean data.
04:04 - And pt, on the other extreme, is what
04:07 - you got if you add the maximum amount of noise, which you can
04:10 - think of as some kind of noise distribution
04:12 - where the structure in the data is completely lost.
04:15 - And that corresponds to basically pure noise.
04:19 - So as you go from left to right, you
04:22 - are increasing the amount of noise that you add to the data
04:25 - as you go from pure data to pure noise at the other extreme.
04:28 -
04:31 - And now you can imagine what happens
04:35 - if you perturb data with increasingly large amounts
04:39 - of noise.
04:40 - What happens is that you start with points
04:44 - that are distributed according to p0 that are distributed
04:46 - according to the data density.
04:48 - For example, you start with these 4 images.
04:50 - And then as you go from left to right,
04:53 - you are increasingly adding noise to these data samples
04:58 - until you are left with pure noise.
05:00 -
05:03 - And so you can think of having a collection of random variables
05:09 - xT, 1 for each time step, which is basically
05:14 - describing what happens as you go from
05:16 - left to right, as you go from pure data to pure noise.
05:20 - And all these random variables, which
05:22 - you can think of as a stochastic process,
05:24 - is just a collection of an infinite number
05:26 - of random variables, they all have densities,
05:28 - pt's which are just these data densities,
05:32 - plus noise, that we've been talking about for a while.
05:36 - And we can describe the evolution of this--
05:39 - Or how these random variables change over time,
05:43 - if you think of this axis as a time slide--
05:46 - dimension, then all these random variables
05:49 - are related to each other.
05:50 - And we can describe their relationship
05:52 - using something called a stochastic differential
05:54 - equation.
05:56 - It's not super important what it is,
05:57 - but it's basically a simple formula
05:59 - that relates the values of these random variables take.
06:04 - And it's similar to an ordinary differential equation, which
06:08 - would just be some kind of deterministic evolution,
06:11 - where the difference is that we add
06:13 - basically noise at every step.
06:16 - So you can imagine particles that
06:18 - evolve from left to right following
06:21 - some kind of deterministic dynamics,
06:26 - where we add a little bit of noise at every step.
06:30 - And in particular, it turns out that if all you want to do
06:33 - is to go from data to pure noise, all you have to do
06:38 - is you have to--
06:39 - You can describe this process very
06:40 - simply with a stochastic differential equation, where
06:43 - you're basically just--
06:45 - The way xt changes infinitesimally
06:49 - is by adding a little bit of noise at every step.
06:52 - You can think of this as some kind of random walk,
06:54 - where every step you add a little bit of noise.
06:57 - And if you keep running this random walk
06:59 - for a sufficiently large amount of time,
07:01 - you end up with a pure noise kind of distribution.
07:07 - And not super important, but what's interesting
07:10 - is that we can start thinking about what happens if we reverse
07:14 - the direction of time.
07:16 - Now we have this kind of stochastic process
07:17 - that evolves over time going from left to right here,
07:21 - you go from data to noise.
07:23 - Now you can start thinking about what
07:24 - happens if you were to reverse the direction of time
07:27 - and you go from capital T to 0.
07:30 - And so you go from pure noise to data.
07:34 -
07:38 - That's what we want to do if we want to generate samples.
07:40 - If you want to generate samples, you
07:42 - want to basically invert this process.
07:44 - You want to change the direction of time.
07:48 - And it turns out that there is a simple stochastic differential
07:53 - equation that describes the process in reverse time.
07:59 - And the interesting thing is that you
08:02 - can describe this process with a stochastic differential
08:06 - equation, which is relatively simple.
08:08 - And really, the only thing that you
08:09 - need is the score functions of these noise
08:14 - perturbed data densities.
08:17 - So if you somehow have access to the score functions
08:21 - of this densities pt, corresponding to data
08:23 - plus noise, corresponding to at, then there
08:27 - is a simple stochastic differential equation
08:29 - that you can use to describe this process of going
08:32 - from noise to data.
08:33 -
08:36 - And so if you somehow knew this score function, which
08:41 - to some extent we can approximate with a score based
08:46 - model, we can build a generative model out
08:50 - of this interpretation.
08:52 - And so the idea is that we're going to train a neural network,
08:56 - just like before, to estimate all these score models
09:03 - or these ground truth scores.
09:05 - These are scores of data plus noise,
09:07 - where there is an infinite number of noise levels now.
09:09 - So this is exactly what we're doing before,
09:12 - except that now t doesn't take 1,000 different possible values.
09:17 - t can take an infinite number of different values,
09:19 - but it's exactly the same thing.
09:21 - Before we were just estimating the score,
09:23 - so pt for 1,000 different chosen noise levels.
09:29 - Now we do it for an infinite number of them.
09:33 - And we do that by doing the usual mixture
09:37 - of de-noising score-matching objectives.
09:40 - So we want to be able to train a single neural network as theta
09:44 - that jointly estimates all these infinite number of scores.
09:49 - And so it's exactly what we had before,
09:51 - except that instead of being a sum over 1,000 different noise
09:54 - levels, now it's kind of an integral over an infinite number
09:59 - of different t's, all the time steps that we have in that plot.
10:06 - And if you can somehow train this model well,
10:11 - you can derive this loss to a small number, which
10:16 - means that this score model approximates
10:19 - the true score accurately.
10:23 - Then you can basically plug in your score model
10:28 - in that reverse time stochastic differential equation.
10:32 - So recall we had this SDE here, such
10:36 - that if you knew this score, then you could just
10:38 - solve this stochastic differential equation
10:40 - and you would go from noise to data.
10:42 - You take this exact equation.
10:44 - You replace the true score with the estimated score
10:48 - and you get that.
10:50 - And the advantage of this is that now this
10:52 - is a well-studied problem, you just
10:54 - have a stochastic differential equation,
10:56 - you just need to solve it.
10:57 - And there is a lot of numerical methods
10:59 - that you can use to solve a stochastic differential
11:02 - equation.
11:02 - The simplest one is basically something similar to Euler
11:05 - method for ODEs, where you basically just discretize time
11:10 - and you just step through this equation.
11:13 - So this is a continuous time kind of evolution.
11:17 - You can just take increments of delta t.
11:21 - And you just basically discretize this.
11:24 - So I guess delta t here is negative, so you decrease time.
11:29 - And then you take a step here following
11:32 - the deterministic part, which is given by the score.
11:35 - And then you add a little bit of noise at every step.
11:39 - So you see how this is basically the same as Langevin dynamics.
11:42 - It's always some kind of follow the gradient
11:45 - and add a little bit of noise at every step.
11:49 - And so you can interpret that as being just a discretization
11:52 - of this stochastic differential equation that tells you
11:55 - exactly what you should do if you
11:58 - want to go from noise to data.
12:01 - So there is a continuous number of them.
12:04 - So t is a uniform continuous random variable between 0 and t,
12:08 - so there is an infinite number of--
12:10 - Let me see if I have it here.
12:12 - So there is an infinite number of noise levels.
12:18 - And what we do is we're basically numerically
12:21 - integrating these stochastic differential equation that
12:24 - goes from noise to data.
12:25 - So you start out here by basically sampling
12:29 - from a pure noise distribution.
12:31 - So then you take small steps.
12:34 - You still need to--
12:35 - But you have a freedom to choose.
12:37 - At that point, you don't necessarily have to take always
12:40 - 1,000th of the length, you can apply whatever.
12:44 - And there are many more advanced kind
12:46 - of numerical schemes for solving stochastic differential
12:49 - equations.
12:50 - The moment you managed to formulate
12:52 - the problem of sampling to solving
12:54 - a stochastic differential equation,
12:55 - then you can use a lot of advanced numerical methods
12:58 - for solving stochastic differential equations.
13:01 -
13:05 - And so you step through time and you discretize.
13:08 - You try to find a solution to this trajectory that goes
13:11 - from noise to data basically.
13:16 - And that's sort of the main idea of this core base diffusion
13:23 - models.
13:24 - And there is actually a connection
13:26 - with what we were seeing before we were doing Langevin dynamics,
13:31 - how is that related to this numerical SDE solver.
13:35 - You can think of it as kind of the numerical SDE solver
13:40 - will take a step.
13:42 - It's kind of trying to approximate the true solution
13:45 - of the SDE, which is kind of this red trajectory
13:49 - that I'm showing here.
13:50 - You can use a numerical SDE solver,
13:53 - and you can help it basically at every step
13:56 - by doing Langevin dynamics.
13:58 - So Langevin dynamics is kind of a procedure that would allow you
14:02 - to sample from this slice.
14:06 - And so what you can do is you can combine
14:09 - or you can either just use character steps, in which case
14:15 - basically you get the procedure that I talked about before,
14:17 - where you do annealed Langevin dynamics.
14:19 - You just follow Langevin for a little bit,
14:21 - and then you follow the Langevin corresponding to the next slice.
14:25 - And you follow the Langevin corresponding
14:27 - to the next slide and so forth.
14:28 - Or you can apply these numerical methods
14:30 - to try to jump across time.
14:34 - And you can kind of combine the two of them
14:36 - to eventually end up with something
14:38 - that can generate samples.
14:41 - So you can think of it as--
14:43 - Once you view it from this perspective,
14:45 - there is, again, many different ways
14:47 - of solving the SDE, including using Langevin dynamics to kind
14:51 - of sample from these intermediate densities
14:56 - that you get as you change the time dimension.
14:59 -
15:02 - And yeah, this is the kind of thing
15:05 - that really works extremely well.
15:08 - This was this kind of model again.
15:11 - I guess we haven't talked exactly about the metrics,
15:14 - but it achieves state of the art results
15:17 - on a variety of image benchmarks.
15:20 - And you can see some of the high resolution images that
15:24 - can be generated by this model.
15:28 - These are fake samples.
15:30 - These people don't exist, but you
15:32 - can see that the model is able to generate very high quality
15:36 - data by basically solving this stochastic differential equation
15:41 - and mapping pure noise to images that have the right structure.
15:47 - They're almost indistinguishable from real samples.
15:51 - Oh, yeah, fingers are a hard one to do.
15:54 - It's a hard problem for these models
15:56 - to learn how to make hands.
15:59 - I guess in this data sets it's typically just the face,
16:01 - so you don't have to worry about it.
16:05 - I think people have made progress on that as well
16:08 - with more training data.
16:09 - I think people have been able to do specialized--
16:11 - You show a lot of hands in the training set,
16:14 - the model kind of learns how to do that.
16:16 - How do you prevent overfitting?
16:18 - You can look at it from the perspective of the loss,
16:20 - like if you believe the square matching
16:22 - loss, like you can kind of see how well it generalizes
16:25 - to validation or test kind of data.
16:28 - We also did extensive tests on trying to find the nearest
16:31 - neighbor in the data set.
16:33 - And we're pretty confident that it's often
16:37 - able to generate new images that you haven't seen before.
16:42 - There are certainly cases, especially text to image
16:46 - diffusion models can actually memorize, which might be OK.
16:51 - I mean, I don't think it's necessarily wrong behavior
16:54 - to memorize some of the data points.
16:58 - But yeah, people have been able to craft captions,
17:01 - such that if you ask the model to generate
17:04 - you an image with that caption, it produces exactly an image
17:08 - from the training set.
17:09 - So memorization does happen, but not to the extent
17:13 - that it's only replicating images in the training set.
17:15 - It's certainly able to generate new images,
17:19 - including composing things in interesting ways that cannot
17:23 - possibly have been seen, I think, even on the internet.
17:26 - So it's certainly showing some generalization capabilities.
17:30 - And I think looking at the loss is a pretty good way
17:32 - to kind of see that indeed it's not overfitting.
17:35 - Like the score matching loss that you
17:36 - see in a training set is pretty close to the one
17:38 - you see on the validation unseen data.
17:41 - So it's not overfitting at least.
17:45 - We're not yet at that level.
17:48 - It's a mix.
17:48 - I think it goes back to what we were saying before,
17:51 - like the models are trained by score matching,
17:54 - so it's a much more stable kind of training objective.
17:57 - From the perspective of the computation graph,
17:59 - like if you think about what happens
18:01 - if you solve an SDE, that's kind of an infinitely
18:05 - deep-computation graph.
18:07 - Because at this point like you have a--
18:09 - I mean, you are discretizing it, of course,
18:11 - but, in principle, you can make it as deep as you want,
18:14 - because you're choosing increasingly small kind of time
18:17 - steps.
18:18 - This can become a very deep kind of computation graph
18:21 - that you can use at inference time.
18:23 - So again, that's kind of the key advantage
18:26 - that you can have a very deep--
18:27 - You can use a lot of computation at inference time
18:29 - to generate images without having
18:31 - to pay a huge price at training time
18:34 - because the models are trained through this score-matching kind
18:37 - of working at the level of small changes, kind of figuring out
18:43 - how to improve an image by a little bit.
18:46 - And then you stack all these little improvements
18:49 - and you get a very deep kind of computation graph that
18:52 - can generate very high quality data sets.
18:55 - Yeah.
18:55 - So latent variables, I guess--
18:57 - I don't have time to talk about it today, unfortunately,
18:59 - but there is a way to think of it from the perspective of--
19:04 - It turns out that there is a way to convert this model
19:07 - into a normalizing flow, at which point
19:10 - you would have latent variables.
19:12 - And the machinery looks something like this.
19:15 - We have this stochastic differential equation
19:17 - that goes from data to noise.
19:20 - It turns out that it's possible to describe a stochastic process
19:24 - that has exactly the same marginals,
19:27 - but it's purely deterministic.
19:29 - So there is an ordinary differential equation,
19:33 - the kind of things you probably have
19:35 - seen in other classes, that has exactly
19:38 - the same marginals over time.
19:40 - And again, this ordinary differential equation
19:43 - depends on the score.
19:45 - So if you are able to estimate the score,
19:49 - you can actually generate samples.
19:51 - You can go from noise to data by solving an ordinary differential
19:56 - equation, instead of solving a stochastic differential
19:59 - equation.
20:00 - And at that point, because you can
20:04 - think of the ordinary differential equation
20:07 - as basically defining a normalizing flow,
20:11 - because the mapping from the initial condition
20:14 - to the final condition of the ordinary differential equation
20:19 - is invertible.
20:21 - So you can go from left to right along these white trajectories
20:26 - or from left to right, or right to left.
20:30 - And that's an invertible mapping.
20:33 - So essentially, this machinery defines a continuous time
20:39 - normalizing flow, where the invertible mapping
20:41 - is given by solving an ordinary differential equation.
20:44 - Like these white trajectories that are the solutions of that
20:49 - same ordinary differential equation with different initial
20:52 - conditions, they cannot cross, because that's how ordinary
20:56 - differential equation.
20:57 - So the paths corresponding to different initial conditions,
21:05 - they can never cross, which basically
21:07 - means that the mapping is invertible,
21:10 - which basically means that this is a normalizing flow.
21:14 - And so that, I guess I don't have time
21:17 - to talk about it, unfortunately, but if you're
21:20 - willing to take the normalizing flow perspective,
21:22 - then you can go from data to noise.
21:26 - And the noise is kind of a latent vector
21:29 - that is encoding the data.
21:32 - And the latent vector has the same dimension,
21:34 - because here it's clean image, image plus noise
21:39 - has the same dimension.
21:40 - It's really just a normalizing flow.
21:41 - The latent variables indeed have a simple distribution,
21:44 - because it's pure noise.
21:46 - And it's just like--
21:47 - The mapping from noise to data it's
21:49 - given by a solving an ordinary differential equation, which
21:53 - is defined by the score model.
21:55 - So it's a flow model that is not being
21:57 - trained by maximum likelihood.
21:59 - It's trained by score matching.
22:02 - You can think of it as a flow with an infinite depth.
22:04 -
22:06 - That's another way to think about it, which means that you
22:10 - can also get likelihoods.
22:11 - That's the other interesting bit that you can get
22:15 - if you take that perspective.