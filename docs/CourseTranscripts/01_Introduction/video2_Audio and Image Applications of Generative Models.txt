00:00 -
00:05 - SPEAKER: And to give you a few examples,
00:07 - if you have a generative model of images,
00:11 - you might be able to control it, let's say, using sketches.
00:15 - Maybe you're not good at painting
00:17 - and you can only produce a rough sketch of a bedroom.
00:21 - And then you fit it as a control signal
00:23 - into your generative model.
00:24 - And you can use it to produce realistic images that
00:28 - have the structure of the stroke painting that you provide,
00:31 - but they look much better.
00:34 - Or you can do maybe text-to-image kind
00:37 - of things, where if you have a generative model that
00:40 - has been trained on paintings, then
00:42 - you can control it through captions.
00:44 - And you can ask the model to generate a new painting that
00:48 - corresponds to the description that is provided by the user.
00:51 -
00:54 - Other examples that you might not think about immediately
00:57 - could be something like you have a generative model
00:59 - over medical images.
01:01 - And in this case, you might use an actual signal
01:05 - coming from an MRI machine or a CT scan machine.
01:08 - And you can use that signal to sort of reconstruct
01:11 - the medical image, the thing you actually care about,
01:14 - given this kind of measurement that is
01:17 - coming from an actual machine.
01:21 - And in this kind of application, generative models
01:24 - have shown to be very effective because they
01:26 - can reduce the number of measurements,
01:31 - the amount of radiation that you have to give to the patient
01:34 - to get a measurement that is good enough
01:36 - to produce the medical images that the doctor needs to come up
01:39 - with a diagnosis.
01:41 -
01:43 - An example of the kind of thing you
01:45 - can do if you can evaluate probabilities
01:47 - is to do outlier detection.
01:49 - We are going to be playing with this
01:51 - in the homework, a variant of this.
01:54 - If you have a generative model that understands traffic signs,
01:58 - you might be able to say, OK, this looks
02:00 - like a reasonable traffic sign you
02:02 - might encounter on the streets.
02:04 - What if I feed you something like this, some kind
02:07 - of adversarial example?
02:08 - Somebody is trying to cause trouble
02:12 - to your self-driving vehicle.
02:14 - The model might be able to say, no,
02:15 - this looks like a low-probability thing.
02:17 - This is weird.
02:18 - Do something about it.
02:20 - Maybe don't trust it.
02:21 - Ask a human for help or something like that.
02:26 - And this is really an exciting time
02:29 - to study generative models because there's
02:31 - been a lot of progress over many different modalities.
02:36 - I'm going to start with images because that's where
02:39 - I've done a lot of my research.
02:42 - When I started working in this space about 10 years ago,
02:47 - these were the sort of images that we were able to generate.
02:50 - And even that was already very, very remarkable.
02:53 - People were very surprised that it
02:55 - was possible to train a machine learning system
02:58 - to produce images of people that sort of are black and white.
03:04 - And they roughly had the right shape.
03:06 - People were very impressed by those sort of results.
03:09 - And you can see that over a few years,
03:11 - this progress was largely driven by generative adversarial
03:16 - networks, which is a class of generative models
03:18 - we're going to be talking about.
03:20 - You can kind of see how the generations are becoming
03:23 - better and better, higher resolution, more detail,
03:26 - more realistic kind of images of people.
03:28 -
03:31 - One of the big improvements that happened over the last two
03:34 - or three years, which was actually largely
03:36 - coming out of Stanford--
03:39 - Yang Song, who was a PhD student in my group,
03:43 - came up with this idea of using score-based diffusion models,
03:47 - which is a different kind of generative models that
03:50 - we're also going to be talking about in this--
03:52 - in this course.
03:53 - And was able to further push the state of the art,
03:56 - for example, generating images, very high-resolution images
03:59 - that look like this.
04:00 - These people don't exist.
04:02 - They are completely synthesized, generated by one
04:06 - of these generative models.
04:09 - And this is really--
04:11 - diffusion models are really the technology that drives a lot
04:17 - of the text-to-image systems that you might have seen,
04:21 - things like Stable Diffusion, or DALL-E, or other--
04:26 - or Midjourney, we think are all based
04:29 - on this type of generative model,
04:32 - this way of representing probability distribution based
04:35 - on a diffusion model.
04:37 - And once you have a good diffusion model,
04:40 - you can try to control it using captions.
04:43 - And now you get this kind of really cool
04:46 - text-to-image systems, where you can ask a user for an input.
04:53 - What kind of image do you want?
04:54 - A caption of what kind of image the system
04:57 - should be able to produce.
04:58 - For example, an astronaut riding a horse.
05:01 - And these are the kind of results
05:04 - that you can get with these systems we have today.
05:08 - This is really cool.
05:12 - I mean, these models have been trained on a lot of data,
05:15 - but presumably, they have not seen something
05:18 - like this on the internet.
05:20 - They might have seen an astronaut.
05:21 - They definitely have seen a horse.
05:23 - But they probably have not seen those two things together.
05:26 - So it's very impressive that the model
05:28 - is able to sort of, again, understand
05:31 - the meaning of astronaut, understand the meaning of horse,
05:34 - putting them together.
05:35 - And the fact that it's able to generate this kind of picture
05:38 - tells me that there is some level of understanding
05:41 - of what it means--
05:42 - what an astronaut means and what riding means,
05:45 - what a horse means.
05:46 - And even if you look at the landscape, I don't know,
05:50 - it could be--
05:50 - it feels like it's probably on some other planet or something.
05:54 - So there is some level of understanding
05:56 - about these concepts that is showing here.
05:59 - And that's super exciting, I think,
06:03 - because it means that we're really
06:04 - making progress in this space and understanding
06:07 - the meaning of text, of images, their relationship,
06:09 - and that's what's driving a lot of the successes
06:12 - that we're seeing in ML these days.
06:16 - Here is another example.
06:18 - If you ask a system about an a perfect Italian meal, you get--
06:22 - here I'm generating multiple samples.
06:25 - So because it's a probability distribution,
06:28 - you can imagine you can sample from it
06:30 - and it will generate different answers.
06:32 - So the generation is stochastic.
06:34 - Different random seed, it will produce different outputs
06:37 - every time.
06:37 - And here we can see four of them.
06:39 - Again, I think it does a pretty good job.
06:41 - I mean, some of the stuff is clearly made up, but it does--
06:45 - it's interesting how it kind of even captures out of the window,
06:48 - the kind of--
06:50 -
06:53 - the kind of buildings you would probably see in Italy.
06:56 - And it kind of has the right flavor, I think.
06:59 - It's pretty impressive kind of thing.
07:03 - Here's another example from a recent system developed
07:07 - in China.
07:08 - This is a teddy bear wearing a costume,
07:11 - is standing in front of the Hall of Supreme Harmony and singing
07:14 - Beijing opera.
07:15 - So again, a pretty crazy sort of caption.
07:18 - And it produces things like this.
07:23 - Pretty impressive.
07:25 -
07:29 - And this is the latest that came out very recently.
07:33 - We don't know yet what this model is built on.
07:37 - DALL-E 3 from OpenAI, this is an example from their blog post.
07:43 - You're asking the model to generate--
07:45 -
07:48 - you can see the caption yourself.
07:52 - Pretty cool.
07:54 - Again, demonstrates a pretty sophisticated level
07:57 - of understanding of concepts.
07:58 - And a good way of combining them together.
08:00 - Right.
08:01 -
08:04 - So this is a text-to-image generation.
08:07 - Again, the nice thing about these models
08:09 - is that you can often control them using different kinds
08:12 - of control signals.
08:13 - So here, we're controlling using text, using captions,
08:17 - but there is a lot of inverse problems.
08:20 - Again, this is a field that I've been-- that has
08:22 - been studied for a long time.
08:24 - People have been thinking about how to colorize an image, how
08:27 - to do super-resolution on an image,
08:29 - how to do inpainting on an image.
08:33 - These problems become pretty much easier
08:35 - to solve once you have a good system that really understands
08:38 - the relationship between all the pixel values
08:42 - that you typically see in an image.
08:44 - And so there's been a lot of progress in,
08:48 - let's say, super-resolution.
08:50 - You go from low-resolution images like this
08:52 - to high-resolution images like that.
08:54 - Or colorization, you can take old black and white photos
08:57 - and you can kind of colorize them in a meaningful way.
09:00 - Or inpainting, so if you have an image where some of the pixels
09:04 - are masked out, you can ask the model to fill them in.
09:09 - And they do a pretty good job at doing these.
09:12 - These are probably not the most up-to-date references,
09:15 - but you can get a sense of why these models are
09:19 - so useful in the real world.
09:23 - And here is an example from SDEdit,
09:25 - which is one of the things that, again, one of my PhD students
09:29 - developed.
09:30 - This is back to the sketch-to-image, where
09:33 - you can start with a sketch of sort of a painting or an image
09:36 - that you would like.
09:37 - The kind of thing I would be able to do.
09:39 - And then you can ask the model to refine it and produce
09:42 - some pretty picture that kind of has the right structure
09:46 - but it's much nicer.
09:48 - I would never be able to produce the image at the bottom,
09:51 - but I can probably come up with the sketch you see on the top.
09:56 - And yeah, here you can see more examples
09:58 - where you can do sketch-to-image or you can
10:01 - do even stroke-based editing.
10:04 - Maybe you start with an image and then
10:06 - you add some-- you want to change it
10:09 - based on some rough sense of what you want the image to have.
10:12 - And then the model will make it pretty for you.
10:17 - And it doesn't have to be editing
10:20 - or sort of like you don't have to control it through strokes.
10:24 - Another natural way of controlling this kind of editing
10:27 - process is through text.
10:30 - So instead of actually drawing what you want,
10:33 - you can ask the model-- you can tell the model how you
10:36 - want your images to be edited.
10:37 - So you might start with an image of a bird,
10:41 - but now you want to change it so that you
10:43 - want it to spread the wings.
10:44 - And you can tell the model, "Now spread the wings."
10:46 - And it's able to do these kind of updates.
10:49 - Or you have an image with two birds.
10:51 - And now you want the birds to be kissing.
10:53 - And then this is what you produce.
10:55 - Or you have an image with a box and you want the box to be open.
10:59 - And you can kind of see some pretty impressive results
11:03 - in terms of--
11:04 - in terms of image editing or changing the pose of this dog
11:07 - or even changing the style of the painting, of the image.
11:11 - You go from a real image to some kind of drawings.
11:15 - And again, that's a pretty good job.
11:18 - You can see it's making some mistakes.
11:20 - Like this knife here, it gets changed
11:23 - in a way that is not quite what we want.
11:26 - They are not perfect yet, but these capabilities
11:29 - are very impressive.
11:30 - They're already very useful.
11:31 -
11:34 - Cool.
11:35 - And yeah, back to the more exotic one
11:38 - that you might not necessarily think fits in this framework.
11:42 - Just to give you a sense of how general these ideas are.
11:45 - If you have a generative model of medical images,
11:48 - you can use it to essentially improve
11:51 - the way we do medical images.
11:56 - In this case, the control signal,
11:57 - it's an actual measurement that you get from, let's say,
12:00 - a CT scan machine.
12:02 - And then you can control the generative process
12:05 - using the measurement from the CT scan machine.
12:08 - And this can drastically reduce the amount of radiations then--
12:12 - say, the number of measurements that you
12:14 - need to get a crisp kind of image
12:16 - that you can show to the doctor.
12:18 - This is very similar to inpainting.
12:20 - It's just inpainting in a slightly different space.
12:23 - But you can get a sense.
12:24 - It's roughly the same problem.
12:26 - And advances in generative models
12:28 - translate into big improvements in these--
12:31 - in these real-world applications.
12:33 -
12:36 - All right, now moving on to different modalities,
12:39 - speech audio has been another modality
12:43 - where people have been able to build some pretty
12:45 - good generative models.
12:47 - This is one of the earliest one, the WaveNet model, back--
12:53 - I think it was 2016.
12:54 - And you can kind of see some examples of-- let's hope
12:58 - this works.
12:59 - So this is an example.
13:00 -
13:06 - This is kind of like the pre-deep learning thing.
13:09 - And they are not-- these are not great text-to-speech--
13:11 - [AUDIO PLAYBACK]
13:12 - - The Blue Lagoon is a 1980 American romance and adventure
13:14 - film directed by Randal Kleiser.
13:16 - [END PLAYBACK]
13:17 - SPEAKER: And then the WaveNet model,
13:18 - which is a deep learning-based model for text-to-speech.
13:21 - You're going to see it's significantly better.
13:23 - [AUDIO PLAYBACK]
13:24 - - The Blue Lagoon is a 1980 American romance and adventure
13:27 - film directed by Randal Kleiser.
13:29 - [END PLAYBACK]
13:30 - SPEAKER: And these are maybe the latest ones
13:33 - that are based on diffusion models again.
13:36 - So this is-- well, this is a combination of diffusion models
13:38 - and autoregressive models.
13:40 - But here, you can see some of the 2023 stuff.
13:43 - [AUDIO PLAYBACK]
13:44 - - Once you have the first token, you want to predict the second
13:47 - token, given the input, and the first token using multi-head
13:51 - attention.
13:51 - [END PLAYBACK]
13:52 - SPEAKER: So you can see it's much more realistic.
13:55 - There is a little bit of an accent here.
13:58 - There is a little bit of emotions that are--
14:02 - it feels a lot less robotic, a lot less fake.
14:05 - Here's another example.
14:07 - This is just text-to-speech.
14:09 - You input a text and you produce the speech corresponding
14:12 - to that text.
14:13 - [AUDIO PLAYBACK]
14:13 - - CS236 is the best class at Stanford.
14:16 - [END PLAYBACK]
14:17 - [LAUGHTER]
14:17 - SPEAKER: So this is another example.
14:19 -
14:21 - And again, you can sort of use these things to do--
14:24 - to solve inverse problems.
14:26 - So you can do super-resolution in the audio space.
14:29 - So you can condition on the--
14:31 - kind of like a low-quality signal, the kind of thing
14:33 - you can get maybe on phones.
14:36 - [AUDIO PLAYBACK]
14:37 - - One is investment.
14:39 - One is reform.
14:39 - [END PLAYBACK]
14:40 - SPEAKER: And then you can super-resolve it.
14:42 - [AUDIO PLAYBACK]
14:43 - - One is investment.
14:44 - One is reform.
14:45 - [END PLAYBACK]
14:46 - SPEAKER: And again, this is the same problem
14:47 - with basically inpainting here.
14:48 - Like you're missing some pixels.
14:50 - You're missing some frequencies.
14:51 - And you can ask the model to make them up for you.
14:54 - And to the extent that it understands the relationship
14:56 - between these values.
14:59 - You can also kind of think of as images.
15:01 - It can do a pretty good job at super-resolving audio.