
00:05 - SPEAKER: Welcome.
00:07 - I'm super excited to see so many people
00:09 - interested in deep generative models.
00:13 - So I'm Stefano.
00:15 - I'm the instructor of this class.
00:17 - I've been teaching this course for a few years now.
00:20 - I guess we started back when before all the generative AI
00:25 - hype and before this topic was so popular in the industry.
00:29 - And so now you're lucky.
00:32 - You get to experience a pretty mature version of this course.
00:36 - And it's going to be a pretty exciting quarter.
00:42 - This is one of the hottest topics in the industry
00:46 - right now.
00:47 - There is, of course a lot of excitement
00:48 - around the language models, about generative models
00:51 - of images, of videos.
00:53 - And the goal of this class is to give you really the foundations
00:57 - to understand how the methods that
01:00 - are used in industry and in academic papers actually work.
01:05 - And hopefully get up to speed with all
01:09 - the really fundamental concepts that you need in order
01:12 - to build a generative model and maybe in the future
01:16 - develop better systems, develop better models,
01:19 - deploy them in industry, start your own company that
01:22 - is sort of leveraging these technologies.
01:25 - So at a high level, one of the reasons
01:31 - I think these models are becoming so important in AI
01:35 - and machine learning is that they really address
01:39 - kind of the fundamental challenge
01:41 - that we encounter in a lot of sub-fields of AI,
01:44 - like computer vision, NLP, computational speech,
01:49 - even robotics, and so forth.
01:52 - If you think about it in a lot of these settings,
01:55 - the fundamental challenge that you have
01:57 - is to make sense of some complex high-dimensional signal
02:02 - or object like an image or a speech signal
02:06 - or a sequence of tokens or a sequence of characters
02:10 - written in some language.
02:12 - And this is challenging because from the perspective
02:16 - of a computer, if you think about an image,
02:17 - it's just like a big matrix of numbers.
02:20 - And the difficulty is making sense of it,
02:23 - trying to figure out how to map that very
02:25 - complex high dimensional object to some kind of representation
02:31 - that is useful for decision making, for a variety of tasks
02:35 - that we care about, like figuring out what kind
02:38 - of objects are in the image or what kind of relationships
02:41 - they are in, what kind of materials they are made of,
02:43 - if they are moving, how fast, things like that.
02:47 - And similarly if you think about NLP, it's a similar story.
02:50 - You have a sequence of characters
02:53 - and you need to make sense of it.
02:54 - You need to understand what's the meaning
02:56 - and maybe you want to translate it in a different language.
02:59 - The challenge is really understanding
03:01 - what these complex objects really mean.
03:05 - And understanding these objects is hard.
03:08 - It's not even clear what it means to understand
03:11 - what an image means.
03:13 - But I like to use this analogy inspired
03:16 - by this quote from Richard Feynman.
03:19 - At some point he said, "What I cannot create,
03:22 - I do not understand."
03:24 - I think this was actually what they found on his whiteboard
03:28 - after he passed.
03:29 - And what he meant in this case is
03:33 - that he was talking about mathematical theorems
03:35 - and he was saying if I can't really derive a proof by myself,
03:39 - I'm not really understanding the concept well enough.
03:43 - But I think the analogy is that we can look
03:46 - at the contrapositive of this.
03:48 - And the philosophy behind generative modeling approaches
03:53 - in AI is that if I claim I'm able to understand
03:57 - what an image means or what a piece of text means,
04:01 - then I should be able to create it, right?
04:03 - I should be able to generate new images.
04:05 - I should be able to generate new text.
04:07 - So if you claim you understand what an apple is,
04:10 - then you should be able to picture one in your head.
04:14 - Maybe you're not able to create a photo of an apple,
04:16 - but you know sort of what it means,
04:18 - or if you claim you can speak Italian,
04:21 - then you should be able to produce--
04:24 - you should be able to speak in that language,
04:26 - you should be able to write text in that language.
04:29 - And that's kind of the philosophy
04:31 - behind this idea of building generative models of images
04:35 - or generative models of text or multimodal, generative models.
04:38 - If you have these kind of capabilities,
04:40 - so you're able to generate text that is coherent
04:43 - and make sense in large language models,
04:46 - like ChatGPT, those kind of things,
04:48 - then it probably means that you have a certain level
04:50 - of understanding, not only of the rules, the grammar
04:55 - of the language, but also about common sense,
04:59 - about what's going on in the world.
05:01 - And essentially, the only way you
05:03 - can do a good job at generating text that is meaningful
05:07 - is to have a certain level of understanding.
05:09 - And if you have that level of understanding,
05:11 - then you can leverage it and you can
05:12 - use it to solve all the tasks that we care about.
05:18 - So how do we go about building a software,
05:23 - writing code that can generate let's say images
05:27 - or can generate text?
05:29 - This is not necessarily a new problem.
05:33 - It's not something that we are looking at for the first time.
05:36 - People in computer graphics, for example,
05:38 - have been thinking about writing code that can generate
05:42 - images for a very long time.
05:44 - And they made a lot of progress in this space.
05:47 - And so you can think of the setting as something
05:52 - like where you're given a high level description of a scene.
05:54 - Maybe there are different kinds of objects,
05:56 - of different colors, different shapes.
05:59 - Maybe you have a viewpoint.
06:01 - And the goal is to kind of write a renderer that
06:05 - can produce an image that corresponds
06:09 - to that high-level description.
06:12 - And again, the idea is that if you can do this,
06:16 - then you probably have a reasonable understanding
06:18 - of what it means, that what the concept of a cube
06:23 - is, what the concept of a cylinder is, what colors mean,
06:27 - the relative position.
06:29 - And in fact, if you can do this well,
06:31 - then you can imagine a procedure where
06:34 - you try to invert this process.
06:37 - And given an image, you can try to figure out
06:40 - what was the high level description that
06:42 - produced this scene?
06:45 - And to the extent that you don't have
06:48 - sort of computational constraints
06:49 - and you can do this efficiently, this
06:52 - gives you a way to think about computer vision
06:55 - in terms of inverse graphics.
06:57 - So if you have a process that can generate images well
07:01 - and you are somehow able to invert it,
07:03 - then you are making progress towards computer vision tasks
07:06 - because you are able to really understand
07:09 - these high-level descriptions of the scenes.
07:13 - And this is not going to be a course on computer graphics.
07:18 - We're going to be looking at very different kinds of models.
07:21 - But they will have a similar structure.
07:23 - Many of them will have a similar structure
07:25 - where there's going to be a generative component.
07:27 - And then often, there's going to be latent variables
07:30 - that you can kind of infer given the raw sensory inputs
07:34 - in this case.
07:35 - And you can use that to get features,
07:38 - to get representations.
07:40 - You can use them to fine-tune your models
07:43 - to solve computer vision tasks.
07:46 - And so this kind of philosophy and this kind of structure
07:50 - will actually show up in the kind of models
07:52 - that we'll build in the class.
07:55 - So the kind of models we're going to work on,
07:58 - they are not graphics-based.
08:00 - They're going to be statistical models.
08:02 - So we're only going to be talking
08:04 - about models that are based on machine learning techniques.
08:08 - And so the generative models that we're going to work with
08:11 - are going to be based on a combination of data
08:14 - and prior knowledge.
08:16 - And so priors are always necessary.
08:20 - But you can imagine that there is a spectrum.
08:23 - You can rely more on data or you can rely more on priors.
08:27 - And you can kind of think of computer graphics
08:30 - as sort of lying on this extreme, where you leverage
08:34 - a lot of knowledge about physics, about light transport,
08:37 - about properties of objects to come up with good renderers.
08:42 - This course is going to be focusing
08:43 - on methods that are more like much more data
08:46 - driven where we're going to be trying
08:47 - to use as little prior knowledge as possible and instead leverage
08:52 - data.
08:54 - Large data sets of images or text,
08:57 - perhaps collected on the internet.
09:01 - And, yeah.
09:03 - So at a very high level, these generative models
09:07 - are just going to be probability distributions over let's say
09:11 - images x or over sequences of text x.
09:15 - And so in that sense, they are statistical.
09:19 - And we're going to be building these models using
09:22 - a combination of data, which you can
09:24 - think of as samples from this probability distribution.
09:27 - And in this case, the prior knowledge is basically
09:31 - going to be a mix of the kind of architectures
09:34 - you're going to be using, the kind
09:35 - of loss functions that you're going
09:37 - to be using for training the models, the kind of optimizer
09:39 - that you're going to be using to try to reduce the loss
09:43 - function as much as possible.
09:45 - And this combination, having access to good data
09:48 - and the right kind of priors is what
09:52 - enables you to build hopefully a good statistical generative
09:56 - model.
09:58 - But at the end of the day, kind of like the abstraction
10:02 - is that we're going to be working
10:03 - with probability distributions.
10:05 - And you can just think of it as a function that
10:07 - takes any input x as input, let's say an image
10:11 - and maps it to some kind of scalar probability value, which
10:14 - basically tells you how likely is this particular input image x
10:20 - according to my generative model.
10:23 - And this might not look like a generative model
10:27 - directly, like it looks like how do you actually generate data
10:31 - if you have access to this kind of object?
10:34 - The idea is that you can basically generate samples
10:37 - from this probability distribution
10:38 - to create new objects.
10:40 - So you train a model, you learn this probability distribution,
10:44 - and then you sample from it.
10:46 - And by doing that, you generate new images that
10:49 - hopefully look like the ones you've
10:51 - used for training the model.
10:55 - So that's the structure.
10:57 - So in some sense, what we're trying to do
10:59 - is we're trying to build data simulators.
11:01 - So we often think of data as an input
11:04 - to our machine learning problems.
11:07 - Here we're kind of changing--
11:09 - we're turning things around and we're thinking of data
11:12 - as being an output.
11:13 - So we need to think about different kinds of machine
11:16 - learning models that we can use to simulate to generate data.
11:20 - Of course, this looks a little bit weird
11:23 - because we just said we're going to use
11:24 - data to build these models.
11:26 - So indeed, the idea is that we're going
11:29 - to use data to build a model.
11:31 - But then we can use to generate new data.
11:33 - And this is useful because often, we're
11:37 - going to be interested in simulators
11:38 - that we can control through control signals.
11:43 - And we'll see some examples of the control
11:45 - signals you might want to use to control your generative process.
11:50 - For example, you might have a model that can generate images
11:54 - and you can control it by providing
11:56 - a caption of the kind of images you want,
11:59 - or you might have a model that can again generate images
12:02 - and you can control it by providing maybe
12:06 - black and white images and you can
12:08 - use it to produce a colorized version of the image.
12:11 - Or maybe you have a data simulator that can produce text
12:14 - in English, and you can control the generative process
12:17 - by feeding in text in a different language,
12:20 - maybe in Chinese.
12:21 - And that's how you build machine translation tools.
12:24 -
12:27 - The API is going to be, again, that
12:30 - of a probability distribution.
12:31 - So really you're going to be able to
12:33 - for a lot of these models, you're
12:35 - going to be able to also query the model with potential data
12:39 - points.
12:40 - And the model will be able to tell you whether or not
12:43 - they are likely to be generated by this data simulator or not.
12:46 - So in some sense, it also allows you
12:48 - to build a certain understanding over what kind of data
12:51 - points make sense and which ones don't, which
12:53 - is going to be useful for some applications.
12:57 - And really this data simulator is at the end
12:59 - of the day a statistical model.
13:01 - It's what we call the machine learning generative model.
13:04 - And in particular in this class, we're
13:06 - going to be thinking about deep generative models
13:08 - where we're going to be using neural networks, deep learning
13:11 - kind of ideas to implement this piece of code that gives you
13:16 - these capabilities of generating data.

00:00 -
00:05 - SPEAKER: And to give you a few examples,
00:07 - if you have a generative model of images,
00:11 - you might be able to control it, let's say, using sketches.
00:15 - Maybe you're not good at painting
00:17 - and you can only produce a rough sketch of a bedroom.
00:21 - And then you fit it as a control signal
00:23 - into your generative model.
00:24 - And you can use it to produce realistic images that
00:28 - have the structure of the stroke painting that you provide,
00:31 - but they look much better.
00:34 - Or you can do maybe text-to-image kind
00:37 - of things, where if you have a generative model that
00:40 - has been trained on paintings, then
00:42 - you can control it through captions.
00:44 - And you can ask the model to generate a new painting that
00:48 - corresponds to the description that is provided by the user.
00:51 -
00:54 - Other examples that you might not think about immediately
00:57 - could be something like you have a generative model
00:59 - over medical images.
01:01 - And in this case, you might use an actual signal
01:05 - coming from an MRI machine or a CT scan machine.
01:08 - And you can use that signal to sort of reconstruct
01:11 - the medical image, the thing you actually care about,
01:14 - given this kind of measurement that is
01:17 - coming from an actual machine.
01:21 - And in this kind of application, generative models
01:24 - have shown to be very effective because they
01:26 - can reduce the number of measurements,
01:31 - the amount of radiation that you have to give to the patient
01:34 - to get a measurement that is good enough
01:36 - to produce the medical images that the doctor needs to come up
01:39 - with a diagnosis.
01:41 -
01:43 - An example of the kind of thing you
01:45 - can do if you can evaluate probabilities
01:47 - is to do outlier detection.
01:49 - We are going to be playing with this
01:51 - in the homework, a variant of this.
01:54 - If you have a generative model that understands traffic signs,
01:58 - you might be able to say, OK, this looks
02:00 - like a reasonable traffic sign you
02:02 - might encounter on the streets.
02:04 - What if I feed you something like this, some kind
02:07 - of adversarial example?
02:08 - Somebody is trying to cause trouble
02:12 - to your self-driving vehicle.
02:14 - The model might be able to say, no,
02:15 - this looks like a low-probability thing.
02:17 - This is weird.
02:18 - Do something about it.
02:20 - Maybe don't trust it.
02:21 - Ask a human for help or something like that.
02:26 - And this is really an exciting time
02:29 - to study generative models because there's
02:31 - been a lot of progress over many different modalities.
02:36 - I'm going to start with images because that's where
02:39 - I've done a lot of my research.
02:42 - When I started working in this space about 10 years ago,
02:47 - these were the sort of images that we were able to generate.
02:50 - And even that was already very, very remarkable.
02:53 - People were very surprised that it
02:55 - was possible to train a machine learning system
02:58 - to produce images of people that sort of are black and white.
03:04 - And they roughly had the right shape.
03:06 - People were very impressed by those sort of results.
03:09 - And you can see that over a few years,
03:11 - this progress was largely driven by generative adversarial
03:16 - networks, which is a class of generative models
03:18 - we're going to be talking about.
03:20 - You can kind of see how the generations are becoming
03:23 - better and better, higher resolution, more detail,
03:26 - more realistic kind of images of people.
03:28 -
03:31 - One of the big improvements that happened over the last two
03:34 - or three years, which was actually largely
03:36 - coming out of Stanford--
03:39 - Yang Song, who was a PhD student in my group,
03:43 - came up with this idea of using score-based diffusion models,
03:47 - which is a different kind of generative models that
03:50 - we're also going to be talking about in this--
03:52 - in this course.
03:53 - And was able to further push the state of the art,
03:56 - for example, generating images, very high-resolution images
03:59 - that look like this.
04:00 - These people don't exist.
04:02 - They are completely synthesized, generated by one
04:06 - of these generative models.
04:09 - And this is really--
04:11 - diffusion models are really the technology that drives a lot
04:17 - of the text-to-image systems that you might have seen,
04:21 - things like Stable Diffusion, or DALL-E, or other--
04:26 - or Midjourney, we think are all based
04:29 - on this type of generative model,
04:32 - this way of representing probability distribution based
04:35 - on a diffusion model.
04:37 - And once you have a good diffusion model,
04:40 - you can try to control it using captions.
04:43 - And now you get this kind of really cool
04:46 - text-to-image systems, where you can ask a user for an input.
04:53 - What kind of image do you want?
04:54 - A caption of what kind of image the system
04:57 - should be able to produce.
04:58 - For example, an astronaut riding a horse.
05:01 - And these are the kind of results
05:04 - that you can get with these systems we have today.
05:08 - This is really cool.
05:12 - I mean, these models have been trained on a lot of data,
05:15 - but presumably, they have not seen something
05:18 - like this on the internet.
05:20 - They might have seen an astronaut.
05:21 - They definitely have seen a horse.
05:23 - But they probably have not seen those two things together.
05:26 - So it's very impressive that the model
05:28 - is able to sort of, again, understand
05:31 - the meaning of astronaut, understand the meaning of horse,
05:34 - putting them together.
05:35 - And the fact that it's able to generate this kind of picture
05:38 - tells me that there is some level of understanding
05:41 - of what it means--
05:42 - what an astronaut means and what riding means,
05:45 - what a horse means.
05:46 - And even if you look at the landscape, I don't know,
05:50 - it could be--
05:50 - it feels like it's probably on some other planet or something.
05:54 - So there is some level of understanding
05:56 - about these concepts that is showing here.
05:59 - And that's super exciting, I think,
06:03 - because it means that we're really
06:04 - making progress in this space and understanding
06:07 - the meaning of text, of images, their relationship,
06:09 - and that's what's driving a lot of the successes
06:12 - that we're seeing in ML these days.
06:16 - Here is another example.
06:18 - If you ask a system about an a perfect Italian meal, you get--
06:22 - here I'm generating multiple samples.
06:25 - So because it's a probability distribution,
06:28 - you can imagine you can sample from it
06:30 - and it will generate different answers.
06:32 - So the generation is stochastic.
06:34 - Different random seed, it will produce different outputs
06:37 - every time.
06:37 - And here we can see four of them.
06:39 - Again, I think it does a pretty good job.
06:41 - I mean, some of the stuff is clearly made up, but it does--
06:45 - it's interesting how it kind of even captures out of the window,
06:48 - the kind of--
06:50 -
06:53 - the kind of buildings you would probably see in Italy.
06:56 - And it kind of has the right flavor, I think.
06:59 - It's pretty impressive kind of thing.
07:03 - Here's another example from a recent system developed
07:07 - in China.
07:08 - This is a teddy bear wearing a costume,
07:11 - is standing in front of the Hall of Supreme Harmony and singing
07:14 - Beijing opera.
07:15 - So again, a pretty crazy sort of caption.
07:18 - And it produces things like this.
07:23 - Pretty impressive.
07:25 -
07:29 - And this is the latest that came out very recently.
07:33 - We don't know yet what this model is built on.
07:37 - DALL-E 3 from OpenAI, this is an example from their blog post.
07:43 - You're asking the model to generate--
07:45 -
07:48 - you can see the caption yourself.
07:52 - Pretty cool.
07:54 - Again, demonstrates a pretty sophisticated level
07:57 - of understanding of concepts.
07:58 - And a good way of combining them together.
08:00 - Right.
08:01 -
08:04 - So this is a text-to-image generation.
08:07 - Again, the nice thing about these models
08:09 - is that you can often control them using different kinds
08:12 - of control signals.
08:13 - So here, we're controlling using text, using captions,
08:17 - but there is a lot of inverse problems.
08:20 - Again, this is a field that I've been-- that has
08:22 - been studied for a long time.
08:24 - People have been thinking about how to colorize an image, how
08:27 - to do super-resolution on an image,
08:29 - how to do inpainting on an image.
08:33 - These problems become pretty much easier
08:35 - to solve once you have a good system that really understands
08:38 - the relationship between all the pixel values
08:42 - that you typically see in an image.
08:44 - And so there's been a lot of progress in,
08:48 - let's say, super-resolution.
08:50 - You go from low-resolution images like this
08:52 - to high-resolution images like that.
08:54 - Or colorization, you can take old black and white photos
08:57 - and you can kind of colorize them in a meaningful way.
09:00 - Or inpainting, so if you have an image where some of the pixels
09:04 - are masked out, you can ask the model to fill them in.
09:09 - And they do a pretty good job at doing these.
09:12 - These are probably not the most up-to-date references,
09:15 - but you can get a sense of why these models are
09:19 - so useful in the real world.
09:23 - And here is an example from SDEdit,
09:25 - which is one of the things that, again, one of my PhD students
09:29 - developed.
09:30 - This is back to the sketch-to-image, where
09:33 - you can start with a sketch of sort of a painting or an image
09:36 - that you would like.
09:37 - The kind of thing I would be able to do.
09:39 - And then you can ask the model to refine it and produce
09:42 - some pretty picture that kind of has the right structure
09:46 - but it's much nicer.
09:48 - I would never be able to produce the image at the bottom,
09:51 - but I can probably come up with the sketch you see on the top.
09:56 - And yeah, here you can see more examples
09:58 - where you can do sketch-to-image or you can
10:01 - do even stroke-based editing.
10:04 - Maybe you start with an image and then
10:06 - you add some-- you want to change it
10:09 - based on some rough sense of what you want the image to have.
10:12 - And then the model will make it pretty for you.
10:17 - And it doesn't have to be editing
10:20 - or sort of like you don't have to control it through strokes.
10:24 - Another natural way of controlling this kind of editing
10:27 - process is through text.
10:30 - So instead of actually drawing what you want,
10:33 - you can ask the model-- you can tell the model how you
10:36 - want your images to be edited.
10:37 - So you might start with an image of a bird,
10:41 - but now you want to change it so that you
10:43 - want it to spread the wings.
10:44 - And you can tell the model, "Now spread the wings."
10:46 - And it's able to do these kind of updates.
10:49 - Or you have an image with two birds.
10:51 - And now you want the birds to be kissing.
10:53 - And then this is what you produce.
10:55 - Or you have an image with a box and you want the box to be open.
10:59 - And you can kind of see some pretty impressive results
11:03 - in terms of--
11:04 - in terms of image editing or changing the pose of this dog
11:07 - or even changing the style of the painting, of the image.
11:11 - You go from a real image to some kind of drawings.
11:15 - And again, that's a pretty good job.
11:18 - You can see it's making some mistakes.
11:20 - Like this knife here, it gets changed
11:23 - in a way that is not quite what we want.
11:26 - They are not perfect yet, but these capabilities
11:29 - are very impressive.
11:30 - They're already very useful.
11:31 -
11:34 - Cool.
11:35 - And yeah, back to the more exotic one
11:38 - that you might not necessarily think fits in this framework.
11:42 - Just to give you a sense of how general these ideas are.
11:45 - If you have a generative model of medical images,
11:48 - you can use it to essentially improve
11:51 - the way we do medical images.
11:56 - In this case, the control signal,
11:57 - it's an actual measurement that you get from, let's say,
12:00 - a CT scan machine.
12:02 - And then you can control the generative process
12:05 - using the measurement from the CT scan machine.
12:08 - And this can drastically reduce the amount of radiations then--
12:12 - say, the number of measurements that you
12:14 - need to get a crisp kind of image
12:16 - that you can show to the doctor.
12:18 - This is very similar to inpainting.
12:20 - It's just inpainting in a slightly different space.
12:23 - But you can get a sense.
12:24 - It's roughly the same problem.
12:26 - And advances in generative models
12:28 - translate into big improvements in these--
12:31 - in these real-world applications.
12:33 -
12:36 - All right, now moving on to different modalities,
12:39 - speech audio has been another modality
12:43 - where people have been able to build some pretty
12:45 - good generative models.
12:47 - This is one of the earliest one, the WaveNet model, back--
12:53 - I think it was 2016.
12:54 - And you can kind of see some examples of-- let's hope
12:58 - this works.
12:59 - So this is an example.
13:00 -
13:06 - This is kind of like the pre-deep learning thing.
13:09 - And they are not-- these are not great text-to-speech--
13:11 - [AUDIO PLAYBACK]
13:12 - - The Blue Lagoon is a 1980 American romance and adventure
13:14 - film directed by Randal Kleiser.
13:16 - [END PLAYBACK]
13:17 - SPEAKER: And then the WaveNet model,
13:18 - which is a deep learning-based model for text-to-speech.
13:21 - You're going to see it's significantly better.
13:23 - [AUDIO PLAYBACK]
13:24 - - The Blue Lagoon is a 1980 American romance and adventure
13:27 - film directed by Randal Kleiser.
13:29 - [END PLAYBACK]
13:30 - SPEAKER: And these are maybe the latest ones
13:33 - that are based on diffusion models again.
13:36 - So this is-- well, this is a combination of diffusion models
13:38 - and autoregressive models.
13:40 - But here, you can see some of the 2023 stuff.
13:43 - [AUDIO PLAYBACK]
13:44 - - Once you have the first token, you want to predict the second
13:47 - token, given the input, and the first token using multi-head
13:51 - attention.
13:51 - [END PLAYBACK]
13:52 - SPEAKER: So you can see it's much more realistic.
13:55 - There is a little bit of an accent here.
13:58 - There is a little bit of emotions that are--
14:02 - it feels a lot less robotic, a lot less fake.
14:05 - Here's another example.
14:07 - This is just text-to-speech.
14:09 - You input a text and you produce the speech corresponding
14:12 - to that text.
14:13 - [AUDIO PLAYBACK]
14:13 - - CS236 is the best class at Stanford.
14:16 - [END PLAYBACK]
14:17 - [LAUGHTER]
14:17 - SPEAKER: So this is another example.
14:19 -
14:21 - And again, you can sort of use these things to do--
14:24 - to solve inverse problems.
14:26 - So you can do super-resolution in the audio space.
14:29 - So you can condition on the--
14:31 - kind of like a low-quality signal, the kind of thing
14:33 - you can get maybe on phones.
14:36 - [AUDIO PLAYBACK]
14:37 - - One is investment.
14:39 - One is reform.
14:39 - [END PLAYBACK]
14:40 - SPEAKER: And then you can super-resolve it.
14:42 - [AUDIO PLAYBACK]
14:43 - - One is investment.
14:44 - One is reform.
14:45 - [END PLAYBACK]
14:46 - SPEAKER: And again, this is the same problem
14:47 - with basically inpainting here.
14:48 - Like you're missing some pixels.
14:50 - You're missing some frequencies.
14:51 - And you can ask the model to make them up for you.
14:54 - And to the extent that it understands the relationship
14:56 - between these values.
14:59 - You can also kind of think of as images.
15:01 - It can do a pretty good job at super-resolving audio.

00:00 -
00:05 - Language, of course, that's another space
00:08 - where there's been a lot of progress and a lot of excitement
00:11 - around large language models.
00:15 - These are basically models that have
00:19 - been trained over large quantities of text, collected
00:22 - on the internet often, and then they
00:25 - learn a probability distribution over which sentences make sense
00:28 - or not.
00:30 - And you can use it to, again, do some sort
00:32 - of inpainting where you can ask the model
00:35 - to create a sentence that starts with some kind of prompt.
00:40 - For example, this was an old language model,
00:44 - I guess in 2019, I think.
00:47 - Where you can ask the model to continue a sentence that starts
00:51 - with, to get an A+ in deep generative models,
00:55 - students have to.
00:57 - And then let's see what the language model does,
01:00 - and then it completes it for you, right?
01:02 - And then it says something somewhat reasonable.
01:05 - They have to be willing to work with problems that
01:07 - are interesting, the best, not great, not perfect for today's
01:14 - standards.
01:14 - But again, for when this thing came out,
01:16 - it was pretty mind-blowing that you
01:19 - could build a model that can generate this quality of text.
01:25 - Now I tried something similar on ChatGPT,
01:29 - and this time I tried something harder.
01:31 - Like here I said, to get an A+ in deep generative models,
01:35 - here I tried, what should I do to get an A+ in CS236
01:40 - at Stanford.
01:40 - So I didn't even tell the model ChatGPT what CS236 is.
01:45 - It actually knows that CS236 is deep generative models,
01:50 - and here it gives you some actually pretty good tips
01:53 - on how to do well in the class.
01:55 - Attend lectures, read the materials,
01:58 - stay organized, seek help, do the homeworks.
02:02 - Then it gives you 15 of them.
02:04 - I cut the prompt here, but it's pretty impressive
02:11 - that you can do these kind of things.
02:13 - And again, it probably means that there
02:16 - is some level of understanding, and that's
02:20 - why these models are so powerful,
02:21 - and people are using them for doing all sorts of things
02:24 - because they can generate means they understand something,
02:27 - and then you can use the knowledge
02:28 - to solve a variety of tasks that we that we care about.
02:31 -
02:35 - Of course, the nice thing about this space
02:41 - is that you can often mix and match.
02:43 - So you can control these models using various sorts
02:49 - of control signals.
02:50 - Once you can do generation, you can
02:51 - steer the generative process using different control signals.
02:56 - A natural one here would be, generate the text in English,
03:01 - conditioned on some text in a different language,
03:04 - so maybe Chinese.
03:06 - So you have-- and this basically is machine translation, right?
03:10 - So progress in generative models basically
03:14 - directly translate into progress in machine translation.
03:18 - If you have a model that really understands how to generate text
03:21 - in English, and it can take advantage of the control signal
03:25 - well, then it means that essentially it's
03:27 - able to do translation reasonably well.
03:31 - And a lot of the progress in the terms of the models
03:34 - and the architectures that we're going to talk about
03:36 - in this class are the kind of ideas
03:38 - that are behind the pretty good machine translation
03:42 - systems that we have today.
03:44 -
03:47 - Another example is code.
03:50 - Of course, very exciting as a computer scientist.
03:53 - Many of you are computer scientists, write a lot of code.
03:56 - At the end of the day, code is text.
03:58 - If you have a model that understands
04:00 - which sequences of text make sense
04:04 - and which ones don't, you can use it to write code for you.
04:08 - So here's an example of a system that exists today
04:13 - where you can try to get the model to autocomplete,
04:17 - let's say the body of a function based
04:19 - on some description of what the function is supposed to do.
04:22 -
04:25 - Again, these systems are not perfect, but they are very--
04:28 - they're already pretty good.
04:30 - Like they can do many, they can solve many interesting tasks,
04:35 - they can solve programming assignments,
04:41 - they can solve competition.
04:42 - They do reasonably well in competitive programming
04:47 - competitions.
04:48 - So again, pretty cool that they understand the natural language,
04:54 - they understand the syntax of the programming language,
04:56 - they know how to put things together so that they
04:59 - do the right thing.
05:00 - They are able to translate in this case from natural language
05:03 - to a formal language and Python in this case
05:08 - and do the right thing.
05:10 - So lots of excitement also around these sort of models.
05:14 -
05:17 - Another one that is pretty cool is video.
05:20 - This is one of the active ones where the first systems are
05:25 - being built. Again, you can imagine a variety
05:28 - of different interfaces where you
05:29 - can control the generative process
05:32 - through many different things.
05:34 - A natural one is text.
05:36 - You might say you start with a caption
05:38 - and then you ask the model to generate a video corresponding
05:47 - to that caption.
05:48 - This is one example.
05:51 - The videos are pretty short right now.
05:53 - That's one of the limitations.
05:55 - But can you see it?
05:57 - There, oh, yeah.
05:58 - OK, it shows up there.
06:01 - It is another example.
06:03 - You're asking it to generate a video of a couple sledding
06:06 - down a snowy hill on the tire Roman chariot style.
06:12 - And this is sort of what it produces.
06:16 - They are pretty short videos.
06:19 - At the end of the day, you think of a video
06:21 - as a sequence of images.
06:22 - So if you can generate images, it's
06:24 - believable that you can also generate a stack of images
06:28 - which is essentially a video.
06:31 - But pretty impressive that there's
06:33 - a good amount of coherence across the frames.
06:36 - It captures roughly what's asked by the user
06:39 - and the quality is pretty high.
06:42 - And if you're willing to work on this
06:46 - and stitch together many different videos,
06:48 - you can generate some pretty cool stuff.
06:50 - [VIDEO PLAYBACK]
06:53 -
07:30 - This is just basically stitching together
07:32 - a bunch of videos generated with the previous system,
07:36 - and again, you can see it's not perfect, but it's remarkable.
07:43 - I mean, we're not at the level where you can just
07:45 - ask the system to produce a movie for you
07:49 - with a certain plot or whatever with your favorite actor,
07:52 - but it's already able to produce pretty high-quality content
07:56 - that people are willing to look at and engage with.
07:59 - So that's an exciting kind of development
08:02 - that we're seeing generative models of videos.
08:04 - I think when that starts to work,
08:06 - and we're seeing the kind of progress in this space
08:08 - that I showed you before for images, it's
08:11 - happening right now.
08:12 - I think when people figure this out and get really good systems,
08:16 - they can generate long videos of high quality.
08:19 - This could be really changing the way we--
08:22 -
08:26 - a lot of the media industry is going
08:28 - to have to pay attention to this.
08:30 - I don't know exactly what went into this.
08:32 - I didn't make it myself, but I know
08:35 - the system allows you to also control it
08:39 - through a caption and a seed image.
08:42 - So if you maybe already know what you want your character
08:47 - to look like, then you can kind of use it and animate,
08:50 - let's say, a given image.
08:52 - And again, it's an example of controlling
08:56 - the generative process like you can control it through text,
08:59 - you can control it through images.
09:01 - There are many different ways to do this.
09:03 -
09:06 - Yeah, and this is actually from a former PhD
09:11 - Student in our group.
09:12 - So yeah, it's a system that they are developing.
09:15 - It's very good, I agree with you.
09:19 - Pretty impressive stuff.
09:20 - So that's the kind of thing you can do once you
09:22 - learn this material very well.
09:27 - All right, other completely different sort
09:29 - of application area, sort of decision-making,
09:34 - robotics, these kind of a lot of these domains, what
09:40 - you care about is taking actions in the world
09:43 - to achieve a certain goal, let's say driving a car
09:45 - or stacking some objects, and so at the end of the day,
09:49 - you can think of it as generating a sequence of actions
09:52 - that makes sense.
09:53 - And so again, the kind of machinery
09:56 - that we're going to talk about in this course
10:00 - translates pretty well to a lot of what we call imitation
10:03 - learning problems where you are given examples
10:05 - of good behavior provided, maybe by a human
10:08 - and you want your model to generate
10:10 - other behaviors that are good.
10:12 - For example, you want the model to learn how to drive the car
10:15 - or how to stack objects.
10:18 - So here's an example of how you can use these sort of techniques
10:22 - that we're going to talk about in the course
10:24 - to learn how to drive the car in this in this video game.
10:29 - And you have to figure out, of course, what actions make sense
10:32 - and to not crash into other cars and stay into the road
10:39 - and so forth.
10:40 - It's non-trivial again, but if you have a good generative model
10:44 - then you can make good decisions in this simulator.
10:48 - This is an example where you can train a diffusion model
10:52 - in this case to stack objects.
10:56 - So again, you need to figure out what trajectories make sense
10:59 - and if you have a good model that understands which
11:02 - trajectories have the right structure then you
11:04 - can use it to stack a different set of objects,
11:06 - and you can control the model to produce high quality policies.
11:12 - There's a lot of excitement in the scientific--
11:16 - science and engineering around generative models.
11:20 - One of your TAs is one of the world's experts
11:23 - on using generative models to synthesize molecules that
11:28 - have certain properties or proteins that
11:30 - have certain properties, and either
11:34 - at the level of their structure, or even
11:38 - at the 3D level kind of really understand
11:40 - the layout of these molecules.
11:42 - And yeah, there is a lot of interest
11:45 - in this space around building generative models
11:50 - to design drugs or to design better catalysts.
11:55 - At the end of the day, you can think of it as again,
11:58 - some kind of generative model where
11:59 - you have to come up with a recipe that
12:01 - does well at a certain task.
12:03 - And if you train a model on a lot of data on what's--
12:07 - let's say, proteins perform well in a certain task
12:10 - then you might be able to generate a sequence
12:12 - of amino acids that perform.
12:14 - That does even better than the things we have
12:17 - or you might be able to design a drug that binds in a certain way
12:21 - because you're targeting, let's say,
12:23 - you know COVID or something.
12:25 - And so there is a lot of interest
12:29 - around building generative models over modalities
12:32 - that are somewhat different from the typical ones.
12:35 - It's not images.
12:35 - It's not text.
12:36 - But it's the same generative models.
12:38 - It's stable diffusion models.
12:40 - So there's going to be autoregressive models.
12:42 - It's going to be the models we're going
12:45 - to talk about in this course.
12:49 - And right, so lots of excitement.
12:53 - There are many other modalities that I
12:57 - didn't put in the slide deck where there's been
13:01 - progress generating 3D objects.
13:05 - That's another very exciting area and many more.
13:11 - Of course, there is also a bit of worry,
13:17 - and hopefully we'll get to talk about it a bit
13:20 - in the class around--
13:24 - if we're computers are getting so good at generating content
13:27 - that is hard to distinguish from the real one,
13:29 - there is this big issue around deepfakes.
13:33 - which one is real, which one is fake.
13:35 - This was produced again by my students.
13:37 - But you can get a sense of the sort of dangers
13:40 - that these kind of technologies can have.
13:42 - And there is a lot of potential for misuse
13:45 - of these sort of systems.
13:47 - So hopefully, we'll get to talk about not
13:49 - doing that in the class.

00:00 -
00:05 - All right, so was the intro.
00:10 - Hopefully, got you excited about the topic
00:13 - and it showed you that it's really an exciting time
00:16 - to be working in this area.
00:17 - And that's why there is so much excitement also in the industry
00:20 - and in academia around these topics.
00:22 - Everybody is trying to innovate, build systems, figure out
00:27 - how to use them in the real world, find new applications.
00:30 - So it's really an exciting time to study this.
00:35 - The course is designed to really give you the--
00:39 - uncover what we think are the core concepts in this space.
00:47 - Once you understand all the different building blocks,
00:50 - the challenges, the trade-offs that all these models do.
00:55 - Then you can not only understand how existing systems work
01:00 - but hopefully you can also design the next generation
01:02 - of these systems, improve them, figure out
01:05 - how to use them on a new application area.
01:10 - Again, the system, the course is designed to be pretty rigorous.
01:14 - There's going to be quite a bit of math.
01:16 - It's really going to delve deep into the key ideas.
01:20 - And so, we're going to talk a lot about representation
01:25 - as we discuss.
01:26 - The key building block is going to be statistical modeling.
01:30 - We're going to be using probability distributions.
01:32 - That's going to be the key building block.
01:34 - And so, we're going to talk a lot about how
01:37 - to represent these probability distributions,
01:40 - how to use neural networks to model probability distributions
01:46 - where we have many random variables.
01:48 - That is the challenge.
01:49 - And you've seen simple probability distributions,
01:51 - like Gaussians and things like that.
01:54 - Doesn't work in this space because you
01:56 - have so many different things that you have to consider
01:59 - and you have to model at the same time.
02:01 - And so, you need to come up with clever ways
02:03 - to represent how all the different pixels in an image
02:06 - interact with each other or how the different words
02:09 - in a sentence-- they are connected to each other.
02:11 - And so, a lot of it will--
02:14 - a lot of the course content will focus
02:17 - on different ideas, the different trade-offs
02:19 - that you have to make when you build these kind of models.
02:24 - We're going to talk about learning.
02:25 - Again, these are going to be statistical generative models.
02:29 - So there's always going to be data,
02:30 - and you're going to use the data to fit the models.
02:33 - And there's many different ways to fit models.
02:36 - There's many different kinds of loss functions
02:38 - that you can use.
02:39 - There's stuff that is used in diffusion model, that is,
02:41 - the stuff that is used in Generative Adversarial
02:43 - Networks.
02:44 - There is the stuff that is used in--
02:46 - say, large language models, autoregressive models.
02:49 - Those are essentially boiled down
02:51 - to different ways of comparing these probability
02:53 - distributions.
02:54 - You have a data distribution, you have the model
02:57 - distribution, and you want those two things to be similar.
03:00 - So that when you generate samples from the model,
03:03 - they look like the ones that came
03:05 - from the data distribution.
03:07 - But probability distributions, again,
03:10 - going back to the first point, they are very complex.
03:13 - If you have very complicated objects,
03:16 - very high-dimensional objects, so it's
03:18 - not straightforward to compare two probability distributions.
03:21 - And measure how similar they are.
03:24 - So you have to have a data distribution,
03:27 - you have a family of models that you can pick from,
03:29 - and you have to pick one that is close to the data.
03:33 - But measuring similarity is very difficult,
03:37 - and depending on how you measure similarity,
03:39 - you're going to get different kinds of models
03:41 - that work well in different kinds of scenarios.
03:45 - And then we're going to talk about inference.
03:47 - We're going to talk about how to generate samples
03:50 - from these models efficiently.
03:52 - Sometimes you have the probability distribution,
03:54 - but it might not be straightforward to sample
03:56 - from it.
03:57 - So we will talk about that.
03:59 - We will talk about how to invert the generative process,
04:02 - how to get representations from these objects.
04:06 - For example, following and making
04:11 - the idea of vision as inverse graphics a little
04:14 - bit more concrete.
04:16 - And so we'll touch on unsupervised learning
04:19 - and different ways of clustering because, at the end of the day,
04:23 - what these models do is they have to find
04:25 - similarity between data points.
04:28 - When you're trying to complete a sentence, what you have to do
04:30 - is you have to go through your training set,
04:33 - you have to find similar sentences,
04:34 - you have to figure out how to combine them,
04:36 - and you have to figure out how to complete
04:38 - the prompt that you're given.
04:40 - So once you have generative models,
04:43 - you can usually also get sort of representations.
04:47 - You have ways of clustering data points that
04:49 - have similar meaning.
04:51 - And again, you can get features, and you
04:54 - can do the things you would want to do
04:57 - in unsupervised learning, which is do machine learning
05:00 - when you don't have labels.
05:01 - You only have the x but you don't have the y.
05:07 - And you want to do interesting things
05:09 - with the features themselves.
05:15 - And so, those are the three key ideas
05:19 - that are going to show up quite a bit in terms of models.
05:23 - We're going to be talking about first the perhaps the simplest
05:29 - model, which is one where essentially you have access
05:34 - to a likelihood directly.
05:36 - And there's going to be two kinds of models in this space.
05:40 - Autoregressive models and flow-based models.
05:43 - So autoregressive models are the ones
05:44 - used in large language models and a few of other systems
05:48 - that I talked about today.
05:50 - Flow-based models are a different kind of idea
05:53 - that is often used for images and other kinds
05:57 - of continuous data.
05:58 -
06:01 - Then we'll talk about latent variable models,
06:03 - the idea of using latent variables to increase
06:05 - the expressive power essentially of your
06:08 - of your generative models.
06:10 - We'll talk about variational inference,
06:12 - variational learning, the variational autoencoder,
06:15 - hierarchical variational autoencoders.
06:17 - Those sort of ideas.
06:19 - We'll talk about implicit generative models.
06:21 - Here the idea is that instead of representing the probability
06:25 - distribution p of x, you're going
06:27 - to represent the sampling process that you
06:29 - use to generate samples.
06:32 - And that has trade-offs.
06:34 - It allows you to generate samples very efficiently,
06:37 - but it becomes difficult to train
06:39 - the models because you don't have access to a likelihood
06:42 - anymore.
06:43 - So you cannot use maximum likelihood estimation,
06:46 - those kind of ideas that we understand very well and we
06:50 - know have good performance.
06:53 - So we'll talk about two-sample tests, F-divergences,
06:57 - and different ways of training these sort of systems.
07:00 - And in particular, we'll talk about Generative Adversarial
07:03 - Networks and how to train them.
07:06 - Then we'll talk about energy-based models
07:08 - and diffusion models.
07:10 - Again, this is sort of a state-of-the-art in terms
07:14 - of image generation, audio generation.
07:17 - People are starting to use them also for text.
07:21 - That's what the technology behind the video
07:23 - generation that I showed you before.
07:26 - So we'll talk in-depth about how they work
07:28 - and how you can think of them in terms of a latent variable
07:31 - model and the connections with all the other things.
07:35 - And Yeah, again, it's going to be a fairly mathematical class.
07:40 - So there's going to be a lot of theory.
07:42 - There's going to be algorithms.
07:44 - And then we'll go through applications.
07:47 - There is going to be homeworks where
07:48 - you're going to get to play around with these models.


00:00 -
00:05 - SPEAKER: Welcome to lecture 2.
00:09 - The plan for today is to talk a little bit
00:15 - about what a generative model is and we're
00:21 - going to encounter the first challenge whenever we want
00:26 - to build a generative model of complex data sets
00:29 - like images, text, which is the usual curse of dimensionality
00:32 - that you might have seen before in other machine learning
00:35 - classes.
00:36 - And so the plan for today is to discuss
00:40 - a little bit various ways at a very high level
00:45 - that people have come up with to deal
00:46 - with the curse of dimensionality.
00:48 - And so we'll do a very brief crash
00:51 - course on graphical models, this is kind of
00:54 - like my CS 228 class or a part of it
00:57 - compressed in a single lecture or half of a lecture.
01:02 - And then we'll talk a little bit about the distinction
01:05 - between a generative model and a discriminative model,
01:07 - which is something, again, you might
01:09 - have seen before in ML classes.
01:11 - And finally, we'll get into the deep part
01:14 - of the deep generative models and we'll
01:16 - start to see how you can use neural networks to deal
01:18 - with the curse of dimensionality.
01:20 -
01:23 - All right.
01:23 - So this is going to be a high level picture, a high level
01:26 - overview that roughly corresponds
01:30 - to a lot of the ideas that we're going to talk about
01:34 - in this course and it deals with this problem of learning
01:37 - a generative model, the challenges that you encounter
01:40 - and the different ways you can address them.
01:43 - And by changing different pieces kind of like in this picture,
01:47 - you're going to get different classes of generative models.
01:50 - You might get autoregressive models
01:53 - like the ones that are usually used for language,
01:55 - you might get diffusion models, generative adversarial networks,
01:59 - these things by changing ingredients
02:02 - into this high level picture.
02:04 - So this picture will come up several times
02:06 - throughout this quarter and it kind of deals
02:08 - with this basic problem that you have whenever you want
02:11 - to train a generative model.
02:13 - So the basic problem is one where
02:16 - you're given a set of examples.
02:19 - This might be images or it might be sentences
02:23 - that you've collected on the internet
02:25 - or it could be, I don't know, DNA sequences,
02:28 - it could really be anything.
02:31 - The assumption is that these data
02:34 - points that you have access to are sampled
02:37 - from some unknown probability distribution
02:41 - that we're often going to denote P data.
02:44 - This is kind of the the data generating process,
02:46 - it's some complicated unknown process that
02:52 - has generated the data for you.
02:54 - And so the assumption is that all these different data
02:57 - points that you have access to are related to each other
03:00 - because they come from some true underlying common data
03:05 - generating process.
03:07 - And in the case of language, this
03:09 - might correspond to maybe if you have a corpora of text collected
03:13 - from the internet, this might correspond to the different ways
03:17 - people write text for websites or for whatever sites
03:23 - you've scraped to collect your data set,
03:25 - or it might correspond to the complicated physical processes
03:29 - that give rise to a natural distribution over images.
03:33 - The key point here is that this data distribution
03:35 - pdata is unknown.
03:38 - You assume there is such an object
03:40 - but the only thing you have access to
03:42 - are a bunch of examples, are a bunch
03:43 - of samples from this distribution.
03:46 - And the whole problem in this class
03:50 - and in the space of generative models and generative AI
03:53 - is to basically come up with a good approximation of this data
03:57 - generating process.
03:59 - Because the idea is that if you have
04:01 - access to a good approximation to this data generating process,
04:04 - this data distribution pdata, then you
04:07 - can sample from this approximation
04:09 - that you have access to and you can generate new text.
04:12 - Or if you have a distribution over images,
04:16 - then you can sample from the distribution
04:18 - and you can generate new images that hopefully
04:21 - are close to the ones you've used for training or model
04:25 - to the extent that you're doing a good job by coming up
04:28 - with a good approximation of this data distribution,
04:30 - hopefully, your samples are also going to be good.
04:33 - And so in order to do that, you need
04:35 - to define a model family which is this set here in green.
04:40 - And you can think of this as a set of different probability
04:44 - distributions that are indexed or parameterized
04:47 - with this variable theta.
04:49 - So think of it as all possible Gaussian distributions
04:52 - that you can get as you change the mean and the covariance
04:55 - or all the distributions that you
04:57 - can get as you change the parameters
04:59 - of your neural network, right?
05:02 - And once you've defined this set,
05:04 - the goal becomes that of trying to find
05:06 - a good approximation of the data distribution within the set.
05:10 - And so in order to do that, you need
05:12 - to define some notion of distance,
05:14 - so you kind of need to define a loss function,
05:17 - you need to specify what you care about
05:19 - and what you don't care about.
05:20 - These objects, this probability distribution, the data
05:23 - distribution, and your model distributions
05:25 - are going to be pretty complex, they
05:27 - are defined over high dimensional spaces.
05:29 - So there's a lot of different, let's say images
05:32 - you can assign probability to and you somehow
05:34 - need to specify what you care about
05:37 - and what you don't or equivalently you
05:39 - need to specify some notion of distance or similarity
05:43 - between the data distribution and your model.
05:46 - And then you have an optimization problem,
05:48 - then becomes a question of how do
05:50 - you find the distribution in your set,
05:52 - in your model family that is as close as possible to the data
05:55 - distribution.
05:56 - And so you try to find this projection
05:58 - and try to find this point and then
06:01 - hopefully, if you can solve this potentially hard optimization
06:04 - problem you come up with your model,
06:07 - you come up with a distribution that
06:09 - is hopefully relatively close to your data distribution.
06:12 - And again, then you can use it then
06:14 - you have your language model or you have your diffusion model
06:17 - and you can use it to generate images,
06:19 - you can use it to generate text, you
06:20 - can do many different things.
06:23 - And so we see that there are several components here always
06:31 - you need to start with some data,
06:33 - then you need to define a model family,
06:35 - and then you need to define a loss function or a similarity
06:39 - metric between distributions that you should optimize over.
06:43 - And what we'll see is that you're
06:44 - going to get different classes of generative models
06:46 - by changing these ingredients.
06:49 - And the issue here is that it's not straightforward to come up
06:55 - with--
06:56 - is not like an optimal solution here
06:58 - and that's why there are many different generative
07:00 - models which is not clear what's the right model family that we
07:05 - should use, it's not clear what's
07:07 - the right notion of similarity that we
07:08 - should use, for example, if you think about different data
07:11 - modalities.
07:12 - So that's why we're going to see different families
07:14 - of the generative models that will essentially
07:17 - make different choices with respect to the model family,
07:20 - with respect to the loss, and so forth.
07:22 - But at the end of the day pretty much all of the models
07:26 - we'll see we'll try to learn this probability distribution.
07:32 - And this is again, useful because once you
07:36 - have this probability distribution,
07:38 - you can sample from it, you can generate new data,
07:42 - you can do density estimation.
07:44 - So if you have access to a probability distribution,
07:47 - then you can query your probability distribution
07:50 - for any input x and the model can tell you
07:53 - how likely this object is.
07:56 - So if you train a model over a bunch of images of dogs,
07:59 - then you come up with this P theta,
08:00 - this distribution here that is as close as possible to the data
08:04 - distribution.
08:04 - Then you can fit in a new image and the model
08:07 - will tell you how likely was it that this image was generated
08:12 - basically by this model distribution
08:14 - that you've come up with.
08:16 - And this is potentially useful because you
08:19 - can do for example anomaly detection,
08:22 - you can check how likely that object is and you
08:24 - can start reasoning about the inputs
08:27 - that your models are seeing, you can identify anomalies,
08:30 - you can do many interesting things once you have access
08:33 - to a density.
08:36 - And finally, this is also useful because essentially it's
08:44 - a clean way to think about unsupervised learning.
08:47 - If you think about it, if you're trying
08:49 - to build a model that assigns high probability to images that
08:54 - look like the ones you have in your training
08:56 - set that, again, in order to do well,
08:59 - you need to understand what all these images have in common.
09:03 - And so maybe in this example you might
09:05 - need to understand what does a dog look like,
09:09 - parts you need to have, what kind of colors
09:13 - exist in the real world, which ones don't, and things
09:16 - like that.
09:17 - And so implicitly by training these models,
09:20 - perhaps on large quantities of unlabeled data, you kind of end
09:24 - up learning the structure of the data,
09:26 - you end up learning what all these data points
09:28 - have in common, you end up learning
09:30 - what are kind of the axes of variation that this data set has
09:36 - and this is useful because it allows you to, for example,
09:39 - essentially discover features in an unsupervised way.
09:42 - And so we'll see that at least some
09:44 - of the generative models we'll talk about
09:46 - will actually allow you explicitly to recover features
09:50 - for the data points and you can use
09:51 - them to do controllable generation
09:54 - or you can use them to do maybe semi-supervised learning
09:58 - or few shot learning.
09:59 - Once you have good features, it should
10:01 - be relatively easy to let's say distinguish different breeds
10:04 - of dogs and things like that.
10:07 - So that's the high level story and we'll
10:10 - see all these different components in much detail
10:14 - throughout the course.
10:15 - The first big question is, how do you represent a probability
10:20 - distribution?
10:21 - How do we actually come up with a reasonable set over which we
10:26 - can optimize if we want to recover a good approximation
10:29 - to the data distribution?
10:31 - And this is not going to be trivial
10:33 - because we care about objects that are pretty complicated
10:37 - in the sense that they have, if you think about an image,
10:41 - it's going to have many pixels or if you think about text,
10:43 - we typically care about many tokens.
10:46 - And so representing a probability distribution
10:49 - over a high dimensional space is actually nontrivial
10:53 - and that's the first challenge and where you
10:56 - need to start making tradeoffs.
10:57 -
11:01 - If you're dealing with low dimensional data,
11:03 - then the problem is not hard and this is something
11:05 - you might have seen before.
11:07 - If you have, let's say a single discrete random variable,
11:13 - perhaps a binary random variable,
11:15 - then it's not hard to describe all the different things that
11:17 - can happen and assign probabilities to these events,
11:20 - right?
11:21 - So if you have a Bernoulli distribution or a Bernoulli
11:24 - random variable, then you only have two outcomes, true/false,
11:28 - heads or tails, something like that.
11:30 - And in order to specify all the possible things that can happen,
11:33 - you just need one parameter.
11:35 - You just need a single number which tells you
11:37 - the probability of heads or the probability of tails
11:40 - is just going to be 1 minus the number p.
11:43 - And learning these distributions from data is, of course, trivial
11:48 - and it's useful but this is not quite going
11:53 - to be enough to deal with let's say models over images or models
11:58 - over text.
12:01 - The other kind of building block that we're going to use
12:05 - are categorical distributions.
12:07 - So if you have more than two outcomes,
12:09 - we have let's say k different outcomes,
12:12 - then you're dealing with a categorical random variable
12:16 - or you have m different outcomes here
12:18 - and again, this is a useful building block.
12:22 - You can use it to model things like rolling a die,
12:25 - many other things.
12:26 - The challenge here or where you're
12:30 - starting to see where the issues might arise
12:34 - is that, again, if you have m different things that
12:39 - can happen, you need to specify a probability
12:42 - for each one of them.
12:43 - And so you basically need to have m numbers
12:47 - and then these numbers have to sum to 1 because it's
12:51 - a valid probability distribution and if you
12:53 - sum all the probabilities of all the different things that
12:56 - can happen, you have to get 1.
12:58 - And so these are the two building blocks
13:01 - and then you can combine them to model more interesting objects.
13:06 - So let's say you want to build a generative model over images,
13:11 - then you're going to have to model many different pixels
13:14 - and to model the color of a single pixel,
13:18 - perhaps you're going to use some RGB
13:22 - encoding where you're going to have to specify three numbers.
13:26 - You're going to have to specify the intensity
13:27 - of the red channel, which let's say is a number between 0
13:31 - and 255, you're going to have to specify a green channel
13:34 - intensity and a blue channel intensity.
13:37 - So you can imagine that with these three random variables,
13:41 - you're going to capture the space of possible colors
13:44 - that has been discretized according to that granularity
13:48 - that you've chosen.
13:50 - And now you're able to describe in many different colors
13:54 - that you can get for that particular pixel
13:56 - each one corresponding to an entry in this kind of cube.
14:01 - And so now we have a richer model
14:04 - and if you somehow are able to model this distribution well,
14:08 - so you're able to assign probabilities
14:11 - to all these entries, to all these different colors
14:13 - that this individual pixel can take, then if you were
14:16 - to sample from it then you would generate colors
14:22 - for that pixel that are reasonable.
14:24 - Hopefully, they match whatever training
14:27 - data you had access to learn this distribution.
14:31 - And how many parameters do you need
14:34 - to specify this joint probability distribution?
14:37 - How many different things can happen here, right?
14:41 - There are basically 256 time 256 time 256 different colors
14:46 - that we're able to capture.
14:49 - And so if you want to be fully general,
14:51 - you have to specify a probability
14:53 - for each one of them.
14:54 - So there is basically 256 cube entries in the cube
14:59 - and you have to be able to assign a non-negative number
15:02 - to each one of them.
15:04 - And then you know that they all have to sum to 1,
15:07 - so you have slightly less parameters
15:10 - to fit but it's still a reasonably high number.
15:14 -
15:17 - And so here you start to see the issue
15:19 - with having multiple random variables where
15:23 - the space of possible outcomes, the possible things that
15:26 - can happen it grows exponentially
15:28 - in however many random variables you want to model.
15:32 - And so as another example now let's
15:34 - say you want to model a distribution over images
15:36 - and for simplicity, let's say the images are just
15:39 - black and white.
15:40 - So you're going to model an image
15:43 - as a collection of random variables
15:45 - that is going to be one random variable for every pixel,
15:48 - maybe there is 28 times 28 pixels.
15:51 - Each pixel by itself is a Bernoulli random variable,
15:55 - it can either be on or off, white or black.
15:59 - And let's say you have a training set, maybe MNIST,
16:02 - you have a bunch of images of handwritten digits
16:06 - and that's your training set.
16:08 - And then you would like to learn a probability distribution
16:11 - over all these black and white images.
16:15 - So how do you represent it?
16:17 - Well, again, we have this collection of random variables
16:20 - and they are all binary and we have n of them,
16:23 - where n is the number of pixels that you have in the image
16:26 - so it depends on the resolution.
16:30 - And you can think about how many different images are there?
16:34 - How many different black and white images
16:36 - are there with n pixels?
16:40 - Yeah, 2 to 2 the number of pixels that you have, right?
16:42 - So there's two possible colors, two possible values
16:46 - that first pixel can take times 2, the second times 2 times
16:49 - 2 times 2.
16:50 - You do it n times and you end up with 2 to the n.
16:55 - So there is a huge number of different images
16:59 - even in this simple scenario where
17:00 - they are just black and white.
17:02 - Very large state space sometimes it's called.
17:07 - And so if you want it to be--
17:09 - somehow if you are able to come up with this model,
17:12 - somehow you are able to come up with a probability distribution
17:15 - over these binary random variables,
17:17 - then you have this object that given any input image
17:20 - it will tell you how likely it is according to the model
17:22 - and if you can sample from it.
17:24 - You can assign values to all the pixels,
17:26 - then it will generate an image.
17:28 - And if you've done a good job again
17:30 - at learning the distribution, it will
17:32 - generate let's say images that look like the ones
17:34 - that you had in the training set.
17:36 - So they will look let's say like MNIST digits.
17:38 -
17:41 - But again, you see sort of the issue
17:43 - is how many parameters do you need to specify this object?
17:47 - 2 to the n minus 1, that's the issue.
17:49 - There's 2 to the n possible things that can happen,
17:52 - you have to assign a probability to each one of them.
17:54 - Then, well, you say 1 parameter because you have to sum to 1
17:57 - but this number quickly becomes huge.
18:01 - I guess for even small number of n,
18:04 - this is more than the number of atoms in the universe.
18:07 - And so the question is, how do you store these parameters
18:12 - in a computer?
18:13 - How do you learn them from data?
18:14 - You need some kind of tricks, you
18:16 - need some kind of assumption, you need somehow
18:19 - to deal with this complexity.
18:21 - That's a challenge that you always
18:23 - encounter whenever you want to build
18:25 - a generative model of anything interesting,
18:28 - whether it's text, DNA sequences, images, videos,
18:31 - whatever, audio, you always have this issue of representing
18:35 - a distribution.
18:39 - Now one way to make progress is to assume something
18:45 - about how the random variables are related to each other
18:47 - and that's always the assumption that you have to make.
18:50 -
18:53 - A strong assumption that you can make
18:56 - is to assume that all these random variables are
18:58 - independent of each other.
19:01 - And if you recall, if the random variables are independent
19:05 - then it means that the joint distribution
19:09 - can be factored as a product of marginal distributions.
19:12 -
19:16 - Now if you're willing to make this assumption, then
19:20 - what happens?
19:21 - How many different images are there here?
19:24 - There is still 2 to the n possible images.
19:27 -
19:31 - You still have a probability distribution
19:33 - over the same space.
19:34 - You're still able to assign a probability
19:38 - number to every possible assignment
19:40 - of these n binary variables.
19:43 - So it's still a distribution over these n binary variables,
19:47 - it's still a high dimensional space.
19:49 - However, what happens is that you can drastically
19:52 - reduce the number of parameters that you need
19:54 - to store this object, right?
19:57 - How many parameters do you need to specify
19:59 - this joint distribution?
20:01 - Now it starts to become n, right,
20:03 - because you just need to be able to store
20:05 - each one of these entries, each one of these marginals,
20:08 - and these are just Bernoulli random variables,
20:11 - so you just need one parameter if they are binary.
20:15 - And so if these are binary variables,
20:19 - you need one parameter for each one
20:21 - of those marginal distributions.
20:23 - You basically just need to model each pixel separately,
20:26 - modeling a single pixel it's easy.
20:30 - And so if you're willing to make this assumption,
20:33 - you are able to represent complicated object,
20:37 - a probability distribution over images
20:39 - with a very small number of parameters,
20:43 - which means that this is something you can actually
20:46 - implement, you can afford to store these things very easily.
20:49 -
20:53 - Of course, the challenge is that this independence assumption
20:56 - is probably way too strong.
20:58 - You are literally saying that you
20:59 - can choose the values of the pixels independently.
21:02 - And if you think about modeling, let's say images of digits,
21:06 - it's probably not going to work because you imagine
21:09 - when you sample from this distribution,
21:11 - you're not allowed to look at any other pixel value
21:15 - to choose a new pixel value, right?
21:16 - And so you're literally choosing values at random, independently.
21:23 - So it's going to be very hard to be
21:25 - able to capture the right structure if you
21:28 - make such a strong independence assumption.
21:32 - So this is not quite going to work.
21:35 - What you can do is you can try to make progress
21:40 - by basically making conditional independence assumptions.
21:44 - And so one very important tool that is actually
21:49 - the thing behind autoregressive models, the language
21:53 - models, large language models, they're
21:55 - all built on that first tool which
21:58 - is the chain rule of probability which hopefully you've
22:01 - seen before.
22:02 - The basic idea is that you can always write down
22:06 - the probability of a bunch of events happening
22:09 - at the same time as a product of conditional probabilities.
22:14 - So you can always say that the probability that's S1
22:17 - happens and S2 happens and S3 happens
22:21 - and so forth, you can always write it as the probability
22:24 - that S1 happens by itself and then the probability
22:27 - that S2 happens given that S1 happened and so forth.
22:31 - And this is always the case that you can always factorize
22:34 - a distribution in that form.
22:37 - And I guess a corollary of that is the famous Bayes' rule
22:42 - which allows you to basically write
22:44 - the conditional probability of one event given
22:46 - another one in terms of the prior probability
22:49 - and kind of like the likelihood of S2 happening given S1.
22:53 -
22:55 - The important one for now is going
22:57 - to be the first one, chain rule although we're also
23:00 - going to use Bayes' rule later.
23:03 - But chain rule basically gives you
23:05 - a way of writing down a joint distribution
23:08 - as a product of potentially simpler objects
23:13 - which are these marginals also conditional probabilities.
23:20 - And so this is how you would use it,
23:25 - you can always take a joint distribution over n variables
23:30 - and write it down as a product in this way as the probability
23:34 - of x1 times the probability of x2 given x1,
23:37 - the probability of x3 given x1 and x2, and so forth.
23:43 - Using chain rule, this is something you can always do.
23:45 -
23:49 - This is the factorization that is
23:52 - used in autoregressive model, which
23:56 - is the first class of models that we're
23:58 - going to talk about which is again,
24:00 - the same thing that is used in for example large language
24:02 - models.
24:03 - And here the idea is that you can write down
24:05 - the probability of observing a sequence of words,
24:09 - let's say in a sentence as the probability of observing
24:11 - the first word times the probability of observing
24:13 - the second word given the first one, times the probability
24:16 - of observing the third word given the first two and so
24:19 - forth.
24:21 - But this is fully general, you can apply it also to pixels,
24:24 - any collection of random variables
24:26 - can always be factorized this way.
24:28 -
24:31 - Now how many parameters do we need if you
24:35 - use this kind of factorization?
24:38 - It seems like maybe we've made progress
24:40 - because this object here is very complicated
24:43 - but now p of x1 for example, is a simple object,
24:47 - is a marginal distribution over a single pixel,
24:50 - so perhaps we've made progress here.
24:55 - So let's do the math, how many parameters do we need?
24:58 - It turns out that we still need an exponentially large number
25:01 - of parameters unfortunately, and the reason
25:04 - is that it's no free lunch.
25:07 - We haven't made any assumptions to get this factorization,
25:10 - so we cannot expect to get any savings.
25:14 - And you can see it here although the first distribution here
25:22 - is indeed simple, you can store it represented
25:25 - with a single parameter.
25:27 - Then how many parameters do you need for the second?
25:31 - Well, if the variables are binary,
25:34 - then x1 can take two different values 0 and 1,
25:37 - and for each one of them, you have
25:39 - to specify a distribution over x2 which
25:41 - will take you one parameter.
25:43 - So p of x2 given x1 will take two parameters,
25:48 - one for the case where the first bit or the first variable
25:52 - is 0 and one for the case where it's 1.
25:55 - And then if you look at the p of x3 given x1 and x2,
26:00 - there are four possible values that x1 and x2 can take,
26:04 - so you need four parameters.
26:06 - So that's where you get this kind of geometric series
26:09 - and that's where you get the exponential blow up.
26:12 - These last conditionals here are very expensive.
26:15 -
26:19 - And so if you do the sum, you still don't get anything here.
26:25 - But it gives us a way to perhaps make progress,
26:29 - it's still a useful building block.
26:31 - And for example, one thing you can do
26:34 - is you can assume conditional independence.
26:39 - For example, you might be willing to assume that the value
26:44 - of the i'th plus 1 word is conditionally independent--
26:49 -
26:53 - given the i'th word, the value of the i'th plus 1 word is
26:57 - conditionally independent of all the previous words, right?
27:01 - So this is kind of like a Markov assumption.
27:03 - So if these x's maybe represent the weather,
27:06 - then you're saying the weather tomorrow
27:09 - is conditionally independent from the past given the weather
27:12 - today.
27:14 - And if you're willing to make this assumption then
27:16 - you get big savings.
27:19 - What this means is that if you think
27:20 - about the definition of conditional independence
27:23 - is that a lot of these conditional distributions
27:25 - will simplify.
27:28 - And so in particular, this probability
27:31 - of x3 given x1 and x2 becomes the probability of x3 given x2.
27:37 - So if you are predicting the third word,
27:40 - this is saying you just need to know the second word,
27:42 - you can ignore the first word.
27:46 - If you're predicting the last word,
27:48 - you don't need to remember the entire sequence,
27:50 - the previous word is sufficient.
27:54 - And if you do that, then you get this nice expression
27:58 - where the conditionals are now simple,
28:01 - like you're always conditioning on at most one variable
28:06 - and so now we get big savings.
28:10 - How many parameters do we need here?
28:12 - Yeah, something like that is linear in n basically,
28:16 - depending on if the variables are binary, this is the formula.
28:22 - So big savings and now we have a much more reasonable model.
28:26 - This is much more reasonable than the full independence
28:30 - model.
28:32 - These Markovian models are quite useful in practice,
28:36 - but again, if you think about language
28:38 - or you think about pixels in an image,
28:39 - it's probably not good enough.
28:43 - You're probably not going to do a great job
28:44 - if you're trying to predict.
28:46 - If you think about your autocomplete in your phone,
28:48 - you're trying to predict the next word just
28:51 - based on the previous one and you ignore everything else,
28:54 - you can do OK but it's not going to be great.
28:56 - You need more context to be able to make a good prediction
29:00 - about the next word.
29:04 - And so although there is an exponential reduction,
29:08 - maybe this assumption is still a little bit too strong.
29:11 - And so one way to generalize this idea
29:16 - is to use something called a Bayesian network which
29:18 - is essentially the same machinery in slightly
29:21 - more generality.
29:24 - The basic idea is again, that we're going to write down
29:29 - the joint as a product of conditionals,
29:32 - but instead of having these simple conditionals where
29:34 - it's always one variable given another variable,
29:40 - we're going to use conditional distributions where
29:43 - the i'th variable will depend on another set of random variables
29:48 - which are the parents in this Bayesian network.
29:53 - And so intuitively, the idea is that we're
29:57 - going to try to write down the joint
30:00 - as a product of conditionals, but now the conditionals
30:03 - are a little bit more complex.
30:05 - Now each variable is allowed to depend on a subset of variables,
30:09 - it could be 1, it could be more, so that buys you a little bit
30:12 - more flexibility.
30:15 - And the idea is that because we're
30:18 - using chain rule, as long as there is some ordering
30:23 - that you've used to come up with this joint distribution
30:26 - by simplifying the expression that you would get from chain
30:30 - rule, then this is kind of guaranteed
30:33 - to correspond to a valid model.
30:36 - So essentially you can specify any conditional distribution
30:41 - you want on the right hand side once you multiply them together
30:44 - you're going to get a valid probability distribution
30:47 - on the left hand side.
30:50 - That's sort of the key intuition behind the Bayesian network.
30:55 - More formally, a Bayesian network
30:58 - is a data structure that you can use to specify a probability
31:01 - distribution.
31:04 - It's a graph based data structure
31:06 - where basically there's going to be
31:08 - an underlying directed acyclic graph, which basically gives you
31:12 - the ordering in that chain rule factorization.
31:15 - So there's going to be one node in the graph
31:18 - for every random variable that you're modeling.
31:20 -
31:24 - If you're modeling images, one node for every pixel,
31:27 - if you're modeling text, one word for every token
31:29 - or every word that you have.
31:31 - And then what you do is, for every node in the graph
31:34 - you specify its conditional distribution given its parent
31:40 - in this directed acyclic graph.
31:42 -
31:45 - The graph is the structure and then
31:47 - by specifying different conditional distributions
31:49 - for each variable given the parents,
31:51 - you get different parameterizations
31:53 - of these joints.
31:56 - And the claim is that basically this is a valid probability
32:01 - distribution and the reason is that it's essentially
32:04 - the same trick we did for the Markov model.
32:08 - You start given a directed acyclic graph,
32:12 - you can always come up with an ordering,
32:14 - you can just do topological sort on the graph.
32:16 - You get an ordering, you can apply chain rule,
32:19 - you factorize with respect to their ordering,
32:21 - then you simplify the conditionals
32:23 - based on some conditional independence assumption.
32:28 - And that gives you a potentially compact data structure,
32:34 - it depends on how many parents, how dense the graph is,
32:37 - but this can give you savings.
32:39 - Again, the challenge is that we have this joint distribution,
32:42 - it takes too many parameters to represent this object.
32:46 - But if these conditionals are relatively simple,
32:50 - so you don't have too many parents for each variable,
32:53 - then these conditionals are simple enough
32:57 - that you can store this object, you can learn these parameters
33:00 - from data, and so forth.
33:02 - So it's exponential in the number of parents
33:05 - that you have for each variable.
33:07 - So if you make a very dense graph,
33:09 - you're going to get a very expressive class of models
33:12 - and you're not going to get big savings.
33:14 - If you use a chain graph where there's
33:17 - only one parent per node, then you get the Markov assumption
33:20 - that we have before and there are things in between.
33:24 - For example, what does it mean?
33:29 - A directed cycle would be something like this,
33:31 - so you need to make sure that there is no directed cycle which
33:34 - means that there is an ordering and it
33:36 - means you can use chain rule.
33:41 - This is an example of a very simple Bayesian network.
33:45 - Here the idea is that you have these five random variables
33:49 - representing the difficulty of an exam, the intelligence
33:53 - of a student, the grade that you get, so forth.
33:58 - And there is a joint distribution
34:00 - over these five random variables, which
34:04 - is obtained as a product of conditional distributions
34:08 - of each variable given the parent.
34:10 - And so for this particular graph,
34:14 - this node doesn't have any parent,
34:16 - so you just write the marginal probability of that node.
34:20 - This node doesn't have any parent,
34:22 - so again, it's just the probability of getting
34:25 - different intelligence values.
34:27 - The grade has two arrows in coming from difficulty
34:31 - and intelligence, so what you're seeing
34:33 - is that the grades that you see depend essentially
34:35 - on the possible values of the difficulty of the exam
34:38 - and the intelligence of the student.
34:41 - And so you can basically write down the joint
34:43 - as a product of conditionals that would look like this.
34:47 - And in this case, this might be more
34:50 - economical than representing the joint
34:52 - because you basically just have to specify
34:54 - these tables, these conditional probability distributions.
34:58 - You only need to work out basically
35:00 - how these random variables are related to each other locally,
35:03 - with respect to this graph.
35:05 - You only need to know how to assign grades given
35:08 - different values of difficulty and intelligence,
35:11 - but you're breaking down the complexity
35:13 - of the joint in terms of smaller local interactions
35:18 - between the random variables.
35:21 - And again, by making this assumption
35:25 - that the global dependencies can be broken down
35:29 - into simpler local dependencies, you
35:31 - get benefits because these conditionals
35:34 - are potentially much smaller, much simpler, easier
35:38 - to represent.
35:40 - And the idea is that assuming this factorization is
35:46 - the same as assuming conditional independence is
35:49 - and you can see it here we have this kind of factorization
35:55 - for the joint which is implied by this graph.
35:58 - In general, we know that you can always
36:01 - have a more complicated factorization where
36:04 - every variable depends on all the variables
36:06 - that come before it in some ordering, right?
36:10 - So in general, you would have to specify
36:11 - the probability of having a certain difficulty for the exam,
36:14 - you would have to specify the probability of some intelligence
36:17 - value given the difficulty, a probability of g given i
36:20 - and d, the probability of the SAT score given everything else,
36:24 - and so forth.
36:25 - Now if you're willing to assume that the intelligence
36:29 - of the student does not depend on the difficulty of the exam,
36:32 - then you can start simplifying these conditionals
36:36 - and they become like the ones you see above.
36:41 - For example, the SAT score only depends on the intelligence
36:45 - and you don't need to know the difficulty of the exam,
36:48 - you don't need to know the grade in the other exam
36:50 - to figure out the SAT score.
36:52 - And so this factorization basically
36:55 - corresponds to a bunch of conditional independencies.
36:59 - We're saying the difficulty and the intelligence
37:01 - are independent of each other.
37:03 - The SAT score is conditionally independent from the difficulty
37:06 - and the grade given the intelligence, and so forth.
37:10 - So Bayesian networks are basically
37:13 - a way to get to simplify complicated distributions based
37:17 - on conditional independence assumptions, which
37:21 - are more reasonable than full independence assumptions.
37:24 - Now to summarize, we can basically
37:33 - represent, use Bayesian networks as a tool
37:36 - to factorize distributions and write them down
37:39 - as a product of conditionals.
37:41 - You get the joint by multiplying together the conditionals,
37:44 - you can sample by basically going through the ordering.
37:47 -
37:50 - In this class, we're actually not going
37:54 - to be going this route, so that's
37:55 - the route that you're going to take if you want to build up
37:57 - probabilistic graph like a graphical model, a PGM,
38:01 - a probabilistic graphical model.
38:03 - In this class, we'll still be using
38:05 - a little bit of graphical models notations,
38:07 - but the graphical models are going to be relatively simple,
38:10 - they typically involve 2 or 3 random variables,
38:12 - random vectors.
38:15 - And instead, we're going to be making other softer notion
38:20 - of conditional independence which
38:21 - is going to be essentially this idea of let's use neural
38:25 - networks to try to represent how the different variables are
38:28 - related to each other.
38:30 - So we'll still have somewhat the flavor of a Bayesian network,
38:33 - but it's going to be a little bit of a software constraint
38:38 - between the variables.
38:41 - Now obviously this was a bit of a crash course
38:46 - but again, we're not going to be leveraging these things too
38:49 - much.
38:51 - We're going to use a little bit of graphical models notation
38:54 - and a little bit of directed acyclic graphs for some
38:57 - of the graphical models but nothing too heavy.
39:00 - We're going to be using different assumptions
39:02 - and different modeling ideas to build deep generative models.
39:09 - And that's going to be again inspired
39:12 - by the use of neural networks for let's say classification
39:17 - or other kind of discriminative tasks
39:19 - that you might have seen before.
39:21 - And so now it's a good time to try
39:24 - to get a sense of what's the difference between building
39:27 - a generative model versus building as usual discriminative
39:30 - model and how do we get the ideas from the things
39:34 - that we know work when you're doing
39:35 - let's say image classification or these are more
39:38 - standard kind of machine learning problems
39:41 - and translate them back into the generative modeling world.

00:00 -
00:05 - SPEAKER: We're going to use a simple generative
00:07 - model to solve a discriminative task
00:11 - and we'll see how that will differ compared
00:14 - to a traditional approach based on, let's say a neural network.
00:19 - So let's say that you want to solve a task where you're
00:22 - given a bunch of images, a bunch of emails,
00:25 - and the goal is to predict whether or not
00:27 - this email is spam.
00:29 - So there is a binary label Y that you're trying to predict
00:32 - and you're doing it using a bunch of features Xi
00:37 - and let's say the features are just binary
00:39 - and they are on or off depending on whether or not
00:42 - different words in some vocabulary appear in the email.
00:47 - And the usual assumption is that there is some underlying data
00:51 - generating process and so there is some relationship
00:54 - between the different words that you see in the email the X's
00:58 - and the Y variable which is the label you're trying to predict.
01:03 - So one way to approach this is by building a Bayesian network.
01:08 - This is a basic classifier called the Naive Bayes
01:12 - classifier which is basically going to say,
01:16 - we want to model this joint distribution,
01:18 - this joint distribution has too many variables,
01:20 - we cannot afford to store it to learn the parameters from data.
01:25 - So we're going to make a conditional independence
01:27 - assumptions and we're going to assume
01:29 - that the joint can be described by this directed acyclic graph.
01:33 -
01:36 - And if you are willing to make this kind of Bayes
01:39 - net assumption, what this means is
01:42 - that the features, the words, the Xi's are basically
01:46 - conditionally independent given the label, given
01:49 - the Y. If you're willing to make this assumption,
01:55 - then you're able to factorize the joint which
01:57 - is usually complicated as a product of conditionals.
02:00 - So you can write it as the p of Y
02:02 - because Y doesn't have any parent, and then
02:04 - the probability of 1 variable given its parent,
02:07 - probability of this variable given its parent, and so forth.
02:09 -
02:12 - Which means that you can basically--
02:14 - According to this very simplified model of the world,
02:16 - you can generate a data point by first choosing whether or not
02:19 - it's spam and then choosing whether different words appear
02:24 - in the email based on whether the email is spam or not.
02:29 - And once you have that kind of model, what you can do
02:37 - is you can try to estimate the parameters of this model
02:39 - from data.
02:40 - So you can try to estimate these probabilities
02:44 - by looking at how frequently do you see different words
02:47 - in different types of emails.
02:49 - And then you can do classification
02:52 - because at the end of the day what you're trying to do
02:54 - is you're trying to classify whether or not
02:56 - a new email is spam or not.
02:58 - And you can use Bayes' rule to write down
03:01 - the conditional distribution of Y given X. So given a new email,
03:05 - you observe which words are there and which ones are not
03:08 - and you can try to compute the probability of Y
03:11 - by basically using Bayes' rule.
03:14 - Probability of X, Y divided by the probability of X
03:18 - essentially, which is what you have at the denominator.
03:21 - And if you've done a good job at estimating these parameters
03:28 - to the extent that the assumption is true,
03:31 - this conditional independence assumption is true,
03:33 - this model might perform reasonably
03:35 - well at predicting the label Y given the features X.
03:42 - The challenge of course, is once again,
03:44 - that perhaps this conditional independence assumptions are not
03:47 - that great.
03:48 - If you think about it you're saying
03:49 - that different words appear in an email independently
03:54 - of each other.
03:55 - So once you know why basically, knowing whether a word appears
03:59 - or not doesn't help you predict whether some other word appears
04:03 - in the email or not, which is probably not reasonable.
04:07 - Nevertheless, this model tends to work OK in practice.
04:11 - So even though the assumption is not quite true,
04:13 - it might give you reasonable results in practice.
04:18 - Now how does this fit into the discriminative
04:22 - versus generative model of the problem?
04:27 - So at the end of the day, we're trying
04:31 - to model this joint distribution between features and a label Y.
04:36 - And using chain rule we can write it
04:40 - like this as the probability of the label times the probability
04:44 - of the features given the label.
04:45 - This is exactly what we've done in the Naive Bayes model
04:48 - that we just saw.
04:50 - Alternatively, you can use chain rule
04:53 - based on a different ordering and you
04:55 - can say, I can write it as the probability of observing
04:58 - this feature vector times the probability
05:02 - that, that particular feature vector has label Y.
05:07 - And so these are basically two Bayesian networks that capture
05:12 - the same joint distribution, one where we have Y and then X,
05:16 - and then one where we have X and Y.
05:19 - And the second one is basically the one that you deal
05:23 - with when you think about usual discriminative models.
05:27 - Like if you think about it, at the end
05:29 - of the day if all you care about is predicting whether a new data
05:34 - point has label 0 or 1, all you care about is p of Y given X.
05:40 - And so the second kind of modeling
05:44 - approach where you are modeling p of Y given X directly
05:48 - might be much more natural.
05:51 - In the left model we were specifying p of Y,
05:55 - we were specifying p of X given Y
05:58 - and then we would compute p of Y given X using Bayes' rule.
06:03 - While in the second model you have access to p of Y given X,
06:07 - the probability of this variable given its parent directly.
06:12 - And so the idea is that if you know that all you care about
06:19 - is p of Y given X, then there is no point in trying
06:23 - to learn or model or deal with this marginal distribution
06:28 - over the features, right?
06:30 - If you know that you're always ever going to be given an email
06:33 - and you just try to predict Y, why
06:35 - do you bother trying to figure out what kind of feature vectors
06:39 - X you're likely to see, right?
06:42 - P of X here will basically be a distribution over the features
06:47 - that your model is going to see.
06:49 - If you know you don't care because you just
06:52 - care about predicting Y from X, then you don't even
06:54 - bother modeling p of X. So that's more convenient
06:59 - and that's why typically the kind of models that you're
07:02 - building that you use in machine learning,
07:05 - they don't bother about modeling the distribution
07:07 - over the features, they just bother
07:09 - about modeling the relationship between a label and the features
07:12 - X.
07:13 - While in a generative model, it's the opposite.
07:16 - You're basically modeling the whole thing,
07:18 - you're modeling the full joint distribution.
07:23 - And so that discriminatory model is basically
07:27 - only useful for discriminating Y given
07:29 - X, while a generative model is also
07:30 - able to reason about its inputs, it's
07:33 - able to reason about the full relationship between X and Y.
07:38 - And so now there is still no free lunch in the sense
07:43 - that if you think about it, it's true
07:45 - that you can do this two factorizations,
07:48 - you can use either factorized as p of Y and then p of X
07:53 - given Y or you can do p of X and then p of Y given X.
07:59 - But in both cases you end up with some of these conditionals
08:03 - which are pretty complicated.
08:05 - So in the generative model, you have a Bayesian,
08:08 - if you were to actually unpack the fact that X
08:11 - is a random vector, so you have a bunch of individual features
08:15 - that you have to deal with, the two graphical models
08:18 - corresponding to the two chain rule factorizations
08:21 - would look like this.
08:23 - In the generative view of the world, you have Y
08:25 - and then you have all the features.
08:27 - In the discriminative view of the world,
08:29 - you have all the X's first and then you have Y given X.
08:33 - And you still need to deal with the fact
08:35 - that you have a lot of X's, you have potentially
08:37 - a lot of features that you have to take into account
08:40 - when you're predicting Y. And so in the generative modeling world
08:47 - p of Y is simple but then you have a bunch of these variables
08:51 - here that have a lot of parents, so there is a lot of complexity
08:55 - that you have to deal with when you
08:56 - need to decide what are the relationships
08:59 - between the features.
09:02 - In the discriminative modeling world,
09:05 - it's true that you're making some progress because maybe you
09:08 - don't need to model all these relationships between the X
09:10 - variables, but you still need to be
09:12 - able to model how Y depends on all the X's and Y has
09:17 - a lot of parents.
09:18 - So again, that conditional distribution
09:21 - is potentially very complicated.
09:25 - And so one way to make progress is
09:28 - to say, OK, let's make conditional independence
09:30 - assumptions.
09:31 - So in general, a generative model
09:34 - would have to look like this, so it
09:36 - would have to be able to capture all dependencies between the X's
09:40 - and the Y. If you're willing to make simplifying assumptions
09:44 - and say, oh, things are conditionally independent,
09:47 - then you basically chop some edges in the graph
09:51 - and you end up with something that is much simpler.
09:54 - Remember the last parents the variables have,
09:56 - the simpler the relationships between the random variables
10:00 - are, the simpler the model is.
10:02 - And so you're saying once I know Y,
10:06 - I can basically figure out the values of the X variables
10:09 - and there is no relationship between them.
10:12 - That's one way to make progress.
10:15 - Obviously, it's a strong assumption,
10:17 - it might or might not work in the real world.
10:21 - In the discriminative model, you still need to be able to model
10:25 - this conditional distribution of Y given all the X's.
10:30 - And again, that's not straightforward
10:32 - because if you think about all these features here,
10:35 - let's say they are binary, there are 2 to the n possible feature
10:40 - vectors that you have to deal with and for each one of them
10:43 - you would have to specify like when
10:45 - you look at this last conditional here
10:48 - is the same as before.
10:49 - You're conditioning on a lot of variables,
10:52 - there are 2 to the n possible combinations of those X
10:55 - variables and in full generality you
10:58 - would have to assign a different number,
11:00 - a different value for the probability of Y
11:02 - for each one of them.
11:05 - So again, the conditional distribution
11:07 - of Y given all the parents is not
11:10 - easy to deal with even in a discriminative model.
11:14 - So the way you make progress usually
11:16 - in a discriminative model is to assume
11:19 - that the dependency is not fully general
11:21 - and somehow takes a particular functional form.
11:25 - So it's true that this X vector can
11:29 - take many, many different values and if you
11:32 - were to use a big table that table would have 2
11:36 - to the n possible rows, so it would not
11:38 - be able to store that, it would not be able to learn from data,
11:41 - you would not be able to use it.
11:43 - But what you can assume is that there is some simple function
11:47 - that you can use to take X and map it to a probability value.
11:53 - So the assumption that you have to make here to make progress
11:57 - is to assume that there is some simple function f that you
12:00 - can apply to the different values
12:02 - that the X variables can take and that will map it
12:05 - to this number that you care about which
12:08 - is the conditional probability of Y given X. And there
12:11 - is many different ways to do it, there are some constraints here
12:19 - and one way to do it is to do what's
12:22 - done in logistic regression for example.
12:25 - So the idea is that, and that's why
12:27 - it's called regression is that essentially it's not
12:30 - a table that is going to be some function that will take
12:33 - different values of X and we'll regress them to probabilities
12:37 - for Y. And it's not an arbitrary regression problem because what
12:44 - we're doing is we're trying to map these X's
12:47 - to conditional probabilities and we know
12:50 - that conditional probability is a number that
12:52 - has to be between 1 and 0, like it doesn't make sense to say,
12:57 - oh, I fit in a certain feature vector X.
13:00 - In the spam classification is a bunch
13:02 - of indicators of whether different words appear
13:06 - in the email.
13:07 - If this function gives me a value of minus 1,
13:10 - it doesn't make sense because we know that probabilities
13:14 - are numbers between 0 and 1.
13:16 - So there are some constraints on this regression problem.
13:21 - And in particular, we want the output to be between 0 and 1,
13:27 - we want the dependency to be simple but reasonable.
13:32 - If it's too complicated, it's a table, a lookup,
13:36 - then you're back to the previous settings,
13:39 - you don't gain anything.
13:40 - So somehow you want a simple dependency
13:43 - but it's sufficiently rich that it captures
13:46 - real ways in which changing X should change
13:49 - the probability of Y. And one way
13:53 - to do it is to assume that there is some vector of parameters,
13:58 - I'll find this case.
14:00 - And then perhaps what you can do is
14:04 - you can assume some linear dependence where you basically
14:09 - take a linear combination of these X's, these features,
14:13 - weighted by these coefficients alpha
14:16 - and you try to do this as a regression.
14:18 - It's like linear regression at the end of the day,
14:21 - you take different values of X and you map them
14:24 - to different outputs.
14:26 - Now by itself this wouldn't work because remember
14:30 - we have to assume that these numbers are between 0 and 1,
14:34 - but that's something easy to fix.
14:35 - You can just transform that value
14:38 - with a function that rescales things and maps them
14:41 - to be between 0 and 1.
14:44 - For example, you can use the logistic function or the sigmoid
14:48 - and if you do that then you get what's
14:51 - known as logistic regression.
14:54 - It's a way to model a conditional distribution of Y
14:57 - given X, where you're assuming that,
15:01 - that conditional distribution takes
15:02 - a specific functional form.
15:04 - You're assuming that given different values of X,
15:08 - you can linearly combine them based
15:10 - on some vector of coefficients alpha
15:14 - and then you pass them through this sigmoid function,
15:16 - this S-shaped function that will take
15:21 - z values between minus infinity and plus infinity
15:25 - and we'll rescale them to be between 0 and 1
15:28 - so then they are valid probabilities.
15:32 - And that's another way to make progress,
15:35 - it's another way to deal with the fact that in general you
15:38 - cannot represent this complicated dependency between Y
15:42 - and all the X variables as a table.
15:44 - You have to either assume that there
15:46 - is conditional independencies or things don't even
15:49 - depend on some of the inputs or you
15:52 - assume that there is some specific functional form that
15:56 - allows you to compute these probabilities.
15:59 - And this is one such assumption is the logistic regression
16:01 - assumption.
16:02 - So the question is whether this implies
16:05 - some conditional independence assumptions
16:06 - and you can actually show the other way around that basically,
16:09 - if you assume the Naive Bayes factorization, then
16:13 - the conditional distribution of Y
16:14 - given X will have this functional form,
16:18 - but not necessarily vice versa.
16:21 - And so in some sense you're making a weaker statement
16:28 - about the relationship of the random variables which
16:31 - is why this model is stronger in practice.
16:34 - You're assuming less about how the random variables are related
16:38 - so to the extent that you have enough data
16:40 - to really learn the relationship.
16:42 - You're better off with this model
16:44 - because you are assuming less.
16:47 - If you have very limited data, you
16:49 - might be better off with the Naive Bayes model
16:51 - because you're making a strong assumption
16:54 - but the prior helps you more because you
16:55 - don't have enough data to figure out how things are
16:58 - really related to each other.
16:59 - But this is kind of a different sort of assumption,
17:02 - you're really saying there is some functional form that
17:04 - tells you how the random variables are
17:06 - related to each other.
17:07 - The question is, does this imply that the joint is a product
17:10 - distribution?
17:12 - You're just working at the level of a single conditional,
17:15 - so what we'll see is that in fact an autoregressive model,
17:19 - a deep autoregressive model will essentially be just
17:22 - be built by assuming that there is a chain rule factorization
17:28 - and then modeling the conditionals using
17:30 - this functional relationship.
17:33 - Maybe a linear regression model or a deep neural network
17:36 - and that's how we will build the first type
17:38 - of useful deep generative model.
17:41 - But this by itself is just for a single conditional.
17:46 - So it's not a statement about the joint,
17:48 - it's just saying I'm not even going to care about modeling
17:51 - the p of X, I'm not going to reason about the inputs that
17:55 - my logistic regression model is going to see because at test
17:57 - time somebody is going to give me the X's.
18:01 - So I don't need to bother about figuring out
18:03 - how the different words are related to each other.
18:05 - I'm only going to bother about modeling how to predict Y from X
18:09 - and that's already hard but I'm going
18:11 - to do it based on this simplifying assumption.
18:14 -
18:19 - And by assuming that you're making this linear dependence,
18:23 - again, you're making some assumptions
18:25 - which might or might not be true in the real world, right?
18:28 - So in particular, this is a relatively simple dependency
18:32 - that you're assuming between Y and X.
18:35 - So what you're doing is you're saying that,
18:39 - let's say if you have two features X1 and X2,
18:44 - then you're basically saying that equal probability
18:47 - contours are straight lines.
18:49 - So there is some straight lines such that all the points
18:54 - that lie on those straight lines they
18:56 - have the same conditional probability for Y
18:58 - or it also means that the decision boundary,
19:02 - so if you're using a threshold to decide
19:05 - whether a variable belongs to class
19:07 - 0 or 1 is going to be again a straight line.
19:11 - So all the points on this side of the line
19:13 - are going to be positive, all the other ones
19:15 - are going to be negative.
19:17 - And specifically, basically it means
19:20 - that if you think about how the probability changes
19:23 - as you change X and Y, it has a very specific functional form.
19:28 - It looks like this S kind of thing where the way
19:32 - you change the probability as you
19:34 - change X, the probability of Y given
19:37 - X changes as you change X has a very specific functional form.
19:41 - If you think about the lookup version of this,
19:44 - it would be an arbitrary function.
19:48 - Here you're saying, no, I'm willing to assume
19:50 - that it takes a very specific relatively simple functional
19:53 - form, which again might or might not be true in the real world
19:58 - maybe the probability of Y given X should have
20:01 - a very different shape and then this model
20:03 - is not going to work well.
20:06 - Like before we were assuming conditional independence
20:09 - might or might not be true in the real world,
20:11 - here we are assuming a specific functional form which
20:14 - might or might not be true in the real world
20:17 - and that determines whether or not
20:18 - your model is going to work well or not in practice.
20:21 -
20:24 - And so again, basically these are
20:28 - dealing with this issue of modeling distributions
20:30 - over high dimensional spaces, you have to make assumptions.
20:35 - Naive Bayes is one way to make progress,
20:37 - conditional independence assumption.
20:39 - The logistic regression model does not make that assumption
20:42 - explicitly, it does not assume that the features
20:45 - are conditionally independent given the label.
20:47 - So it's a little bit more powerful.
20:50 - If you think about the spam classification,
20:53 - there might be two words in your vocabulary like bank
20:57 - and account, knowing whether one appears in the email,
21:01 - so knowing X1 tells you a lot about whether X2
21:04 - appears in the email.
21:07 - But the Naive Bayes model assumes that it doesn't help,
21:11 - so that assumption is clearly wrong in the real world.
21:15 - The discriminative model does not make that assumption
21:18 - explicitly.
21:20 - And so let's say that in your data set
21:22 - these two words always appear together,
21:25 - so whenever there is bank, there is also account.
21:28 - The Naive Bayes model is forced to assume by construction
21:31 - that they are independent.
21:33 - So whenever you see that both of them appear,
21:37 - it's going to double count the evidence,
21:39 - is going to think both of them are telling me something about
21:42 - whether this is spam or not.
21:44 - I know that they are independent,
21:45 - so when I see both of them at the same time,
21:48 - I'm doubly confident that maybe this is spam.
21:52 - The logistic regression model can actually just
21:55 - set one of the coefficients to 0 and it doesn't double
21:59 - count the evidence.
22:00 - So you can see that you're making a weaker assumption
22:04 - and it's actually powerful and that's
22:06 - why this logistic regression model tends
22:08 - to work better in practice.
22:10 -
22:12 - However, the issue is that one thing you cannot do,
22:18 - let's say if you have a logistic regression model is that you
22:21 - cannot reason about your own inputs.
22:24 - So the only thing you can do is you can map X to Y,
22:28 - but you cannot, let's say the same thing happens also in image
22:36 - classification, so let's say that you have a model that is
22:39 - predicting a label of an image given the image X,
22:43 - that's the only thing you can do, predict Y from X.
22:46 - So if somebody gives you a new image where some of the pixels
22:50 - are missing, there is no way for you to impute the missing values
22:55 - because you don't know what's the relationship between the X
22:58 - variables.
22:59 - You didn't model p of X at all, you only model p of Y given X
23:04 - and so that's one thing you cannot do with a discriminative
23:08 - model that you can do with a generative model.
23:12 - A generative model is trying to model
23:14 - the full joint distribution between Y and X,
23:19 - and so at least in principle as long
23:22 - as you can do inference, as long as you can compute
23:24 - the right conditionals like modulo computational issues,
23:28 - you have enough information to predict anything from anything.
23:33 - So you can impute missing values,
23:35 - you can do more interesting things.
23:38 - But it's a harder problem because you're not only
23:41 - modeling the relationship between how to predict Y from X,
23:45 - you are also modeling the full thing,
23:47 - you're modeling the relationship between the features,
23:51 - between the inputs as well.

00:00 -
00:05 - How do neural networks come in here?
00:08 - Well, as we said, the issue with, or one
00:13 - of the issues with the logistic regression model,
00:15 - is that you're still making some simplifying assumption
00:18 - on how y depends on x.
00:20 - Right, we're assuming that there is this linear dependence.
00:23 - You take the x, the features, you combine them linearly,
00:27 - you pass them through the sigmoid,
00:29 - and that's what gives you y, which again might not
00:33 - be true in the real world.
00:35 - And so one way to get a more expressive,
00:39 - even make even weaker assumptions, in some sense,
00:42 - is to basically allow for some non-linear dependence.
00:49 - Right, you could say, instead of directly taking the x features
00:53 - and map them by linearly combining them to a probability
00:57 - value, I'm going to compute some features of the input x.
01:02 - Perhaps I'll do it by taking some linear combination
01:07 - of the features and then applying a non-linear function
01:11 - to each value that I get out of this.
01:15 - And then I'm going to do linear regression
01:18 - on top of these features.
01:21 - Right, so instead of directly applying linear regression
01:24 - to x, first I transform x by multiplying it by a matrix, a
01:29 - and then shifting by some vector of coefficients, b,
01:32 - and then I do logistic regression on these features.
01:36 - That's essentially a very simple one-layer neural network.
01:41 - Instead of predicting directly based on x,
01:43 - I transform x to get these features h,
01:46 - and then I do linear regression based on that.
01:48 - And that's strictly more powerful
01:50 - because now I'm allowed to do more complicated kind
01:53 - of computations.
01:54 - And if you think about that graph,
01:57 - that shape of that function of how y depends on x, now
02:01 - I have two more parameters.
02:03 - I have this matrix, this vectors,
02:05 - vector of coefficients of biases b,
02:10 - and I can use this to change the shape of the function.
02:13 - I can get more complicated relationships between y and x.
02:19 - And so there's a trade-off here.
02:21 - I'm using more parameters to represent
02:24 - this conditional distribution.
02:26 - I no longer have just a vector of coefficients, alpha.
02:29 - I also have a bunch of matrices for the previous layer
02:32 - in the neural network.
02:34 - But that gives me more flexibility
02:36 - in predicting y from x.
02:39 - And of course, you can imagine stacking this many, many times,
02:44 - and then you can use a deep neural network
02:46 - to predict y from x.
02:49 - Essentially, what you can do is you
02:51 - can repeat this multiple times, and you
02:53 - can get a more expressive way of capturing
02:56 - the relationship between some y variable
02:59 - and the input variables x.
03:03 - And this is going to be the building
03:04 - block that we're going to use to build deep generative models.
03:08 - So what we're going to do is we're
03:10 - going to take advantage of this fact
03:11 - that neural networks seem to work very well at solving
03:15 - this kind of prediction tasks, and we're
03:18 - going to combine them to build generative models.
03:22 - And the simplest way to do it is to use chain rule
03:26 - and then use neural networks to represent
03:28 - each one of those conditionals.
03:31 - And that's essentially on neural autoregressive model,
03:34 - and essentially that's what large language models do.
03:38 - They use chain rule and then they represent,
03:41 - they simplify the conditionals by assuming that you can model
03:46 - them using a neural network.
03:49 - So you can predict the next word given the previous ones
03:53 - using a neural network.
03:54 -
03:57 - But there's going to be other ways,
03:59 - and when we see other classes of generative models
04:01 - that are still kind of going to use this kind of ideas,
04:04 - but maybe we're going to combine them in different ways,
04:06 - and we're going to get different types of generative models.
04:10 - So that's kind of the story.
04:13 - There is the chain rule factorization,
04:16 - which is fully general.
04:17 - So given a joint, you can always write it
04:19 - as a product of conditionals with no assumptions.
04:23 - In a Bayesian network, you're going
04:25 - to try to simplify these conditionals somehow
04:28 - by assuming that the variables are conditionally independent.
04:31 - So whenever you're trying to predict x4,
04:34 - you don't really need x2 and x3.
04:35 - You just need x1, for example, which is usually too strong,
04:39 - and this doesn't work on high-dimensional data
04:42 - sets on images, text, the kind of things we care about.
04:46 - The one class of deep generative models,
04:51 - very successful one conceptually, does this.
04:54 - It just replaces all these conditionals
04:57 - that we don't know how to deal with, with neural networks.
05:01 - And you can choose different architectures,
05:04 - but fundamentally, that's the whole idea.
05:07 - We're going to use a neural network
05:09 - to predict what's the fourth word given
05:12 - the first, the second, and the third.
05:15 - And again, there's no free lunch in the sense
05:17 - that we're giving up is we're assuming that there
05:21 - is some relationship that these conditional distributions can
05:24 - basically be captured by a neural network, which
05:29 - might or might not be the case in practice.
05:33 - But that's the one way to get tractability to the extent
05:37 - that these neural networks are not too big,
05:39 - and somehow you're able to tie them together,
05:41 - you can see that sort of you need a different neural network
05:44 - for every position in the sequence, which
05:47 - would be very tricky.
05:48 - So somehow you need to figure out
05:49 - a way to tie together the weights of this neural network.
05:53 - So this can be done in practice, but ideally, this
05:57 - is the one way to get a deep generative model.
06:02 - The underlying idea is that you're
06:03 - going to simplify this conditionals by dropping
06:07 - the dependence on some variables,
06:09 - and that gives you a Bayesian network.
06:11 - Depending on which variables you drop,
06:13 - you're going to get different graphs.
06:15 - If you were to not drop any variable,
06:18 - you get this you get the fully general model.
06:21 - And that makes no assumptions.
06:24 - So that's fully general, but it's
06:26 - too expensive because these conditions are too--
06:31 - whenever you're conditioning on too many things,
06:34 - that conditional distribution is too complicated,
06:37 - and you cannot store it.
06:38 - You cannot learn it.
06:39 - And so you cannot actually use it in practice.
06:41 -
06:45 - Cool
06:47 - The last thing I wanted to mention
06:51 - is how to deal with continuous variables.
06:55 - So we often want to model not just discrete data
06:58 - but actually data that is more naturally
07:01 - thought of as continuous, so taking values
07:05 - over the whole real axis.
07:07 - And luckily, the machinery is very similar.
07:10 - So here, instead of working with probability mass functions,
07:13 - we work with probability density functions.
07:18 - And here you can start to see how
07:20 - the idea of working with tables already
07:22 - doesn't work because kind of there
07:24 - is an infinite number of different values
07:26 - that x can take.
07:28 - You cannot write down a table that will assign a number
07:30 - to each one of them.
07:32 - So you have to basically assume that there is some functional
07:37 - form, there is some function that you
07:40 - can use to map different values of x to a scalar.
07:46 - And for example, you can assume that x
07:48 - is Gaussian, which means that there
07:50 - is a relatively simple function that depends on two parameters,
07:54 - mu and sigma.
07:56 - And then you can plug them into this expression
07:58 - and you get back the density of the Gaussian
08:02 - at any particular point x, where mu and sigma here
08:06 - are the mean and the standard deviation of the Gaussian.
08:09 - Or you could say, OK, maybe a uniform random variable.
08:12 - Again this is another kind of relatively simple function
08:16 - that you can use to map x to densities.
08:21 - The uniform distribution over the interval between a and b
08:24 - would have that kind of functional form, et cetera.
08:29 - And the good news is that, again, we often care
08:32 - about modeling many random variables, which
08:35 - could be continuous or maybe a mix of continuous and discrete.
08:39 - In this case, we care about the joint probability density
08:44 - function.
08:45 - And the same, for example, a joint Gaussian
08:51 - would have that sort of functional form.
08:53 - So now x is a vector of numbers.
08:57 - And the good news is that the whole machinery
08:59 - of chain rule-based rule, they all still apply.
09:02 - So for example, we can write down the joint over PDF,
09:06 - over probability density function,
09:08 - over three random variables as a marginal PDF
09:12 - over the first one, a conditional over the first,
09:15 - the second given the first, and so forth.
09:19 - And this is useful because we can again mix and match,
09:25 - we can use Bayesian networks, or we
09:27 - can use neural networks plus Bayesian networks
09:30 - in different ways to get different types
09:33 - of generative models.
09:36 - So for example, you can get a mixture
09:39 - of two Gaussians using a simple Bayesian network with two
09:43 - random variables, z and x.
09:47 - So the Bayesian network has two random variables
09:49 - Z and x. x has z as a parent, z doesn't have any parent,
09:55 - and so what it means is that the joint over x and z
09:59 - can be factorized as the probability of z times
10:02 - the probability of x given z.
10:04 - And for example you could say z is a Bernoulli random variable
10:10 - with parameter p.
10:11 - So z is binary.
10:13 - It's either 0 or 1, and you choose a value with probability
10:17 - with flipping a biased coin with probability p.
10:22 - And then condition on z, you choose a value for x by,
10:26 - let's say, sampling from a Gaussian.
10:28 - And because z can take two different values,
10:31 - there's actually two Gaussians.
10:33 - There is one Gaussian when z is 0
10:35 - and there is one Gaussian when z is 1,
10:38 - and these two Gaussians are allowed
10:39 - to have different means and different variances.
10:42 - So this would be our graphical model that
10:47 - corresponds to a mixture of two Gaussians,
10:49 - and because you're mixing together two Gaussians,
10:51 - you have a slightly more flexible model.
10:54 - The parameters here are p, which is the probability of choosing
10:58 - 0 versus 1 for this latent variables,
11:01 - and then you have the mean and the means
11:03 - and the standard deviations.
11:05 - Of course, you could choose other things.
11:07 - For example, you could choose z to be a uniform random variable
11:11 - between a and b, and then given z, x
11:15 - let's say is a Gaussian with a mean, which is z and then maybe
11:19 - a fixed standard deviation.
11:22 - A more interesting one is the variational autoencoder,
11:26 - which we're going to cover in-depth in future lectures.
11:29 - But at the end of the day, a variational autoencoder
11:32 - is this Bayesian network with two nodes, z and x,
11:36 - and the assumption is that z is sampled from a Gaussian.
11:40 - So p of z is just a simple Gaussian random variable. .
11:45 - And here you see how we are going
11:47 - to mix and match Bayesian networks and neural networks.
11:51 - Given zx is, again, a Gaussian distribution, but the mean
11:58 - and the variance of this Gaussian
12:00 - are the outputs of some neural network or two
12:05 - neural networks, mu theta and sigma phi, which depend on z.
12:13 - So the sampling process is kind of a generalization of the ones
12:17 - you see before.
12:19 - Or again, you first sample z , then you feed z into a neural
12:22 - network that will give you means and variances that you're using
12:27 - another Gaussian distribution to sample a value for x.
12:32 - And this kind of machinery is essentially
12:34 - a variational autoencoder.
12:36 - This corresponds to the generative process
12:38 - that you use in a VAE or a variational autoencoder.
12:42 - And we're going to have to talk about how you actually
12:45 - train these kind of models and how to learn them,
12:47 - but fundamentally you see how we take this idea of mix
12:53 - and match them.
12:55 - There's a little bit of Bayesian network,
12:57 - a little bit of chain rule, a little bit of neural networks
13:00 - to represent complicated conditionals,
13:04 - but everything can be stitched together,
13:06 - and that's how you get different kinds of generative models.
13:10 - And yeah, just as a note, even though mu and sigma
13:15 - could be very complicated, the conditional distribution
13:18 - of x given z is still Gaussian in this case.
13:20 - Right, so there are some kind of trade-offs
13:22 - that you have to deal with.